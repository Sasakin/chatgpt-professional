{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В некоторых случаях необходимо объединить уже созданные индексные базы для их общего использования, посмотреть на составляющие базы, удалить отдельные чанки из векторной базы. Вот как это можно сделать:"
      ],
      "metadata": {
        "id": "KdW4U73ucyCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Устанавливаем библиотеки:"
      ],
      "metadata": {
        "id": "fiTMxuwWdFJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPNCBOQXJ4cC",
        "outputId": "7d464ff2-72df-42b4-e3f1-faf8284e6270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain_openai==0.1.22 faiss-cpu==1.8.0 openai==1.41.1 tiktoken==0.7.0 langchain-core==0.2.33 langchain==0.2.14 langchain_community==0.2.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# запустите эту ячейку, если используете секретный ключ в колабе\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "# Получение API ключа из пользовательских данных Colab и установка его как переменной среды\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "# Создание клиента OpenAI с использованием API ключа из переменных среды\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "fagko1FgT_tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import requests\n",
        "import re\n",
        "import tiktoken\n",
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "_kDaAd2hT_wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загружаем БЗ, делим на чанки, переводим в эмбеддинги, создаем 2 векторные базы Faiss:"
      ],
      "metadata": {
        "id": "EXJqU8eadLRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для загрузки документа по ссылке из гугл драйв\n",
        "def load_document_text(url):\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Swu1Gd_HT_0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем первую БЗ\n",
        "db_1=load_document_text(\"https://docs.google.com/document/d/1CadgV8oI_MgYZBfaIEa5vAnrQXyme1qK-qIHFwlZL58/edit?usp=sharing\")\n",
        "db_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "2XkTf3TGT_33",
        "outputId": "7690c9cd-0957-4e97-81f7-2c76dd5f0124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСсылка https://colab.research.google.com/drive/14n-IdIQlE8mlQhA88vhxXDcC2VcuDIIt?usp=sharing\\r\\nАлгоритмы сортировки\\r\\nСуществуют десятки алгоритмов сортировки. Разные алгоритмы оптимальны\\r\\nдля разных наборов и типов данных. Мы рассмотрим некоторые из них:\\r\\n-   Пузырьковая сортировка\\r\\n-   Сортировка выборкой\\r\\n-   Сортировка вставками\\r\\n-   Сортировка слиянием\\r\\n-   Быстрая сортировка\\r\\n\\r\\n\\r\\nПузырьковая сортировка\\r\\nПузырьковая сортировка или сортировка простыми обменами – один из\\r\\nпростейших алгоритмов сортировки. Он применяется для упорядочивания\\r\\nмассивов небольших размеров.\\r\\n    # Алгоритм пузырьковой сортировки\\r\\nСуть алгоритма в том, что совершается несколько проходов по массиву. При\\r\\nкаждом проходе попарно сравниваются два соседних элемента. Если они\\r\\nнаходятся в верном порядке, то ничего не происходит, в противном случае\\r\\nони меняются местами. В результате первого прохода максимальный элемент\\r\\nокажется в конце, то есть всплывет словно пузырек. Затем все повторяется\\r\\nдо того момента пока весь массив не будет отсортирован.\\r\\n    def bubble_sort(nums):\\r\\n        # Устанавливаем swapped в True, чтобы цикл запустился хотя бы один раз\\r\\n        swapped = True\\r\\n        while swapped:\\r\\n            swapped = False\\r\\n            for i in range(len(nums) - 1):\\r\\n                if nums[i] > nums[i + 1]:\\r\\n                    # Меняем элементы\\r\\n                    nums[i], nums[i + 1] = nums[i + 1], nums[i]\\r\\n                    # Устанавливаем swapped в True для следующей итерации\\r\\n                    swapped = True\\r\\n\\r\\n\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    bubble_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма пузырьковой сортировки\\r\\nВ алгоритме пузырьковой сортировки есть два вложенных цикла while и for.\\r\\nЕсли взять самый худший случай (изначально список отсортирован по\\r\\nубыванию), то сложность алгоритма будет квадратичной:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка выборкой\\r\\nЭтот алгоритм сегментирует список на две части: отсортированные и\\r\\nнесортированные. Он постоянно удаляет наименьший элемент из\\r\\nнесортированного сегмента списка и добавляет его в отсортированный\\r\\nсегмент.\\r\\n    # Алгоритм сортировки выборкой\\r\\nНа практике нам не нужно создавать новый список для отсортированных\\r\\nэлементов, мы будем обрабатывать крайнюю левую часть списка как\\r\\nотсортированный сегмент. Затем мы ищем во всем списке наименьший элемент\\r\\nи меняем его на первый элемент.\\r\\nТеперь мы знаем, что первый элемент списка отсортирован, мы получаем\\r\\nнаименьший элемент из оставшихся элементов и заменяем его вторым\\r\\nэлементом. Это повторяется до тех пор, пока последний элемент списка не\\r\\nстанет оставшимся элементом для изучения.\\r\\n    def selection_sort(nums):\\r\\n        # Значение i соответствует кол-ву отсортированных значений\\r\\n        for i in range(len(nums)):\\r\\n            # Исходно считаем наименьшим первый элемент\\r\\n            lowest_value_index = i\\r\\n            # Этот цикл перебирает несортированные элементы\\r\\n            for j in range(i + 1, len(nums)):\\r\\n                if nums[j] < nums[lowest_value_index]:\\r\\n                    lowest_value_index = j\\r\\n            # Самый маленький элемент меняем с первым в списке\\r\\n            nums[i], nums[lowest_value_index] = nums[lowest_value_index], nums[i]\\r\\n\\r\\n\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    selection_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки выборкой\\r\\nВ этом алгоритме два вложенных цикла for, следовательно сложность\\r\\nквадратичная:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка вставками\\r\\nКак и сортировка выборкой, этот алгоритм делит список на отсортированные\\r\\nи несортированные части. Он перебирает неотсортированный сегмент и\\r\\nвставляет просматриваемый элемент в правильную позицию отсортированного\\r\\nсписка.\\r\\n    # Алгоритм сортировки вставками\\r\\nПредполагается, что первый элемент списка отсортирован. Переходим к\\r\\nследующему элементу, обозначим его х. Если х больше первого, оставляем\\r\\nего на своём месте. Если он меньше, копируем его на вторую позицию, а х\\r\\nустанавливаем как первый элемент.\\r\\nПереходя к другим элементам несортированного сегмента, перемещаем более\\r\\nкрупные элементы в отсортированном сегменте вверх по списку, пока не\\r\\nвстретим элемент меньше x или не дойдём до конца списка. В первом случае\\r\\nx помещается на правильную позицию.\\r\\n    def insertion_sort(nums):\\r\\n        # Сортировку начинаем со второго элемента, т.к. считается, что первый элемент уже отсортирован\\r\\n        for i in range(1, len(nums)):\\r\\n            item_to_insert = nums[i]\\r\\n            # Сохраняем ссылку на индекс предыдущего элемента\\r\\n            j = i - 1\\r\\n            # Элементы отсортированного сегмента перемещаем вперёд, если они больше\\r\\n            # элемента для вставки\\r\\n            while j >= 0 and nums[j] > item_to_insert:\\r\\n                nums[j + 1] = nums[j]\\r\\n                j -= 1\\r\\n            # Вставляем элемент\\r\\n            nums[j + 1] = item_to_insert\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    insertion_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки вставками\\r\\nТак же, как и предыдущие 2 алгоритма этот имеет 2 вложенных цикла for и\\r\\nwhile, значит сложность тоже квадратичная:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка слиянием\\r\\nЭтот алгоритм разбивает список на две части, каждую из них он разбивает\\r\\nещё на две и т. д. Список разбивается пополам, пока не останутся\\r\\nединичные элементы.\\r\\nСоседние элементы становятся отсортированными парами. Затем эти пары\\r\\nобъединяются и сортируются с другими парами. Этот процесс продолжается\\r\\nдо тех пор, пока не отсортируются все элементы.\\r\\n    # Алгоритм сортировки слиянием\\r\\nСписок рекурсивно разделяется пополам, пока в итоге не получатся списки\\r\\nразмером в один элемент. Массив из одного элемента считается\\r\\nупорядоченным. Соседние элементы сравниваются и соединяются вместе. Это\\r\\nпроисходит до тех пор, пока не получится полный отсортированный список.\\r\\nСортировка осуществляется путём сравнения наименьших элементов каждого\\r\\nподмассива. Первые элементы каждого подмассива сравниваются первыми.\\r\\nНаименьший элемент перемещается в результирующий массив. Счётчики\\r\\nрезультирующего массива и подмассива, откуда был взят элемент,\\r\\nувеличиваются на 1.\\r\\n    def merge(left_list, right_list):\\r\\n        sorted_list = []\\r\\n        left_list_index = right_list_index = 0\\r\\n        # Длина списков часто используется, поэтому создадим переменные для удобства\\r\\n        left_list_length, right_list_length = len(left_list), len(right_list)\\r\\n        for _ in range(left_list_length + right_list_length):\\r\\n            if left_list_index < left_list_length and right_list_index < right_list_length:\\r\\n                # Сравниваем первые элементы в начале каждого списка\\r\\n                # Если первый элемент левого подсписка меньше, добавляем его\\r\\n                # в отсортированный массив\\r\\n                if left_list[left_list_index] <= right_list[right_list_index]:\\r\\n                    sorted_list.append(left_list[left_list_index])\\r\\n                    left_list_index += 1\\r\\n                # Если первый элемент правого подсписка меньше, добавляем его\\r\\n                # в отсортированный массив\\r\\n                else:\\r\\n                    sorted_list.append(right_list[right_list_index])\\r\\n                    right_list_index += 1\\r\\n            # Если достигнут конец левого списка, элементы правого списка\\r\\n            # добавляем в конец результирующего списка\\r\\n            elif left_list_index == left_list_length:\\r\\n                sorted_list.append(right_list[right_list_index])\\r\\n                right_list_index += 1\\r\\n            # Если достигнут конец правого списка, элементы левого списка\\r\\n            # добавляем в отсортированный массив\\r\\n            elif right_list_index == right_list_length:\\r\\n                sorted_list.append(left_list[left_list_index])\\r\\n                left_list_index += 1\\r\\n        return sorted_list\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nАлгоритм сортировки слиянием\\r\\n    def merge_sort(nums):\\r\\n        # Возвращаем список, если он состоит из одного элемента\\r\\n        if len(nums) <= 1:\\r\\n            return nums\\r\\n        # Для того чтобы найти середину списка, используем деление без остатка\\r\\n        # Индексы должны быть integer\\r\\n        mid = len(nums) // 2\\r\\n        # Сортируем и объединяем подсписки\\r\\n        left_list = merge_sort(nums[:mid])\\r\\n        right_list = merge_sort(nums[mid:])\\r\\n        # Объединяем отсортированные списки в результирующий\\r\\n        return merge(left_list, right_list)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    random_list_of_nums = merge_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки слиянием\\r\\nБлагодаря тому, что функция merge_sort() возвращает новый список, а не\\r\\nсортирует существующий, такая сортировка имеет только один цикл, но при\\r\\nэтом для такого алгоритма требуется больше памяти.\\r\\nСложность такого алгоритма является линейно-логарифмической:\\r\\n     O(n*log n), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nБыстрая сортировка\\r\\nПри использовании этого алгоритма, массив разделяется на две части по\\r\\nразные стороны от опорного элемента. В процессе сортировки элементы\\r\\nменьше опорного помещаются перед ним, а равные или большие — позади.\\r\\n    # Алгоритм быстрой сортировки\\r\\nБыстрая сортировка начинается с разбиения списка и выбора одного из\\r\\nэлементов в качестве опорного. А всё остальное передвигаем так, чтобы\\r\\nэтот элемент встал на своё место. Все элементы меньше него перемещаются\\r\\nвлево, а равные и большие элементы перемещаются вправо.\\r\\n    def partition(nums, low, high):\\r\\n        # Выбираем средний элемент в качестве опорного\\r\\n        # Также возможен выбор первого, последнего\\r\\n        # или произвольного элементов в качестве опорного\\r\\n        pivot = nums[(low + high) // 2]\\r\\n        i = low - 1\\r\\n        j = high + 1\\r\\n        while True:\\r\\n            i += 1\\r\\n            while nums[i] < pivot:\\r\\n                i += 1\\r\\n            j -= 1\\r\\n            while nums[j] > pivot:\\r\\n                j -= 1\\r\\n            if i >= j:\\r\\n                return j\\r\\n            # Если элемент с индексом i (слева от опорного) больше, чем\\r\\n            # элемент с индексом j (справа от опорного), меняем их местами\\r\\n            nums[i], nums[j] = nums[j], nums[i]\\r\\n    def quick_sort(nums):\\r\\n        # Создадим вспомогательную функцию, которая вызывается рекурсивно\\r\\n        def _quick_sort(items, low, high):\\r\\n            if low < high:\\r\\n                # This is the index after the pivot, where our lists are split\\r\\n                split_index = partition(items, low, high)\\r\\n                _quick_sort(items, low, split_index)\\r\\n                _quick_sort(items, split_index + 1, high)\\r\\n        _quick_sort(nums, 0, len(nums) - 1)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    quick_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма быстрой сортировки\\r\\nВ худшем случае, если опорный элемент будет минимальным или максимальным\\r\\nиз списка, то сложность такого алгоритма будет квадратичной:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСравнение рассмотренных алгоритмов сортировки\\r\\nЕсли оценка сложности при помощи Big O это статистический анализ\\r\\nскорости работы алгоритма, то теперь проведем эмпирический анализ.\\r\\nВозьмём один и тот же список из 5000 значений, выполним сортировку\\r\\nразными алгоритмами и сравним время работы каждого из них.\\r\\n    import time\\r\\n    import numpy as np\\r\\n    # Создадим список длиной 5000 значений с рандомными числами от 1 до 1000\\r\\n    exp_list = np.random.randint(0, 1000, 5000)\\r\\nПузырьковая сортировка\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    bubble_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    10.612551212310791\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict = {}\\r\\n    time_dict[\\'bubble\\'] = round(end, 2)\\r\\nТема: Сортировка выборкой.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    selection_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    4.216295003890991\\r\\n    # Добавим время работы алгоритма в словарь\\r\\n    time_dict[\\'selection\\'] = round(end, 2)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nПродолжение Сравнение рассмотренных алгоритмов сортировки\\r\\nТема: Сортировка вставками.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    insertion_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    2.7952945232391357\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict[\\'insertion\\'] = round(end, 2)\\r\\nТема: Сортировка слиянием.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    merge_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    0.03851127624511719\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict[\\'merge\\'] = round(end, 2)\\r\\nТема: Быстрая сортировка.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    quick_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    0.0353546142578125\\r\\n    # Добавим время работы алгоритма в словарь\\r\\n    time_dict[\\'quick\\'] = round(end, 2)\\r\\nВыведем график скорости работы алгоритмов\\r\\n    import matplotlib.pyplot as plt\\r\\n    plt.figure(figsize=(8,6))\\r\\n    plt.bar(time_dict.keys(), time_dict.values())\\r\\n    plt.show()\\r\\nСтандартная функция сортировки python sort() использует сортировку Тима,\\r\\nкоторая представляет собой комбинацию сортировки слиянием и сортировки\\r\\nвставками.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nАлгоритмы поиска\\r\\nСамый простой пример поиска можно осуществить при помощи операторов in\\r\\nили not in.\\r\\n    # Пример поиска символа в строке\\r\\n    \\'t\\' in \\'qwerty\\'\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    True\\r\\n# Пример поиска значения в списке\\r\\n    5 in [1,2,3,4]\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    False\\r\\nМинус такого поиска в том, что мы не знаем, где именно находится искомая\\r\\nинформация (например индекс значения в списке), а знаем только то, что\\r\\nона там есть или нет.\\r\\nТема: Линейный поиск.\\r\\nАлгоритм линейного поиска очень простой. Мы просто проходим по всем\\r\\nэлементам массива по порядку и сравниваем с искомым значением.\\r\\n    # Алгоритм линейного поиска\\r\\nПлюс алгоритма линейного поиска в том, что он работает как с\\r\\nотсортированными массивами, так и с несортированными.\\r\\n    def LinearSearch(nums, val):\\r\\n        for i in range(len(nums)):\\r\\n            if nums[i] == val:\\r\\n                return i\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    result = LinearSearch(random_list_of_nums, 8)\\r\\n    print(\\'Значение 8 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 8 имеет индекс - 4\\r\\nСложность алгоритма линейного поиска\\r\\nСложность данного алгоритма линейная, так как количество итераций\\r\\nнапрямую зависит от размера входных данных.\\r\\n     O(n), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Бинарный поиск.\\r\\nБинарный поиск работает по принципу «разделяй и властвуй». Он быстрее,\\r\\nчем линейный поиск, но требует, чтобы массив был отсортирован перед\\r\\nвыполнением алгоритма.\\r\\n    # Алгоритм бинарного поиска\\r\\nРабота данного алгоритма основана на определении среднего значения. Если\\r\\nэто значение не является искомым, то нужно определить в какой половине\\r\\nмассива мы будем продолжать поиски. Если то значение, которое мы ищем\\r\\nбольше среднего, то поиски продолжатся в правой части массива, если\\r\\nменьше, то в левой. Так на каждой итерации мы сужаем область поиска,\\r\\nпостепенно двигаясь к правильному ответу.\\r\\n    def BinarySearch(nums, val):\\r\\n        first = 0\\r\\n        last = len(nums)-1\\r\\n        index = -1\\r\\n        while (first <= last) and (index == -1):\\r\\n            mid = (first+last)//2\\r\\n            if nums[mid] == val:\\r\\n                index = mid\\r\\n            else:\\r\\n                if val<nums[mid]:\\r\\n                    last = mid -1\\r\\n                else:\\r\\n                    first = mid +1\\r\\n        return index\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = BinarySearch(sorted_list_of_nums, 31)\\r\\n    print(\\'Значение 31 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 31 имеет индекс - 6\\r\\nСложность алгоритма бинарного поиска\\r\\nНа каждой итерации бинарного поиска мы делим массив на 2 части (одну из\\r\\nкоторых отбрасываем), следовательно сложность данного алгоритма\\r\\nлогарифмическая.\\r\\n     O(log n), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Jump search.\\r\\nJump search - это ещё один алгоритм поиска, который работает по принципу\\r\\n\"разделяй и властвуй\" и требует на входе отсортированный массив.\\r\\n    # Алгоритм jump search\\r\\nСуть алгоритма jump search скрыта в его названии. Вместо того, чтобы\\r\\nпроходить по каждому элементу массива, или делить его пополам, мы будем\\r\\nдвигаться с определённым шагом, как бы перепрыгивая (jump) некоторые\\r\\nэлементы. Как правило, шаг берется не случайно, а считается как\\r\\nквадратный корень из количества элементов массива. Например, для массива\\r\\nдлиной 9, шаг будет составлять 3, а для массива 15 шаг будет тоже 3\\r\\n(округление идёт до целого числа, отбрасывая остаток).\\r\\n    def JumpSearch(nums, val):\\r\\n        length = len(nums)\\r\\n        jump = int(length**0.5)\\r\\n        left, right = 0, 0\\r\\n        while left < length and nums[left] <= val:\\r\\n            right = min(length - 1, left + jump)\\r\\n            if nums[left] <= val and nums[right] >= val:\\r\\n                break\\r\\n            left += jump;\\r\\n        if left >= length or nums[left] > val:\\r\\n            return -1\\r\\n        right = min(length - 1, right)\\r\\n        i = left\\r\\n        while i <= right and nums[i] <= val:\\r\\n            if nums[i] == val:\\r\\n                return i\\r\\n            i += 1\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = JumpSearch(sorted_list_of_nums, 25)\\r\\n    print(\\'Значение 25 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 25 имеет индекс - 5\\r\\nСложность алгоритма jump search\\r\\nВременная сложность jump search равна:\\r\\n     O(√n), где √n — размер прыжка, n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Поиск Фибоначчи\\r\\nЧисла Фибоначчи — это последовательность чисел, где каждый элемент\\r\\nявляется суммой двух предыдущих чисел.\\r\\n    0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 ...\\r\\nАлгоритм поиска Фибоначчи использует числа Фибоначчи для расчета\\r\\nдиапазона поисков элемента в массиве.\\r\\nРазберем как работает этот подход.\\r\\nПусть дан список из 11 значений. Нужно найти индекс значения 28 в этом\\r\\nсписке. Так как длина списка 11, то мы ищем число в ряде Фибоначчи\\r\\nравное 11 или ближайшее большее. Число 13 является таковым.\\r\\nИтерация 1:\\r\\nДальше сдвигаемся на 2 шага назад (на первой итерации) по ряду\\r\\nФибоначчи, получаем число 5. Для расчета индекса искомого значения в\\r\\nсписке используется формула:\\r\\n     i = min(idx+a, n-1), где\\r\\n        idx - индекс полученный на предыдущем шаге (или -1 на первой итерации)\\r\\n        a - рассчитанное нами число 5\\r\\n        n - длина списка\\r\\nЗначения idx=-1, a=5. Подставляем значения в формулу и считаем:\\r\\n    i = min(-1+5, 11-1) = min(4, 10) = 4\\r\\nИндекс искомого значения на первой итерации получили 4. Переходим к\\r\\nзначению списка с индексом 4 - это значение 13. Сравниваем полученное\\r\\nзначение 13 с искомым 28. Они не равны, следовательно продолжаем поиски дальше.\\r\\nИтерация 2:\\r\\nЕсли искомое значение не найдено, важно то, больше полученное значение\\r\\nили меньше. Если значение, которое мы получили в ходе расчетов меньше,\\r\\nчем искомое, то в ряду Фибоначчи мы смещаемся на 1 шаг назад (от числа 5 переходим к 3).\\r\\nТеперь idx=4, a=3. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на второй итерации получили 7. Переходим к значению\\r\\nсписка с индексом 7 - это значение 25. Сравниваем полученное значение 25\\r\\nс искомым 28. Они не равны, следовательно продолжаем поиски дальше.\\r\\nИтерация 3:\\r\\nПолученное значение 25 снова меньше искомого 28, следовательно в ряду\\r\\nФибоначчи мы смещаемся так же на 1 шаг назад (от числа 3 переходим к 2).\\r\\nТеперь idx=7, a=2. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на третьей итерации получили 9. Переходим к значению\\r\\nсписка с индексом 9 - это значение 34. Сравниваем полученное значение 34\\r\\nс искомым 28. Они не равны, следовательно продолжаем поиски дальше.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Продолжаем Поиск Фибоначчи\\r\\nИтерация 4:\\r\\nПосле третьей итерации полученное значение оказалось больше искомого,\\r\\nпоэтому алгоритм дальнейшего поиска немного меняется. Отличие в том, что\\r\\nзначение idx для расчетов мы возьмём не последнее рассчитанное (9), а\\r\\nполученное на предыдущей итерации (7). Что касается смещения в ряду\\r\\nФибоначчи, то теперь оно на 2 шага назад (от числа 2 переходим к 1).\\r\\nТеперь idx=7, a=1. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на четвертой итерации получили 8. Переходим к значению\\r\\nсписка с индексом 8 - это значение 28.\\r\\nПолученное значение 28 сравниваем с искомым 28 - они равны,\\r\\nследовательно поиск окончен.\\r\\nИндекс искомого значения 28 равен 8.\\r\\n    def FibonacciSearch(nums, val):\\r\\n        fibM_minus_2 = 0\\r\\n        fibM_minus_1 = 1\\r\\n        fibM = fibM_minus_1 + fibM_minus_2\\r\\n        while (fibM < len(nums)):\\r\\n            fibM_minus_2 = fibM_minus_1\\r\\n            fibM_minus_1 = fibM\\r\\n            fibM = fibM_minus_1 + fibM_minus_2\\r\\n        index = -1;\\r\\n        while (fibM > 1):\\r\\n            i = min(index + fibM_minus_2, (len(nums)-1))\\r\\n            if (nums[i] < val):\\r\\n                fibM = fibM_minus_1\\r\\n                fibM_minus_1 = fibM_minus_2\\r\\n                fibM_minus_2 = fibM - fibM_minus_1\\r\\n                index = i\\r\\n            elif (nums[i] > val):\\r\\n                fibM = fibM_minus_2\\r\\n                fibM_minus_1 = fibM_minus_1 - fibM_minus_2\\r\\n                fibM_minus_2 = fibM - fibM_minus_1\\r\\n            else :\\r\\n                return i\\r\\n        if(fibM_minus_1 and index < (len(nums)-1) and nums[index+1] == val):\\r\\n            return index+1;\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = FibonacciSearch(sorted_list_of_nums, 13)\\r\\n    print(\\'Значение 13 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 13 имеет индекс - 3\\r\\nСложность алгоритма поиска Фибоначчи\\r\\nВременная сложность данного алгоритма логарифмическая:\\r\\n     O(log n), где n — количество элементов списка.\\r\\nЭтот алгоритм в большинстве случаев работает быстрее, чем линейный поиск\\r\\nи jump search.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nЭкспоненциальный поиск\\r\\n    # Алгоритм экспоненциального поиска\\r\\nЭкспоненциальный поиск это надстройка над бинарным поиском. Его\\r\\nособенность в том, что индекс при первичном поиске элемента 2^n, а после\\r\\nиспользуется алгоритм бинарного поиска.\\r\\n    def ExponentialSearch(nums, val):\\r\\n        if nums[0] == val:\\r\\n            return 0\\r\\n        index = 1\\r\\n        while index < len(nums) and nums[index] <= val:\\r\\n            prev_index = index\\r\\n            index = index * 2\\r\\n        return BinarySearch(nums[:min(index, len(nums))], val)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39, 45, 55, 58]\\r\\n    result = ExponentialSearch(sorted_list_of_nums, 55)\\r\\n    print(\\'Значение 55 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 55 имеет индекс - 10\\r\\nСложность алгоритма экспоненциального поиска\\r\\nВременная сложность данного алгоритма логарифмическая:\\r\\n     O(log n), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nЭкспоненциальный поиск работает лучше, чем бинарный, когда искомый\\r\\nэлемент находится ближе к началу массива.\\r\\nДанный алгоритм поиска считается одним из наиболее эффективных.\\r\\nИнтерполяционный поиск\\r\\nИнтерполяционный поиск является разновидностью бинарного поиска, только\\r\\nвероятную позицию (индекс) искомого элемента он вычисляет по формуле:\\r\\n    index = low + [(val-nums[low])*(high-low) / (nums[high]-nums[low])],\\r\\n    где low  - начальный индекс массива;\\r\\n        high - конечный индекс массива;\\r\\n        val  - искомое значение;\\r\\n        nums - исходный массив данных.\\r\\n    def InterpolationSearch(nums, val):\\r\\n        low = 0\\r\\n        high = (len(nums) - 1)\\r\\n        while low <= high and val >= nums[low] and val <= nums[high]:\\r\\n            index = low + int(((float(high - low) / ( nums[high] - nums[low])) * ( val - nums[low])))\\r\\n            if nums[index] == val:\\r\\n                return index\\r\\n            if nums[index] < val:\\r\\n                low = index + 1;\\r\\n            else:\\r\\n                high = index - 1;\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39, 45, 55, 58]\\r\\n    result = InterpolationSearch(sorted_list_of_nums, 45)\\r\\n    print(\\'Значение 45 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 45 имеет индекс - 9\\r\\nСложность алгоритма интерполяционного поиска\\r\\nДанный алгоритм работает быстрее на отсортированных, равномерно\\r\\nраспределенных массивах. Сложность алгоритма на таких массивах O(log log\\r\\nn), но если значения массива не равномерно распределены, тогда сложность\\r\\nалгоритма линейная:\\r\\n     O(n), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСравнение рассмотренных алгоритмов поиска\\r\\nВ первой части ноутбука мы проводили сравнение разных алгоритмов\\r\\nсортировки при помощи библиотеки time. Для сравнения скорости работы\\r\\nалгоритмов поиска воспользуемся магической комендой %%timeit\\r\\n\\r\\n\\r\\n    import numpy as np\\r\\n    # Создадим массив из 50 рандомных значений и отсортируем его\\r\\n    random_list = np.random.randint(1,100,50)\\r\\n    random_list.sort()\\r\\n    print(random_list)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [ 1  7  8  9 10 11 11 15 15 15 16 21 22 24 24 27 30 32 33 38 41 45 49 49 52 55 58 59 77 78 79 79 81 82 82 84 84 85 85 85 87 92 93 93 93 94 95 96 98 98]\\r\\nБудем искать значение 30\\r\\n    %%timeit\\r\\n    # Линейный поиск\\r\\n    result = LinearSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    4.75 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Бинарный поиск\\r\\n    result = BinarySearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    4.32 µs ± 197 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Jump search\\r\\n    result = JumpSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    5.81 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Поиск Фибоначчи\\r\\n    result = FibonacciSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    5.61 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n    %%timeit\\r\\n        \\r\\n    # Экспоненциальный поиск\\r\\n    result = ExponentialSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    6.47 µs ± 152 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n    %%timeit\\r\\n        \\r\\n    # Интерполяционный поиск\\r\\n    result = InterpolationSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    8.84 µs ± 224 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n\\r\\n\\r\\nПостроим график средней скорости\\r\\n    import matplotlib.pyplot as plt\\r\\n\\r\\n\\r\\n    time_dict2 = {\\'linear\\':4.75,\\r\\n                  \\'binary\\':4.32,\\r\\n                  \\'jump\\':5.81,\\r\\n                  \\'fibonacci\\':5.61,\\r\\n                  \\'exponential\\':6.47,\\r\\n                  \\'interpolation\\':8.84}\\r\\n    plt.figure(figsize=(8,6))\\r\\n    plt.bar(time_dict2.keys(), time_dict2.values())\\r\\n    plt.show()\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Алгоритмы поиска и сортировки: важность и принципы работы\\r\\nНа уроке лектор рассказывает о важности алгоритмов поиска и сортировки в цифровой эпохе. Он объясняет, что алгоритмы - это рецепты или планы действий для решения задач. Алгоритмы помогают структурировать действия и делать их эффективнее. Лектор также говорит о том, что алгоритмы помогают оптимизировать ресурсы, такие как время и память, и решать задачи с большими данными быстрее и эффективнее. Он упоминает, что знание алгоритмов помогает понять работу различных библиотек и технологий в Python, таких как SQL и Docker. Лектор также объясняет, как оценивать алгоритмы, используя элементарные шаги и понятие Big O Notation, которое описывает сложность алгоритма. Он приводит примеры различных сложностей, таких как константная, линейная и логарифмическая, и объясняет, как они меняются с увеличением размера входных данных.\\r\\n\\r\\n\\r\\nТема: Квадратичная сложность и алгоритм сортировки пузырьком\\r\\nНа уроке рассказывается о сложности алгоритмов и различных методах сортировки. Лектор объясняет, что квадратичная сложность возникает, когда время выполнения алгоритма увеличивается пропорционально квадрату размера входных данных. Это происходит, например, при использовании двух вложенных циклов. Лектор также упоминает о других видах сложности, таких как кубическая и экспоненциальная.\\r\\nЗатем рассматривается пример алгоритма сортировки пузырьком, который является одним из самых простых. Лектор объясняет, как работает этот алгоритм и демонстрирует его на конкретном списке чисел. Он также обсуждает различные методы сортировки, такие как выборка, вставка, слияние и быстрая сортировка.\\r\\nВ конце урока лектор отвечает на вопросы студентов, в том числе о том, работает ли бинарный поиск только для сортированных списков. Он также объясняет, как поменять местами значения двух переменных без использования третьей переменной.\\r\\nВ целом, на уроке рассматривается тема сложности алгоритмов и методов сортировки, а также демонстрируется пример алгоритма сортировки пузырьком.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Сортировка выборкой и использование флага swap\"\\r\\nНа уроке лектор рассказывает о сортировке пузырьком и сортировке выборкой. Он объясняет, что в сортировке пузырьком используется флаг swap, который указывает, нужно ли повторно запускать цикл FOR для проверки и перестановки элементов. Если после прохождения цикла FOR по списку не было перестановок, то флаг swap становится false и цикл больше не запускается. Лектор также объясняет, что в сортировке выборкой на каждой итерации находится самый маленький элемент и ставится на первое место. Он также демонстрирует код и объясняет его работу. В конце лектор отвечает на вопросы слушателей и переходит к объяснению следующего алгоритма сортировки.\\r\\n\\r\\n\\r\\nТема: Работа флага в цикле while\\r\\nНа уроке рассказывается о сортировке списка значений с использованием флага в цикле while. Лектор объясняет, что сортировка значений может быть полезна в различных задачах и что существуют разные способы сортировки, каждый из которых может быть оптимальным для конкретной задачи. \\r\\nДалее лектор объясняет алгоритм сортировки с использованием флага. Он приводит пример списка значений и объясняет, как происходит сравнение и перестановка элементов в цикле. Лектор также отмечает, что в данном алгоритме используется переменная \"last value index\", которая хранит индекс минимального элемента, и что после каждой итерации цикла значение этой переменной изменяется. \\r\\nЛектор также отмечает, что данный алгоритм сортировки отличается от предыдущего алгоритма, который был рассмотрен ранее. Он объясняет, что в данном алгоритме происходит поиск минимального элемента и его перестановка с первым элементом списка, а затем происходит сортировка оставшейся части списка. \\r\\nЛектор также обсуждает различные алгоритмы сортировки, такие как сортировка вставками, и объясняет, что выбор алгоритма сортировки может существенно влиять на скорость выполнения программы. Он отмечает, что некоторые алгоритмы могут иметь квадратичную сложность, что может привести к значительному увеличению времени выполнения программы при большом количестве элементов в списке. \\r\\nВ конце урока лектор предлагает рассмотреть подробнее код алгоритма сортировки с использованием флага и объясняет его работу. Он также отмечает, что понимание различных алгоритмов сортировки и их оптимизация могут помочь в оптимизации кода и улучшении производительности программы.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Описание алгоритма сортировки вставками\"\\r\\nНа уроке рассказывается о сортировке вставками. Лектор объясняет, что в данном алгоритме используются переменные value, itemToInsert, i и j. Затем он по шагам объясняет, как происходит сортировка. Сначала берется первый элемент и сравнивается с предыдущими элементами. Если предыдущий элемент больше, то он копируется в следующую позицию, и так продолжается до тех пор, пока не будет найдено место для вставки элемента. Затем процесс повторяется для следующего элемента. Лектор также отмечает, что алгоритм имеет квадратичную сложность. Он предлагает перейти к следующему алгоритму - сортировке слиянием.\\r\\n\\r\\n\\r\\nТема: Алгоритм сортировки слиянием с использованием рекурсии\\r\\nНа уроке рассказывается о сортировке слиянием (merge sort). Алгоритм разделяет список на две части, рекурсивно сортирует каждую часть, а затем объединяет их в отсортированный список. Для объединения используется функция merge, которая принимает два отсортированных списка и возвращает новый отсортированный список, объединяя элементы из обоих списков. Алгоритм продолжает разбивать и сортировать списки до тех пор, пока не останется один элемент. Затем он объединяет все списки с помощью функции merge, пока не получит исходный отсортированный список.\\r\\n\\r\\n\\r\\nТема: Запись длины списков и присвоение нулевых значений\\r\\nНа уроке рассказывается о сортировке слиянием и быстрой сортировке. Лектор объясняет, как работает алгоритм сортировки слиянием, где два отсортированных списка объединяются в один отсортированный список. Он также объясняет, как работает алгоритм быстрой сортировки, который использует рекурсию и опорный элемент для разделения списка на две части. Лектор приводит примеры кода и объясняет каждую часть алгоритма. Он также отвечает на вопросы студентов и дает дополнительные объяснения.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Определение опорной точки и проверка значений слева и справа\\r\\nНа уроке рассказывается о быстрой сортировке (Quick Sort). Лектор объясняет, что в начале алгоритма выбирается опорная точка (pivot), которая помогает разделить массив на две части. Затем происходит сравнение значений слева и справа от опорной точки. Если значение слева меньше опорной точки, то индекс i увеличивается на 1. Если значение справа больше опорной точки, то индекс j уменьшается на 1. Если i становится больше или равно j, то алгоритм завершается и возвращается индекс j. Если это не так, то значения i и j меняются местами. Затем алгоритм рекурсивно вызывается для двух подмассивов, до и после опорной точки. Процесс повторяется до тех пор, пока массив не будет полностью отсортирован.\\r\\n\\r\\n\\r\\nТема: Сравнение и сортировка чисел в Python\\r\\nНа уроке лектор рассказывает о быстрой сортировке (Quick Sort) и ее сложности. Он объясняет, что в начале сравниваются два элемента списка, и если первый элемент меньше второго, то они меняются местами. Затем процесс повторяется для каждой пары элементов, пока весь список не будет отсортирован. Лектор также упоминает о других алгоритмах сортировки, таких как сортировка слиянием и комбинированная сортировка (TeamSort), которые работают быстрее и имеют более эффективную сложность. Он также говорит о том, что следующей темой будет алгоритм поиска.\\r\\n\\r\\n\\r\\nТема: \"Бинарный поиск: принцип работы и сложность\"\\r\\nНа уроке рассказывается о бинарном поиске - алгоритме поиска элемента в отсортированном списке. Бинарный поиск основан на разделении списка на две части и последовательном сужении интервала поиска до нахождения искомого элемента. Алгоритм имеет логарифмическую сложность и работает эффективно на больших объемах данных. В примере приводится код на языке Python, который демонстрирует работу бинарного поиска.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Поиск с помощью прыжков и последовательность Фибоначчи\"\\r\\nНа уроке рассказывается о поиске элемента в последовательности с помощью алгоритма Jump Search и о последовательности чисел Фибоначчи. \\r\\nАлгоритм Jump Search основан на прыжках через определенные интервалы в последовательности. Длина интервала определяется как корень из длины последовательности, округленный до целого числа. Если длина последовательности не делится нацело на корень, то остаток отбрасывается. Алгоритм прыгает через интервалы, пропуская элементы, пока не найдет элемент, который больше или равен искомому значению. Затем алгоритм возвращает индекс найденного элемента или -1, если элемент не найден.\\r\\nЧисла Фибоначчи - это последовательность чисел, где каждое число равно сумме двух предыдущих чисел. Начальные значения последовательности - 0 и 1. Числа Фибоначчи имеют много интересных свойств и применений, включая золотое сечение в фотографии. Золотое сечение используется для создания эстетически приятных кадров, где объекты попадают на пересечение определенных рамок в сетке Фибоначчи.\\r\\n\\r\\n\\r\\nТема: \"Использование чисел Фибоначчи для поиска удачных кадров\"\\r\\nНа уроке рассказывается о том, как использовать числа Фибоначчи и золотое сечение для создания удачных кадров в фотографии. Лектор объясняет, что когда объект на снимке совпадает с спиралью Фибоначчи, получается более гармоничное и привлекательное изображение. Затем он предлагает использовать ряд Фибоначчи для определения максимального значения, от которого будем отталкиваться при поиске удачных кадров. Далее он приводит пример кода, который использует значения Фибоначчи для поиска искомого элемента в списке. Цикл выполняется до тех пор, пока значение Фибоначчи не превысит длину списка или не станет равным ей. Затем происходит поиск искомого элемента в списке с использованием значений Фибоначчи. Если значение больше искомого элемента, то значения Фибоначчи обновляются, и цикл продолжается.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Алгоритмы поиска: минус 1, плюс, F2, минус 1, плюс 2, получается 1, и i падает по первому индексу.\\r\\nНа уроке лектор рассказывает о различных алгоритмах поиска. Он начинает с алгоритма \"прыжкового поиска\", который основан на итеративном сравнении значения с элементами списка и изменении индекса в зависимости от результата сравнения. Затем он переходит к алгоритму \"экспоненциального поиска\", который является модификацией бинарного поиска и использует увеличение индекса в два раза на каждой итерации. Лектор также упоминает о том, что экспоненциальный поиск может быть ограничен диапазоном, чтобы ускорить процесс. Затем он переходит к алгоритму \"интерполяционного поиска\", который использует интерполяционную функцию для приближенного определения положения искомого значения в списке. Лектор объясняет, что каждый из этих алгоритмов имеет свои преимущества и недостатки, и выбор алгоритма зависит от конкретной задачи.\\r\\n\\r\\n\\r\\nТема: сравнение\\r\\nНа уроке лектор рассказывает о различных алгоритмах поиска в массиве данных. \\r\\nОн объясняет, что линейный поиск осуществляется путем последовательного перебора элементов массива до нахождения нужного значения. \\r\\nБинарный поиск, в свою очередь, применяется только к отсортированным массивам и заключается в делении массива пополам и сравнении искомого значения с серединным элементом. Если искомое значение больше, то поиск продолжается во второй половине массива, иначе - в первой. Джамп-поиск использует шаги фиксированной длины для перехода к ближайшему элементу, превышающему искомое значение, а затем осуществляет линейный поиск в предыдущем блоке. Интерпретационный поиск основан на интерпретации значений массива как индексов искомого значения. \\r\\nЛектор также сравнивает эффективность этих алгоритмов и объясняет, что выбор оптимального алгоритма зависит от характеристик входных данных. \\r\\nОн также упоминает, что следующий урок будет посвящен графам.\\r\\n1062_Алгоритмическое мышление_Алгоритмы поиска и сортировки.txt\\r\\nОткрыть с помощью...\\r\\n  \\r\\n\\r\\n1062_Алгоритмическое мышление_Алгоритмы поиска и сортировки.txt. На экране.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем 2 БЗ:\n",
        "db_2=load_document_text(\"https://docs.google.com/document/d/1VnnEk1-DASWaBUa5dMTeZvuAMjqgpY_K86trhCvx5TI/edit?usp=sharing\")\n",
        "db_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "RQPmBhgWT_6j",
        "outputId": "2fcb73d0-4380-44f4-dcd9-8f0f25f2dd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nВведение.\\r\\nБиологические связи.\\r\\nВначале немного истории. Когда вы впервые услышали термин сверточные\\r\\nнейронные сети, возможно, подумали о чем-то, связанном с нейронауками\\r\\nили биологией, и отчасти были правы.\\r\\n  Сверточные нейронные сети – это своеобразная имитация зрительной коры\\r\\n  мозга. Зрительная кора имеет небольшие участки клеток, которые\\r\\n  чувствительны к конкретным областям поля зрения. Эту идею в 1962 году\\r\\n  детально рассмотрели Хьюбел и Визель с помощью потрясающего\\r\\n  эксперимента (видео).\\r\\n  Они показали, что отдельные мозговые нервные клетки реагировали (или\\r\\n  активировались) только при визуальном восприятии границ определенной\\r\\n  ориентации. Например, некоторые нейроны активировались, когда\\r\\n  воспринимали вертикальные границы, а некоторые — горизонтальные или\\r\\n  диагональные.\\r\\n  Хьюбел и Визель выяснили, что все эти нейроны организованы в блоки\\r\\n  стержневой архитектуры и вместе формируют визуальное восприятие. Эту\\r\\n  идею специализированных компонентов внутри системы, которые решают\\r\\n  конкретные задачи (как клетки зрительной коры, которые ищут\\r\\n  специфические характеристики), и взяли за основу построения сверточных\\r\\n  нейронных сетей (СНС, CNN).\\r\\n\\r\\n\\r\\n\"Входы\" и \"Выходы\"\\r\\nКогда вы смотрите на происходящее вокруг, чаще всего вы можете сразу\\r\\nохарактеризовать место действия и узнать окружающие объекты. Когда вы\\r\\nсмотрите на изображение собаки, вы без труда узнаете ее (при наличии\\r\\nхарактерных особенностей, которые можно идентифицировать, например,\\r\\nлапы).\\r\\nВсе это происходит бессознательно, незаметно для разума. Это и есть\\r\\nнавыки быстрого распознавания шаблонов и обобщения уже полученных\\r\\nзнаний.\\r\\nКогда компьютер \"видит\" изображение (принимает данные на вход), он имеет\\r\\nдело с массивом чисел от 0 до 255, описывающих интенсивность каждого\\r\\nпикселя.\\r\\nРазмер массива чисел зависит от разрешения и размера изображения:\\r\\n-   для цветного (RGB, 3 цвета) с размерами картинки 32 x 32 - это 32х32х3.\\r\\n-   для монохромного (Grey, 1 цвет) с размерами картинки 480 x 480 - это 480х480х1.\\r\\nЗначения пикселей, визуально оставаясь бессмысленными для вас,\\r\\nстановятся единственными вводными данными об изображении, доступными\\r\\nнейросети. Вы подаете ей на вход эту матрицу, а на выходе получаете\\r\\nзначения вероятностей принадлежности картинки (входа) к тому или иному\\r\\nклассу. Например, если классов три, то вы можете получить список [0.80,\\r\\n0.15, 0.05], что означает уверенность нейросети в 80%, что это кошка;\\r\\n15%, что это собака; 5%, что это птица.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСтруктура сверточной нейронной сети\\r\\nСферы применения сверточных нейронных сетей включают:\\r\\n-   работу с изображениями: это великолепный способ структурирования\\r\\n    визуальной информации, построения так называемой инвариантности -\\r\\n    выделения постоянных признаков. Например, возьмём идею распознавания\\r\\n    образов на сцене или фотографии: вы хотите понять, изображён человек\\r\\n    на ней или нет. Неважно, в какой части фотографии он располагается,\\r\\n    где его голова — в центре фотографии или в углу. Здесь инвариантный\\r\\n    (неизменный) признак - голова, которая выделяется и распознается\\r\\n    свёрточной нейронной сетью в разных местах изображения.\\r\\n-   распознавание объектов по фото, обработка медицинских снимков,\\r\\n    космоснимков и пр.\\r\\n-   работу с аудио и видео.\\r\\n-   применение в языковых технологиях, где специалисты используют\\r\\n    глубокое обучение для извлечения смысла и генерации предложений на\\r\\n    естественном языке.\\r\\nЗадача нейронщика в том, чтобы правильно подобрать архитектуру и научить\\r\\nнейросеть распознавать уникальные особенности объектов (аналогично мозгу\\r\\nчеловека), и на этом основании классифицировать все подаваемые на вход\\r\\nнейросети изображения.\\r\\nНейросеть может классифицировать изображения через поиск характеристик\\r\\nбазового уровня, например, границ и кривых. Следующий сверточный слой\\r\\nбудет анализировать уже не исходное изображение, а найденные объекты\\r\\n(границы и кривые), обнаруживая признаки более высокого уровня: контуры,\\r\\nфигуры и т.п. Таким образом, имея группу сверточных слоев, нейросеть\\r\\nсможет выдать требуемый для нас результат, проведя серию операций по\\r\\nобнаружению определенных элементов изображения (при этом каждый новый\\r\\nслой будет искать признаки более высокого уровня, опираясь на результаты\\r\\nодного или нескольких предыдущих слоев).\\r\\n\\r\\n\\r\\nТаково общее описание работы сверточных нейронных сетей. Теперь вы\\r\\nможете углубиться в их структуру.\\r\\nСравнение полносвязных и сверточных сетей\\r\\nСвёрточные нейронные сети очень похожи на обычные полносвязные нейронные\\r\\nсети, которые вы изучали ранее: они состоят из нейронов, которые, в свою\\r\\nочередь, содержат изменяемые в процессе обучения веса и смещения.\\r\\nКаждый нейрон получает какие-то входные данные, вычисляет их скалярное\\r\\nпроизведение с весами и, опционально, использует нелинейную функцию\\r\\nактивации.\\r\\nВся сеть по-прежнему представляет собой единственную дифференцируемую\\r\\nфункцию: от исходного набора пикселей (изображения) на входе до\\r\\nраспределения вероятностей принадлежности к определённому классу на\\r\\nвыходе.\\r\\n\\r\\n\\r\\nУ этих сетей по-прежнему есть функция-классификатор (например, Softmax)\\r\\nв последнем (полносвязном) слое, и все те советы и рекомендации, которые\\r\\nбыли даны к обычным нейронным сетям, применимы и для свёрточных\\r\\nнейронных сетей.\\r\\nТак что же изменилось? Архитектура свёрточных нейронных сетей явно\\r\\nпредполагает получение на входе изображений, что позволяет нам учесть\\r\\nопределённые свойства входных данных в самой архитектуре сети.\\r\\nОбычные нейронные сети плохо масштабируются для больших изображений. В\\r\\nнаборе данных CIFAR-10, например, изображения имеют размер 32х32х3 (32\\r\\nпикселя высота, 32 пикселя ширина, 3 цветовых канала).\\r\\nДля обработки такого изображения полносвязный нейрон в первом скрытом\\r\\nслое обычной нейронной сети будет иметь 32х32х3 = 3072 весов. Такое\\r\\nколичество всё ещё является допустимым, но становится очевидным тот\\r\\nфакт, что подобная структура не будет работать с изображениями большего\\r\\nразмера. Например, для изображения большего размера — 200х200х3 —\\r\\nколичество весов станет равным 200х200х3 = 120000.\\r\\nБолее того, нам понадобится не один подобный нейрон, поэтому общее\\r\\nколичество параметров модели (весов) начнет расти очень быстро.\\r\\nСтановится очевидным тот факт, что полносвязность избыточна, и слишком\\r\\nбольшое количество параметров быстро приведет к переобучению.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСверточный слой Conv2D.\\r\\nСуществуют одномерные, двумерные и даже трехмерные сверточные слои. В\\r\\nданном уроке будут рассмотрены двумерные сверточные слои.\\r\\nДля моделей нейронных сетей, построенных на основе класса Sequential,\\r\\nдля создания слоя используйте конструкцию:\\r\\n        .add(Conv2D(32, (3, 3), padding=\\'same\\', activation=\\'relu\\', strides=(1,1))\\r\\n1.  Первый параметр 32 - количество ядер (фильтров) свертки.\\r\\n2.  Второй параметр (3, 3) – размер ядра свертки.\\r\\n3.  Третий параметр padding=\\'same\\' - тип заполнения краев, нужен для\\r\\n    сохранения размеров изображения, по умолчанию значение padding=\\'valid\\'.\\r\\n4.  Четвертый параметр activation=\\'relu\\' – указание функции активации.\\r\\n5.  Пятый параметр strides=(1, 1) – необязательный, задает шаг смещения\\r\\n    фильтра (подробнее в следующем разделе).\\r\\n------------------------------------------------------------------------\\r\\nДополнительная информация (База знаний УИИ - «Функции активации») Ссылка: https://colab.research.google.com/drive/1pGc7CFdrkKBhcXLqZNUzLXH4N83rRAl7?usp=sharing\\r\\n-----------------------------------------------------------------------\\r\\nДополнительная информация (База знаний УИИ - «Сверточный слой Conv2D») Ссылка: https://colab.research.google.com/drive/1bQNGBTEqen_QiYBDGqQ5Uu3iqet_G5ps?usp=sharing\\r\\n------------------------------------------------------------------------\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nРазличия полносвязного и сверточного слоев.\\r\\nПринципиальное различие между полносвязным и сверточным слоем состоит в\\r\\nтом, что первый изучает глобальные характеристики изображения целиком, а\\r\\nвторой - локальные шаблоны в небольших фрагментах изображения.\\r\\nМинус полносвязного слоя в том, что:\\r\\n-   он обладает низкой вариативностью. То, что мы будем подавать на\\r\\n    вход, должно быть практически идентично тому, на чем обучали, даже\\r\\n    смещение объекта на листе уже критично. Локализованные признаки он\\r\\n    не обнаружит.\\r\\n-   при использовании полносвязного слоя каждый нейрон связан с каждым\\r\\n    нейроном предыдущего слоя, т.е. каждый нейрон анализирует все\\r\\n    изображение и может найти ложные зависимости.\\r\\nВариант решения: создаем \"маленький Dense\" и им пробегаем по всему\\r\\nизображению в поисках совпадения. Это позволит обнаружить объект, даже\\r\\nесли он уменьшен или смещен.\\r\\nПринцип работы сверточного слоя\\r\\nОсновная задача сверточного слоя – выделить признаки во входном\\r\\nизображении ядрами свертки (фильтрами) и сформировать карту признаков в\\r\\nвиде тензора.\\r\\nКарта признаков или Карта активации - это обработанное ядром свертки исходное изображение.\\r\\nЯдра свертки (фильтры) - это тензоры одного размера (задается при настройке слоя). Количество ядер в слое определяет глубину выходного массива (т.е. количество ядер вместе с разрешением изображения определяют форму выходного тензора).\\r\\nСвёртка – это операция вычисления нового значения на основе значения\\r\\nвыбранного пикселя и значений окружающих его пикселей.\\r\\n------------------------------------------------------------------------\\r\\nАлгоритм свёртки можно описать так:\\r\\n-   фильтр накладывается на левую верхнюю часть входящего изображения;\\r\\n-   производится поэлементное умножение значений фильтра и значений\\r\\n    пикселей изображения;\\r\\n-   полученные значения складываются, сумма будет результатом свертки\\r\\n    области наложения (одно число);\\r\\n-   фильтр перемещается дальше по изображению (за смещение отвечает\\r\\n    параметр strides, по умолчанию равен (1,1), т.е. смещение на 1\\r\\n    пиксель вправо, при достижении конца строки сдвиг вниз на 1 пиксель,\\r\\n    и вновь с начала строки);\\r\\n-   в новом положении окна фильтра производится поэлементное умножение\\r\\n    значений фильтра ...\\r\\n-   повтор до тех пор, пока аналогичным образом не будут обработаны все\\r\\n    участки.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nЗаполнение (Padding).\\r\\nКогда мы выбираем достаточно малый размер ядра фильтра, потеря в\\r\\nразмерах выходного слоя невелика. Если же это значение увеличивается, то\\r\\nи размеры выходного слоя уменьшается по отношению к оригинальному\\r\\nразмеру входного изображения. Это может стать проблемой, особенно если в\\r\\nмодели нейронной сети подключено последовательно много сверточных слоев.\\r\\nЧтобы избежать потерь в разрешении выходных изображений, в настройках\\r\\nсверточных слоев используют дополнительный параметр – заполнение\\r\\n(padding), позволяющий расширить полученное изображение по краям до того\\r\\nже размера, что и на входе свертки.\\r\\nПример: если мы по картинке размером 5 x 5 пикселей проходим ядром 3 x\\r\\n3, то в конце получим потерю в размерах.\\r\\nРешение: добавляем параметр padding=\\'same\\', тогда Keras дополнит\\r\\nвыходное изображение по внешней границе нулями (другие числа исказили бы\\r\\nинформацию, а нули дают только изменение размера), расположит их так,\\r\\nчтобы выходной массив не потерял в размерах. Заполнение распределяется\\r\\nпо границам исходя из того, как много потерь нужно возместить.\\r\\n\\r\\n\\r\\nРазмеры ядра свертки.\\r\\nСвёрточные нейронные сети используют допущение, что входные данные —\\r\\nизображения, поэтому они образуют более чувствительную архитектуру к\\r\\nподобному типу данных.\\r\\nВ частности, в отличие от обычных нейронных сетей, слои в свёрточной\\r\\nнейронной сети располагают ядра свертки в трех измерениях — ширине,\\r\\nвысоте, глубине (здесь термин \"глубина\" относится к третьему измерению\\r\\nядер, а не глубине самой нейронной сети, измеряемой в количестве слоёв).\\r\\nНапример, входные изображения из набора данных CIFAR-10 являются\\r\\nвходными данными в 3D-представлении, форма которых равна 32х32х3\\r\\n(ширина, высота, глубина). Как мы увидим позднее, нейроны в одном слое\\r\\nбудут связаны с небольшим количеством нейронов предыдущего слоя, вместо\\r\\nтого чтобы быть связанными с ними всеми.\\r\\nТаким образом, глубина фильтров первого слоя совпадает с количеством\\r\\nканалов входного изображения. Если на вход свёрточному слою подаётся RGB\\r\\nизображение (3 канала) и требуется получить 32 карты признаков, то\\r\\nсвёрточный слой должен содержать в себе 32 фильтра глубиной 3.\\r\\n\\r\\n\\r\\nВеса фильтра, который пробегает по изображению, не изменяются во время\\r\\nсамого прохождения, но обучаются от батча к батчу. В результате обучения\\r\\nфильтр начинает отвечать за поиск определенного признака. в конце концов\\r\\nодно ядро будет выделять горизонтальную прямую, другое – вертикальную,\\r\\nтретье – диагональ и т.п.\\r\\nДля обработки изображений (не только нейросетями) нередко требуется\\r\\nрешать конкретные задачи: выделять границы, повышать резкость или\\r\\nприменять размытие.\\r\\nДля подобных целей были получены сверточные фильтры, которые сегодня\\r\\nчасто встраиваются в различные графические редакторы.\\r\\nСамые распространенные размеры ядра двумерных сверточных слоев – 3х3,\\r\\n5х5 и 7х7. Количество ядер, как правило, кратно двум – 8, 16, 32, 64 и\\r\\nт.д. Но никто не запрещает использовать ядро 3х7 или 27х27 и количество\\r\\nядер 315.\\r\\nНапример, такие ядра свертки используются в фильтрах Photoshop.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСлой подвыборки (Pooling-слой).\\r\\nОсновные задачи слоев типа Pooling:\\r\\n1.  Распознавание объектов вне зависимости от масштаба;\\r\\n2.  Факт наличия признака важнее знания места его точного положения на\\r\\n    изображении.\\r\\nЭтот слой немного похож на сверточный, поскольку у него тоже есть ядро\\r\\n(окно фильтра). Но, в отличие от сверточного слоя, он уменьшает размер\\r\\nизображения, выбирая максимальное (MaxPooling), среднее (AveragePooling)\\r\\nили суммарное (SumPooling) значение из окна фильтра. В некотором смысле\\r\\nслой подвыборки делает информацию более сконцентрированной, обобщенной.\\r\\nСлой подвыборки имеет один обязательный параметр — pool_size (размер\\r\\nокна подвыборки), и один необязательный - strides (шаг смещения окна).\\r\\nПричем если strides не указан, то по умолчанию strides=pool_size, то\\r\\nесть окно смещается на размер фильтра.\\r\\nНаиболее часто используется слой MaxPooling с ядром (2, 2) и смещением\\r\\nпо умолчанию - он уменьшает размеры входного тензора по ширине и высоте\\r\\nв два раза. Поэтому рекомендуется использовать такие разрешения\\r\\nизображений, чтобы размеры по обоим измерениям делились на 2 без\\r\\nостатка, иначе при уменьшении размеров часть информации будет потеряна.\\r\\n  Если \"сжать\" слоем MaxPooling изображение размером 15 x 15, на выходе\\r\\n  получим 7 x 7. А при \"разжатии\" изображения слоем Upsampling или\\r\\n  Conv2DTranspose (выполняют что-то вроде обратной к MaxPooling\\r\\n  операции) получим 14 x 14, что не совпадет с исходными размерами.\\r\\nНекоторые библиотеки позволяют задавать раздельные параметры уменьшения\\r\\nпо высоте и ширине, создавать прямоугольное ядро подвыборки. Однако чаще\\r\\nвсего оно квадратное.\\r\\nЧтобы добавить слой, используйте:\\r\\n    .add(MaxPooling2D(pool_size=(2, 2)), где (2,2) – размер окна, в котором выбирается максимальное значение.\\r\\n\\r\\n\\r\\nФинальная классификация данных.\\r\\nПосле прохождения через сверточные слои и слои подвыборки данные\\r\\nнеобходимо систематизировать и классифицировать. Для этого применяют\\r\\nполносвязные слои Dense, которые вы изучили на предыдущих уроках.\\r\\nЧтобы правильно передать данные от сверточного слоя на полносвязный,\\r\\nнужно сделать данные одномерными. Для данной задачи подходят слои:\\r\\n-   Flatten() - \"сплющивает\" многомерные входные данные в одномерный\\r\\n    вектор, при этом размеры данных по всем осям перемножаются;\\r\\n    дополнительных параметров нет. Например, входящее изображение формы\\r\\n    (28, 28, 3) преобразуется в вектор формы (2352).\\r\\n-   Reshape(...) - смена формы данных. Требуется указать в скобках\\r\\n    желаемую форму данных. Позволяет не только вытягивать данные в\\r\\n    вектор, но и произвольно менять форму, например (28, 28, 3)\\r\\n    преобразовать в (3, 784), или в (14, 14, 12). Объем формы данных\\r\\n    (произведение размеров по всем осям) на входе должен совпадать с\\r\\n    объемом желаемой формы данных.\\r\\nВ зависимости от задачи, выходной Dense-слой может вычислять вероятности\\r\\nдля каждого класса или выдавать номер (метку) класса.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСоздание простой модели сверточной нейронной сети.\\r\\nПодключите основу – класс создания последовательной модели Sequential:\\r\\n    from tensorflow.keras.models import Sequential\\r\\nС помощью него создайте экземпляр вашей модели:\\r\\n    model = Sequential()\\r\\nЭто и есть ваша модель! Сейчас она больше похожа на пустую коробку.\\r\\nЧтобы она что-то делала, нужно поместить в нее какой-нибудь механизм.\\r\\nЭто не механизм в обычном смысле слова, потому что вы будете оперировать\\r\\nне предметами, а информацией – главным ресурсом XXI века. Механизм будет\\r\\nпринимать на вход и выдавать на выход какие-то данные.\\r\\nТак из чего же вы можете создать механизм? Для начала определитесь,\\r\\nсколько информации вы будете давать нейросети на вход. Один экземпляр\\r\\nтакой информации называется объектом. Не углубляйтесь пока, какими они\\r\\nбывают и как устроены. Сейчас достаточно знать, что объекты всегда\\r\\nсостоят из чисел.\\r\\nНапример, вы решили, что ваши объекты - изображения. Для подачи в\\r\\nнейросеть их надо оцифровать.\\r\\nУ изображений есть высота img_height, ширина img_width и количество\\r\\nцветовых каналов channels.\\r\\nИх называют входной формой (или формой входных данных) и записывают как:\\r\\n    input_shape=(img_height, img_width, channels)\\r\\n------------------------------------------------------------------------\\r\\nВажно: все изображения, которые вы подаете на вход нейронной сети,\\r\\nдолжны иметь общие высоту, ширину и количество каналов. Ниже вы узнаете,\\r\\nкак это сделать.\\r\\n------------------------------------------------------------------------\\r\\nДобавьте в модель первый слой при помощи .add():\\r\\n    from tensorflow.keras.layers import Conv2D\\r\\n    # Первый сверточный слой\\r\\n    model.add(Conv2D(8, (3, 3), padding=\\'same\\', activation=\\'relu\\', input_shape=(28, 56, 3)))\\r\\nРасшифруем написанное выше: первый сверточный слой принимает на вход\\r\\nцветное изображение (3 канала) размерами 28 на 56 пикселей. То есть\\r\\nформа входящего массива - (28, 56, 3).\\r\\nВнутри слоя к нему применяется свертка 8-ю фильтрами (3, 3) с шагом смещения (1, 1), а затем функция активации relu.\\r\\nКакой формы получится выходной массив?\\r\\nОбратите внимание, что padding =\\'same\\', stride=(1,1) по умолчанию;\\r\\nвычислим pad = (size - 1) / 2 = (3-1) /2 = 1.\\r\\nИспользуем формулу:\\r\\n    output_h = (input_h + 2 * pad - size) // stride + 1 = (28 + 2 * 1 - 3) // 1 + 1 = 27 + 1 = 28\\r\\n    output_w = (input_w + 2 * pad - size) // stride + 1 = (56 + 2 * 1 - 3) // 1 + 1 = 55 + 1 = 56\\r\\nКак видите, при стандартном stride и padding =\\'same\\' длина и ширина\\r\\nвходного и выходного массивов сверточного слоя равны. Но отличие все-таки будет - это глубина!\\r\\nНа вход пришло 3 канала, а сверточный слой имеет 8 ядер свертки, каждое\\r\\nиз которых выдает свою карту признаков, обработав любое количество\\r\\nканалов. Значит, глубина на выходе будет 8 вместо 3. Полная форма данных\\r\\nна выходе получится (28, 56, 8).\\r\\nПроверьте это методом модели .summary():\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 224\\r\\n    Trainable params: 224\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nОтлично!\\r\\nПосмотрим, что будет, если добавить следующий сверточный слой c 5\\r\\nфильтрами с ядром (3, 2), шагом смещения (2, 3) и padding =\\'valid\\':\\r\\n    # Второй сверточный слой\\r\\n    model.add(Conv2D(5, (3, 2), strides = (2,3), padding=\\'valid\\', activation=\\'relu\\'))\\r\\nЕсли padding=\\'valid\\', то по правилам pad = 0. Подставим значения:\\r\\n        output_h = (input_h + 2 * pad - size) // stride + 1 = (28 + 2 * 0 - 3) // 2 + 1 = 25 // 2 + 1 = 12 + 1 = 13\\r\\n        output_w = (input_w + 2 * pad - size) // stride + 1 = (56 + 2 * 0 - 2) // 3 + 1 = 54 // 3 + 1 = 18 + 1 = 19\\r\\nУ вас 5 фильтров, значит форма данных на выходе получится (13, 19, 5).\\r\\nПроверьте:\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nПримените слой MaxPooling2D:\\r\\n    from tensorflow.keras.layers import MaxPooling2D \\r\\n    # Слой подвыборки\\r\\n    model.add(MaxPooling2D(pool_size=(3, 3)))\\r\\nMaxPooling2D изменит форму данных следующим образом (учитывая, что\\r\\nstride=pool_size):\\r\\n     output_h = (input - pool_size) // strides + 1 = (13 - 3) // 3 + 1 = 4 \\r\\n     output_w = (input - pool_size) // strides + 1 = (19 - 3) // 3 + 1 = 6\\r\\nГлубина в MaxPooling2D не меняется, выходная форма данных (4, 6, 5).\\r\\nПроверьте:\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nДалее примените слой Flatten, который вытягивает входящий тензор в\\r\\nодномерный вектор. На входе слоя ожидается тензор (4, 6, 5), а на выходе\\r\\nбудет вектор (4 * 6 * 5) = (120)\\r\\n    from tensorflow.keras.layers import Flatten\\r\\n    # Слой преобразования многомерных данных в одномерные \\r\\n    model.add(Flatten())\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n     flatten (Flatten)           (None, 120)               0         \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nНужно обратить внимание, что размерность первых двух элементов тензора\\r\\nперед Flatten слоем (в этом случае, 4 х 6) не должна быть очень\\r\\nбольшая - можно добавлять слои Conv2D и MaxPooling2D пока эта\\r\\nразмерность не станет равна 1 х 1. Например, если бы мы не добавили слой\\r\\nMaxPooling2D, эта размерность была бы равна 13 х 19, что бы было уже\\r\\nоднозначно много.\\r\\nЕсли размерность, которая мы подаем в Flatten() слой слишком большая,\\r\\nследующий слой не сможет извлечь достаточно значемые признаки для\\r\\nправильной классификации, потому что на вход слоя приходят слишком много\\r\\nданных.\\r\\n------------------------------------------------------------------------\\r\\nДалее мы создадим последний, выходной слой Dense. Он получит на вход\\r\\nодномерный вектор (120), а на выходе выдаст одномерный вектор (3).\\r\\nАктивационная функция softmax выдаст вероятности принадлежности входных\\r\\nданных к каждому из трех классов.\\r\\nПроверьте:\\r\\n    from tensorflow.keras.layers import Dense\\r\\n    model.add(Dense(3, activation=\\'softmax\\'))\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n     flatten (Flatten)           (None, 120)               0         \\r\\n                                                                     \\r\\n     dense (Dense)               (None, 3)                 363                                                                           \\r\\n    =================================================================\\r\\n    Total params: 832\\r\\n    Trainable params: 832\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nВот вы и построили простую сверточную сеть. Теперь у вас есть небольшой\\r\\nопыт, который пригодится для построения более сложной сети, чтобы решить\\r\\nзадачу классификации.\\r\\nПрактический ноутбук 1 - ссылка: https://colab.research.google.com/drive/1wR8vbQ1LSJy1ZER3wIgXENUT0K-Q15Qh?usp=sharing\\r\\nПрактический ноутбук 2 - ссылка: https://colab.research.google.com/drive/1f95JuEQcz2LYrCbWS3GnvnXUSN2GL2HR?usp=sharing\\r\\n\\r\\n\\r\\n761_1_Базовый_блок___Сверточные_нейронные_сети_(Теория)___УИИ.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Делим на чанки 1 БЗ:\n",
        "db_1_chunks=[]\n",
        "splitter = CharacterTextSplitter(separator='<Chunk>')\n",
        "for chunk in splitter.split_text(db_1):\n",
        "    db_1_chunks.append(Document(page_content=chunk, metadata={\"meta\":\"data\"}))\n",
        "print(\"Общее количество чанков: \", len(db_1_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATgHacbwT_9P",
        "outputId": "1a9f5dca-6b9c-4996-8e64-094f106943cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Общее количество чанков:  15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Делим на чанки 2 БЗ:\n",
        "db_2_chunks=[]\n",
        "splitter = CharacterTextSplitter(separator='<Chunk>')\n",
        "for chunk in splitter.split_text(db_2):\n",
        "    db_2_chunks.append(Document(page_content=chunk, metadata={\"meta\":\"data\"}))\n",
        "print(\"Общее количество чанков: \", len(db_2_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq0TMwvT__Y",
        "outputId": "574bd281-9f3e-47aa-86cb-11e68a9b8d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Общее количество чанков:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем 2 векторные базы:\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db_1_faiss = FAISS.from_documents(db_1_chunks, embeddings)\n",
        "db_2_faiss = FAISS.from_documents(db_2_chunks, embeddings)"
      ],
      "metadata": {
        "id": "8oauvANbUACX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Объединяем две векторные базы:"
      ],
      "metadata": {
        "id": "5ERdpCfidzlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_db = FAISS.from_documents(db_1_faiss.docstore._dict.values(), embeddings)  # создаете новую базу данных на основе db1\n",
        "merged_db.merge_from(db_2_faiss)  # добавляете данные из db2"
      ],
      "metadata": {
        "id": "Xbobo28qzwPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы успешно объединили базы данных db_1_faiss и db_2_faiss в merged_db. Теперь merged_db содержит документы из обеих баз данных. Каждый документ представлен словарем, где ключом является уникальный идентификатор, а значением - объект Document, содержащий контент страницы и метаданные."
      ],
      "metadata": {
        "id": "jIWwUQWH2SfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# вот так можно посмотреть на объединенную базу (на все чанки)\n",
        "merged_db.docstore._dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqCKy85jcgWk",
        "outputId": "00810a98-7a64-4f6b-ba20-4703129fc0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a5196dfa-0c0e-491d-bca0-2dde0b0045a8': Document(metadata={'meta': 'data'}, page_content='\\ufeff<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСсылка https://colab.research.google.com/drive/14n-IdIQlE8mlQhA88vhxXDcC2VcuDIIt?usp=sharing\\r\\nАлгоритмы сортировки\\r\\nСуществуют десятки алгоритмов сортировки. Разные алгоритмы оптимальны\\r\\nдля разных наборов и типов данных. Мы рассмотрим некоторые из них:\\r\\n-   Пузырьковая сортировка\\r\\n-   Сортировка выборкой\\r\\n-   Сортировка вставками\\r\\n-   Сортировка слиянием\\r\\n-   Быстрая сортировка\\r\\n\\r\\n\\r\\nПузырьковая сортировка\\r\\nПузырьковая сортировка или сортировка простыми обменами – один из\\r\\nпростейших алгоритмов сортировки. Он применяется для упорядочивания\\r\\nмассивов небольших размеров.\\r\\n    # Алгоритм пузырьковой сортировки\\r\\nСуть алгоритма в том, что совершается несколько проходов по массиву. При\\r\\nкаждом проходе попарно сравниваются два соседних элемента. Если они\\r\\nнаходятся в верном порядке, то ничего не происходит, в противном случае\\r\\nони меняются местами. В результате первого прохода максимальный элемент\\r\\nокажется в конце, то есть всплывет словно пузырек. Затем все повторяется\\r\\nдо того момента пока весь массив не будет отсортирован.\\r\\n    def bubble_sort(nums):\\r\\n        # Устанавливаем swapped в True, чтобы цикл запустился хотя бы один раз\\r\\n        swapped = True\\r\\n        while swapped:\\r\\n            swapped = False\\r\\n            for i in range(len(nums) - 1):\\r\\n                if nums[i] > nums[i + 1]:\\r\\n                    # Меняем элементы\\r\\n                    nums[i], nums[i + 1] = nums[i + 1], nums[i]\\r\\n                    # Устанавливаем swapped в True для следующей итерации\\r\\n                    swapped = True\\r\\n\\r\\n\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    bubble_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма пузырьковой сортировки\\r\\nВ алгоритме пузырьковой сортировки есть два вложенных цикла while и for.\\r\\nЕсли взять самый худший случай (изначально список отсортирован по\\r\\nубыванию), то сложность алгоритма будет квадратичной:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка выборкой\\r\\nЭтот алгоритм сегментирует список на две части: отсортированные и\\r\\nнесортированные. Он постоянно удаляет наименьший элемент из\\r\\nнесортированного сегмента списка и добавляет его в отсортированный\\r\\nсегмент.\\r\\n    # Алгоритм сортировки выборкой\\r\\nНа практике нам не нужно создавать новый список для отсортированных\\r\\nэлементов, мы будем обрабатывать крайнюю левую часть списка как\\r\\nотсортированный сегмент. Затем мы ищем во всем списке наименьший элемент\\r\\nи меняем его на первый элемент.\\r\\nТеперь мы знаем, что первый элемент списка отсортирован, мы получаем\\r\\nнаименьший элемент из оставшихся элементов и заменяем его вторым\\r\\nэлементом. Это повторяется до тех пор, пока последний элемент списка не\\r\\nстанет оставшимся элементом для изучения.\\r\\n    def selection_sort(nums):\\r\\n        # Значение i соответствует кол-ву отсортированных значений\\r\\n        for i in range(len(nums)):\\r\\n            # Исходно считаем наименьшим первый элемент\\r\\n            lowest_value_index = i\\r\\n            # Этот цикл перебирает несортированные элементы\\r\\n            for j in range(i + 1, len(nums)):\\r\\n                if nums[j] < nums[lowest_value_index]:\\r\\n                    lowest_value_index = j\\r\\n            # Самый маленький элемент меняем с первым в списке\\r\\n            nums[i], nums[lowest_value_index] = nums[lowest_value_index], nums[i]\\r\\n\\r\\n\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    selection_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки выборкой\\r\\nВ этом алгоритме два вложенных цикла for, следовательно сложность\\r\\nквадратичная:\\r\\n     O(n²), где n — количество элементов списка.'),\n",
              " '4562a89f-64ed-457e-8ccb-1d2bb394fd1b': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка вставками\\r\\nКак и сортировка выборкой, этот алгоритм делит список на отсортированные\\r\\nи несортированные части. Он перебирает неотсортированный сегмент и\\r\\nвставляет просматриваемый элемент в правильную позицию отсортированного\\r\\nсписка.\\r\\n    # Алгоритм сортировки вставками\\r\\nПредполагается, что первый элемент списка отсортирован. Переходим к\\r\\nследующему элементу, обозначим его х. Если х больше первого, оставляем\\r\\nего на своём месте. Если он меньше, копируем его на вторую позицию, а х\\r\\nустанавливаем как первый элемент.\\r\\nПереходя к другим элементам несортированного сегмента, перемещаем более\\r\\nкрупные элементы в отсортированном сегменте вверх по списку, пока не\\r\\nвстретим элемент меньше x или не дойдём до конца списка. В первом случае\\r\\nx помещается на правильную позицию.\\r\\n    def insertion_sort(nums):\\r\\n        # Сортировку начинаем со второго элемента, т.к. считается, что первый элемент уже отсортирован\\r\\n        for i in range(1, len(nums)):\\r\\n            item_to_insert = nums[i]\\r\\n            # Сохраняем ссылку на индекс предыдущего элемента\\r\\n            j = i - 1\\r\\n            # Элементы отсортированного сегмента перемещаем вперёд, если они больше\\r\\n            # элемента для вставки\\r\\n            while j >= 0 and nums[j] > item_to_insert:\\r\\n                nums[j + 1] = nums[j]\\r\\n                j -= 1\\r\\n            # Вставляем элемент\\r\\n            nums[j + 1] = item_to_insert\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    insertion_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки вставками\\r\\nТак же, как и предыдущие 2 алгоритма этот имеет 2 вложенных цикла for и\\r\\nwhile, значит сложность тоже квадратичная:\\r\\n     O(n²), где n — количество элементов списка.'),\n",
              " '345a827d-3cb0-4612-b1b1-8bfcf389cbc1': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка слиянием\\r\\nЭтот алгоритм разбивает список на две части, каждую из них он разбивает\\r\\nещё на две и т. д. Список разбивается пополам, пока не останутся\\r\\nединичные элементы.\\r\\nСоседние элементы становятся отсортированными парами. Затем эти пары\\r\\nобъединяются и сортируются с другими парами. Этот процесс продолжается\\r\\nдо тех пор, пока не отсортируются все элементы.\\r\\n    # Алгоритм сортировки слиянием\\r\\nСписок рекурсивно разделяется пополам, пока в итоге не получатся списки\\r\\nразмером в один элемент. Массив из одного элемента считается\\r\\nупорядоченным. Соседние элементы сравниваются и соединяются вместе. Это\\r\\nпроисходит до тех пор, пока не получится полный отсортированный список.\\r\\nСортировка осуществляется путём сравнения наименьших элементов каждого\\r\\nподмассива. Первые элементы каждого подмассива сравниваются первыми.\\r\\nНаименьший элемент перемещается в результирующий массив. Счётчики\\r\\nрезультирующего массива и подмассива, откуда был взят элемент,\\r\\nувеличиваются на 1.\\r\\n    def merge(left_list, right_list):\\r\\n        sorted_list = []\\r\\n        left_list_index = right_list_index = 0\\r\\n        # Длина списков часто используется, поэтому создадим переменные для удобства\\r\\n        left_list_length, right_list_length = len(left_list), len(right_list)\\r\\n        for _ in range(left_list_length + right_list_length):\\r\\n            if left_list_index < left_list_length and right_list_index < right_list_length:\\r\\n                # Сравниваем первые элементы в начале каждого списка\\r\\n                # Если первый элемент левого подсписка меньше, добавляем его\\r\\n                # в отсортированный массив\\r\\n                if left_list[left_list_index] <= right_list[right_list_index]:\\r\\n                    sorted_list.append(left_list[left_list_index])\\r\\n                    left_list_index += 1\\r\\n                # Если первый элемент правого подсписка меньше, добавляем его\\r\\n                # в отсортированный массив\\r\\n                else:\\r\\n                    sorted_list.append(right_list[right_list_index])\\r\\n                    right_list_index += 1\\r\\n            # Если достигнут конец левого списка, элементы правого списка\\r\\n            # добавляем в конец результирующего списка\\r\\n            elif left_list_index == left_list_length:\\r\\n                sorted_list.append(right_list[right_list_index])\\r\\n                right_list_index += 1\\r\\n            # Если достигнут конец правого списка, элементы левого списка\\r\\n            # добавляем в отсортированный массив\\r\\n            elif right_list_index == right_list_length:\\r\\n                sorted_list.append(left_list[left_list_index])\\r\\n                left_list_index += 1\\r\\n        return sorted_list\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nАлгоритм сортировки слиянием\\r\\n    def merge_sort(nums):\\r\\n        # Возвращаем список, если он состоит из одного элемента\\r\\n        if len(nums) <= 1:\\r\\n            return nums\\r\\n        # Для того чтобы найти середину списка, используем деление без остатка\\r\\n        # Индексы должны быть integer\\r\\n        mid = len(nums) // 2\\r\\n        # Сортируем и объединяем подсписки\\r\\n        left_list = merge_sort(nums[:mid])\\r\\n        right_list = merge_sort(nums[mid:])\\r\\n        # Объединяем отсортированные списки в результирующий\\r\\n        return merge(left_list, right_list)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    random_list_of_nums = merge_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки слиянием\\r\\nБлагодаря тому, что функция merge_sort() возвращает новый список, а не\\r\\nсортирует существующий, такая сортировка имеет только один цикл, но при\\r\\nэтом для такого алгоритма требуется больше памяти.\\r\\nСложность такого алгоритма является линейно-логарифмической:\\r\\n     O(n*log n), где n — количество элементов списка.'),\n",
              " 'a47c6b76-a540-4c73-9fb0-71b7794fbbba': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nБыстрая сортировка\\r\\nПри использовании этого алгоритма, массив разделяется на две части по\\r\\nразные стороны от опорного элемента. В процессе сортировки элементы\\r\\nменьше опорного помещаются перед ним, а равные или большие — позади.\\r\\n    # Алгоритм быстрой сортировки\\r\\nБыстрая сортировка начинается с разбиения списка и выбора одного из\\r\\nэлементов в качестве опорного. А всё остальное передвигаем так, чтобы\\r\\nэтот элемент встал на своё место. Все элементы меньше него перемещаются\\r\\nвлево, а равные и большие элементы перемещаются вправо.\\r\\n    def partition(nums, low, high):\\r\\n        # Выбираем средний элемент в качестве опорного\\r\\n        # Также возможен выбор первого, последнего\\r\\n        # или произвольного элементов в качестве опорного\\r\\n        pivot = nums[(low + high) // 2]\\r\\n        i = low - 1\\r\\n        j = high + 1\\r\\n        while True:\\r\\n            i += 1\\r\\n            while nums[i] < pivot:\\r\\n                i += 1\\r\\n            j -= 1\\r\\n            while nums[j] > pivot:\\r\\n                j -= 1\\r\\n            if i >= j:\\r\\n                return j\\r\\n            # Если элемент с индексом i (слева от опорного) больше, чем\\r\\n            # элемент с индексом j (справа от опорного), меняем их местами\\r\\n            nums[i], nums[j] = nums[j], nums[i]\\r\\n    def quick_sort(nums):\\r\\n        # Создадим вспомогательную функцию, которая вызывается рекурсивно\\r\\n        def _quick_sort(items, low, high):\\r\\n            if low < high:\\r\\n                # This is the index after the pivot, where our lists are split\\r\\n                split_index = partition(items, low, high)\\r\\n                _quick_sort(items, low, split_index)\\r\\n                _quick_sort(items, split_index + 1, high)\\r\\n        _quick_sort(nums, 0, len(nums) - 1)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    quick_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма быстрой сортировки\\r\\nВ худшем случае, если опорный элемент будет минимальным или максимальным\\r\\nиз списка, то сложность такого алгоритма будет квадратичной:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСравнение рассмотренных алгоритмов сортировки\\r\\nЕсли оценка сложности при помощи Big O это статистический анализ\\r\\nскорости работы алгоритма, то теперь проведем эмпирический анализ.\\r\\nВозьмём один и тот же список из 5000 значений, выполним сортировку\\r\\nразными алгоритмами и сравним время работы каждого из них.\\r\\n    import time\\r\\n    import numpy as np\\r\\n    # Создадим список длиной 5000 значений с рандомными числами от 1 до 1000\\r\\n    exp_list = np.random.randint(0, 1000, 5000)\\r\\nПузырьковая сортировка\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    bubble_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    10.612551212310791\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict = {}\\r\\n    time_dict['bubble'] = round(end, 2)\\r\\nТема: Сортировка выборкой.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    selection_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    4.216295003890991\\r\\n    # Добавим время работы алгоритма в словарь\\r\\n    time_dict['selection'] = round(end, 2)\"),\n",
              " '0f6b4f7d-0444-422d-a574-0ad5fe591495': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nПродолжение Сравнение рассмотренных алгоритмов сортировки\\r\\nТема: Сортировка вставками.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    insertion_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    2.7952945232391357\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict['insertion'] = round(end, 2)\\r\\nТема: Сортировка слиянием.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    merge_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    0.03851127624511719\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict['merge'] = round(end, 2)\\r\\nТема: Быстрая сортировка.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    quick_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    0.0353546142578125\\r\\n    # Добавим время работы алгоритма в словарь\\r\\n    time_dict['quick'] = round(end, 2)\\r\\nВыведем график скорости работы алгоритмов\\r\\n    import matplotlib.pyplot as plt\\r\\n    plt.figure(figsize=(8,6))\\r\\n    plt.bar(time_dict.keys(), time_dict.values())\\r\\n    plt.show()\\r\\nСтандартная функция сортировки python sort() использует сортировку Тима,\\r\\nкоторая представляет собой комбинацию сортировки слиянием и сортировки\\r\\nвставками.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nАлгоритмы поиска\\r\\nСамый простой пример поиска можно осуществить при помощи операторов in\\r\\nили not in.\\r\\n    # Пример поиска символа в строке\\r\\n    't' in 'qwerty'\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    True\\r\\n# Пример поиска значения в списке\\r\\n    5 in [1,2,3,4]\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    False\\r\\nМинус такого поиска в том, что мы не знаем, где именно находится искомая\\r\\nинформация (например индекс значения в списке), а знаем только то, что\\r\\nона там есть или нет.\\r\\nТема: Линейный поиск.\\r\\nАлгоритм линейного поиска очень простой. Мы просто проходим по всем\\r\\nэлементам массива по порядку и сравниваем с искомым значением.\\r\\n    # Алгоритм линейного поиска\\r\\nПлюс алгоритма линейного поиска в том, что он работает как с\\r\\nотсортированными массивами, так и с несортированными.\\r\\n    def LinearSearch(nums, val):\\r\\n        for i in range(len(nums)):\\r\\n            if nums[i] == val:\\r\\n                return i\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    result = LinearSearch(random_list_of_nums, 8)\\r\\n    print('Значение 8 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 8 имеет индекс - 4\\r\\nСложность алгоритма линейного поиска\\r\\nСложность данного алгоритма линейная, так как количество итераций\\r\\nнапрямую зависит от размера входных данных.\\r\\n     O(n), где n — количество элементов списка.\"),\n",
              " 'c950f742-459a-4062-8aca-9721a0f92bb6': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Бинарный поиск.\\r\\nБинарный поиск работает по принципу «разделяй и властвуй». Он быстрее,\\r\\nчем линейный поиск, но требует, чтобы массив был отсортирован перед\\r\\nвыполнением алгоритма.\\r\\n    # Алгоритм бинарного поиска\\r\\nРабота данного алгоритма основана на определении среднего значения. Если\\r\\nэто значение не является искомым, то нужно определить в какой половине\\r\\nмассива мы будем продолжать поиски. Если то значение, которое мы ищем\\r\\nбольше среднего, то поиски продолжатся в правой части массива, если\\r\\nменьше, то в левой. Так на каждой итерации мы сужаем область поиска,\\r\\nпостепенно двигаясь к правильному ответу.\\r\\n    def BinarySearch(nums, val):\\r\\n        first = 0\\r\\n        last = len(nums)-1\\r\\n        index = -1\\r\\n        while (first <= last) and (index == -1):\\r\\n            mid = (first+last)//2\\r\\n            if nums[mid] == val:\\r\\n                index = mid\\r\\n            else:\\r\\n                if val<nums[mid]:\\r\\n                    last = mid -1\\r\\n                else:\\r\\n                    first = mid +1\\r\\n        return index\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = BinarySearch(sorted_list_of_nums, 31)\\r\\n    print(\\'Значение 31 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 31 имеет индекс - 6\\r\\nСложность алгоритма бинарного поиска\\r\\nНа каждой итерации бинарного поиска мы делим массив на 2 части (одну из\\r\\nкоторых отбрасываем), следовательно сложность данного алгоритма\\r\\nлогарифмическая.\\r\\n     O(log n), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Jump search.\\r\\nJump search - это ещё один алгоритм поиска, который работает по принципу\\r\\n\"разделяй и властвуй\" и требует на входе отсортированный массив.\\r\\n    # Алгоритм jump search\\r\\nСуть алгоритма jump search скрыта в его названии. Вместо того, чтобы\\r\\nпроходить по каждому элементу массива, или делить его пополам, мы будем\\r\\nдвигаться с определённым шагом, как бы перепрыгивая (jump) некоторые\\r\\nэлементы. Как правило, шаг берется не случайно, а считается как\\r\\nквадратный корень из количества элементов массива. Например, для массива\\r\\nдлиной 9, шаг будет составлять 3, а для массива 15 шаг будет тоже 3\\r\\n(округление идёт до целого числа, отбрасывая остаток).\\r\\n    def JumpSearch(nums, val):\\r\\n        length = len(nums)\\r\\n        jump = int(length**0.5)\\r\\n        left, right = 0, 0\\r\\n        while left < length and nums[left] <= val:\\r\\n            right = min(length - 1, left + jump)\\r\\n            if nums[left] <= val and nums[right] >= val:\\r\\n                break\\r\\n            left += jump;\\r\\n        if left >= length or nums[left] > val:\\r\\n            return -1\\r\\n        right = min(length - 1, right)\\r\\n        i = left\\r\\n        while i <= right and nums[i] <= val:\\r\\n            if nums[i] == val:\\r\\n                return i\\r\\n            i += 1\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = JumpSearch(sorted_list_of_nums, 25)\\r\\n    print(\\'Значение 25 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 25 имеет индекс - 5\\r\\nСложность алгоритма jump search\\r\\nВременная сложность jump search равна:\\r\\n     O(√n), где √n — размер прыжка, n — количество элементов списка.'),\n",
              " 'f2f4cd12-a035-4da0-a93f-d0f8ab82b166': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Поиск Фибоначчи\\r\\nЧисла Фибоначчи — это последовательность чисел, где каждый элемент\\r\\nявляется суммой двух предыдущих чисел.\\r\\n    0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 ...\\r\\nАлгоритм поиска Фибоначчи использует числа Фибоначчи для расчета\\r\\nдиапазона поисков элемента в массиве.\\r\\nРазберем как работает этот подход.\\r\\nПусть дан список из 11 значений. Нужно найти индекс значения 28 в этом\\r\\nсписке. Так как длина списка 11, то мы ищем число в ряде Фибоначчи\\r\\nравное 11 или ближайшее большее. Число 13 является таковым.\\r\\nИтерация 1:\\r\\nДальше сдвигаемся на 2 шага назад (на первой итерации) по ряду\\r\\nФибоначчи, получаем число 5. Для расчета индекса искомого значения в\\r\\nсписке используется формула:\\r\\n     i = min(idx+a, n-1), где\\r\\n        idx - индекс полученный на предыдущем шаге (или -1 на первой итерации)\\r\\n        a - рассчитанное нами число 5\\r\\n        n - длина списка\\r\\nЗначения idx=-1, a=5. Подставляем значения в формулу и считаем:\\r\\n    i = min(-1+5, 11-1) = min(4, 10) = 4\\r\\nИндекс искомого значения на первой итерации получили 4. Переходим к\\r\\nзначению списка с индексом 4 - это значение 13. Сравниваем полученное\\r\\nзначение 13 с искомым 28. Они не равны, следовательно продолжаем поиски дальше.\\r\\nИтерация 2:\\r\\nЕсли искомое значение не найдено, важно то, больше полученное значение\\r\\nили меньше. Если значение, которое мы получили в ходе расчетов меньше,\\r\\nчем искомое, то в ряду Фибоначчи мы смещаемся на 1 шаг назад (от числа 5 переходим к 3).\\r\\nТеперь idx=4, a=3. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на второй итерации получили 7. Переходим к значению\\r\\nсписка с индексом 7 - это значение 25. Сравниваем полученное значение 25\\r\\nс искомым 28. Они не равны, следовательно продолжаем поиски дальше.\\r\\nИтерация 3:\\r\\nПолученное значение 25 снова меньше искомого 28, следовательно в ряду\\r\\nФибоначчи мы смещаемся так же на 1 шаг назад (от числа 3 переходим к 2).\\r\\nТеперь idx=7, a=2. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на третьей итерации получили 9. Переходим к значению\\r\\nсписка с индексом 9 - это значение 34. Сравниваем полученное значение 34\\r\\nс искомым 28. Они не равны, следовательно продолжаем поиски дальше.'),\n",
              " '6ed00722-40f1-40e4-8257-29941af16700': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Продолжаем Поиск Фибоначчи\\r\\nИтерация 4:\\r\\nПосле третьей итерации полученное значение оказалось больше искомого,\\r\\nпоэтому алгоритм дальнейшего поиска немного меняется. Отличие в том, что\\r\\nзначение idx для расчетов мы возьмём не последнее рассчитанное (9), а\\r\\nполученное на предыдущей итерации (7). Что касается смещения в ряду\\r\\nФибоначчи, то теперь оно на 2 шага назад (от числа 2 переходим к 1).\\r\\nТеперь idx=7, a=1. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на четвертой итерации получили 8. Переходим к значению\\r\\nсписка с индексом 8 - это значение 28.\\r\\nПолученное значение 28 сравниваем с искомым 28 - они равны,\\r\\nследовательно поиск окончен.\\r\\nИндекс искомого значения 28 равен 8.\\r\\n    def FibonacciSearch(nums, val):\\r\\n        fibM_minus_2 = 0\\r\\n        fibM_minus_1 = 1\\r\\n        fibM = fibM_minus_1 + fibM_minus_2\\r\\n        while (fibM < len(nums)):\\r\\n            fibM_minus_2 = fibM_minus_1\\r\\n            fibM_minus_1 = fibM\\r\\n            fibM = fibM_minus_1 + fibM_minus_2\\r\\n        index = -1;\\r\\n        while (fibM > 1):\\r\\n            i = min(index + fibM_minus_2, (len(nums)-1))\\r\\n            if (nums[i] < val):\\r\\n                fibM = fibM_minus_1\\r\\n                fibM_minus_1 = fibM_minus_2\\r\\n                fibM_minus_2 = fibM - fibM_minus_1\\r\\n                index = i\\r\\n            elif (nums[i] > val):\\r\\n                fibM = fibM_minus_2\\r\\n                fibM_minus_1 = fibM_minus_1 - fibM_minus_2\\r\\n                fibM_minus_2 = fibM - fibM_minus_1\\r\\n            else :\\r\\n                return i\\r\\n        if(fibM_minus_1 and index < (len(nums)-1) and nums[index+1] == val):\\r\\n            return index+1;\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = FibonacciSearch(sorted_list_of_nums, 13)\\r\\n    print('Значение 13 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 13 имеет индекс - 3\\r\\nСложность алгоритма поиска Фибоначчи\\r\\nВременная сложность данного алгоритма логарифмическая:\\r\\n     O(log n), где n — количество элементов списка.\\r\\nЭтот алгоритм в большинстве случаев работает быстрее, чем линейный поиск\\r\\nи jump search.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nЭкспоненциальный поиск\\r\\n    # Алгоритм экспоненциального поиска\\r\\nЭкспоненциальный поиск это надстройка над бинарным поиском. Его\\r\\nособенность в том, что индекс при первичном поиске элемента 2^n, а после\\r\\nиспользуется алгоритм бинарного поиска.\\r\\n    def ExponentialSearch(nums, val):\\r\\n        if nums[0] == val:\\r\\n            return 0\\r\\n        index = 1\\r\\n        while index < len(nums) and nums[index] <= val:\\r\\n            prev_index = index\\r\\n            index = index * 2\\r\\n        return BinarySearch(nums[:min(index, len(nums))], val)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39, 45, 55, 58]\\r\\n    result = ExponentialSearch(sorted_list_of_nums, 55)\\r\\n    print('Значение 55 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 55 имеет индекс - 10\\r\\nСложность алгоритма экспоненциального поиска\\r\\nВременная сложность данного алгоритма логарифмическая:\\r\\n     O(log n), где n — количество элементов списка.\"),\n",
              " '101acec8-30a5-4725-b8a7-21ee5864019b': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nЭкспоненциальный поиск работает лучше, чем бинарный, когда искомый\\r\\nэлемент находится ближе к началу массива.\\r\\nДанный алгоритм поиска считается одним из наиболее эффективных.\\r\\nИнтерполяционный поиск\\r\\nИнтерполяционный поиск является разновидностью бинарного поиска, только\\r\\nвероятную позицию (индекс) искомого элемента он вычисляет по формуле:\\r\\n    index = low + [(val-nums[low])*(high-low) / (nums[high]-nums[low])],\\r\\n    где low  - начальный индекс массива;\\r\\n        high - конечный индекс массива;\\r\\n        val  - искомое значение;\\r\\n        nums - исходный массив данных.\\r\\n    def InterpolationSearch(nums, val):\\r\\n        low = 0\\r\\n        high = (len(nums) - 1)\\r\\n        while low <= high and val >= nums[low] and val <= nums[high]:\\r\\n            index = low + int(((float(high - low) / ( nums[high] - nums[low])) * ( val - nums[low])))\\r\\n            if nums[index] == val:\\r\\n                return index\\r\\n            if nums[index] < val:\\r\\n                low = index + 1;\\r\\n            else:\\r\\n                high = index - 1;\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39, 45, 55, 58]\\r\\n    result = InterpolationSearch(sorted_list_of_nums, 45)\\r\\n    print('Значение 45 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 45 имеет индекс - 9\\r\\nСложность алгоритма интерполяционного поиска\\r\\nДанный алгоритм работает быстрее на отсортированных, равномерно\\r\\nраспределенных массивах. Сложность алгоритма на таких массивах O(log log\\r\\nn), но если значения массива не равномерно распределены, тогда сложность\\r\\nалгоритма линейная:\\r\\n     O(n), где n — количество элементов списка.\"),\n",
              " 'e0df6937-e524-4b65-9acb-b84b551f86bb': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСравнение рассмотренных алгоритмов поиска\\r\\nВ первой части ноутбука мы проводили сравнение разных алгоритмов\\r\\nсортировки при помощи библиотеки time. Для сравнения скорости работы\\r\\nалгоритмов поиска воспользуемся магической комендой %%timeit\\r\\n\\r\\n\\r\\n    import numpy as np\\r\\n    # Создадим массив из 50 рандомных значений и отсортируем его\\r\\n    random_list = np.random.randint(1,100,50)\\r\\n    random_list.sort()\\r\\n    print(random_list)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [ 1  7  8  9 10 11 11 15 15 15 16 21 22 24 24 27 30 32 33 38 41 45 49 49 52 55 58 59 77 78 79 79 81 82 82 84 84 85 85 85 87 92 93 93 93 94 95 96 98 98]\\r\\nБудем искать значение 30\\r\\n    %%timeit\\r\\n    # Линейный поиск\\r\\n    result = LinearSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    4.75 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Бинарный поиск\\r\\n    result = BinarySearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    4.32 µs ± 197 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Jump search\\r\\n    result = JumpSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    5.81 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Поиск Фибоначчи\\r\\n    result = FibonacciSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    5.61 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n    %%timeit\\r\\n        \\r\\n    # Экспоненциальный поиск\\r\\n    result = ExponentialSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    6.47 µs ± 152 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n    %%timeit\\r\\n        \\r\\n    # Интерполяционный поиск\\r\\n    result = InterpolationSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    8.84 µs ± 224 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n\\r\\n\\r\\nПостроим график средней скорости\\r\\n    import matplotlib.pyplot as plt\\r\\n\\r\\n\\r\\n    time_dict2 = {'linear':4.75,\\r\\n                  'binary':4.32,\\r\\n                  'jump':5.81,\\r\\n                  'fibonacci':5.61,\\r\\n                  'exponential':6.47,\\r\\n                  'interpolation':8.84}\\r\\n    plt.figure(figsize=(8,6))\\r\\n    plt.bar(time_dict2.keys(), time_dict2.values())\\r\\n    plt.show()\"),\n",
              " 'de5eb69f-5c27-4302-a430-9de81ebb6840': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Алгоритмы поиска и сортировки: важность и принципы работы\\r\\nНа уроке лектор рассказывает о важности алгоритмов поиска и сортировки в цифровой эпохе. Он объясняет, что алгоритмы - это рецепты или планы действий для решения задач. Алгоритмы помогают структурировать действия и делать их эффективнее. Лектор также говорит о том, что алгоритмы помогают оптимизировать ресурсы, такие как время и память, и решать задачи с большими данными быстрее и эффективнее. Он упоминает, что знание алгоритмов помогает понять работу различных библиотек и технологий в Python, таких как SQL и Docker. Лектор также объясняет, как оценивать алгоритмы, используя элементарные шаги и понятие Big O Notation, которое описывает сложность алгоритма. Он приводит примеры различных сложностей, таких как константная, линейная и логарифмическая, и объясняет, как они меняются с увеличением размера входных данных.\\r\\n\\r\\n\\r\\nТема: Квадратичная сложность и алгоритм сортировки пузырьком\\r\\nНа уроке рассказывается о сложности алгоритмов и различных методах сортировки. Лектор объясняет, что квадратичная сложность возникает, когда время выполнения алгоритма увеличивается пропорционально квадрату размера входных данных. Это происходит, например, при использовании двух вложенных циклов. Лектор также упоминает о других видах сложности, таких как кубическая и экспоненциальная.\\r\\nЗатем рассматривается пример алгоритма сортировки пузырьком, который является одним из самых простых. Лектор объясняет, как работает этот алгоритм и демонстрирует его на конкретном списке чисел. Он также обсуждает различные методы сортировки, такие как выборка, вставка, слияние и быстрая сортировка.\\r\\nВ конце урока лектор отвечает на вопросы студентов, в том числе о том, работает ли бинарный поиск только для сортированных списков. Он также объясняет, как поменять местами значения двух переменных без использования третьей переменной.\\r\\nВ целом, на уроке рассматривается тема сложности алгоритмов и методов сортировки, а также демонстрируется пример алгоритма сортировки пузырьком.'),\n",
              " '4a3f475b-029b-4487-b7b9-55d7d73d3104': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Сортировка выборкой и использование флага swap\"\\r\\nНа уроке лектор рассказывает о сортировке пузырьком и сортировке выборкой. Он объясняет, что в сортировке пузырьком используется флаг swap, который указывает, нужно ли повторно запускать цикл FOR для проверки и перестановки элементов. Если после прохождения цикла FOR по списку не было перестановок, то флаг swap становится false и цикл больше не запускается. Лектор также объясняет, что в сортировке выборкой на каждой итерации находится самый маленький элемент и ставится на первое место. Он также демонстрирует код и объясняет его работу. В конце лектор отвечает на вопросы слушателей и переходит к объяснению следующего алгоритма сортировки.\\r\\n\\r\\n\\r\\nТема: Работа флага в цикле while\\r\\nНа уроке рассказывается о сортировке списка значений с использованием флага в цикле while. Лектор объясняет, что сортировка значений может быть полезна в различных задачах и что существуют разные способы сортировки, каждый из которых может быть оптимальным для конкретной задачи. \\r\\nДалее лектор объясняет алгоритм сортировки с использованием флага. Он приводит пример списка значений и объясняет, как происходит сравнение и перестановка элементов в цикле. Лектор также отмечает, что в данном алгоритме используется переменная \"last value index\", которая хранит индекс минимального элемента, и что после каждой итерации цикла значение этой переменной изменяется. \\r\\nЛектор также отмечает, что данный алгоритм сортировки отличается от предыдущего алгоритма, который был рассмотрен ранее. Он объясняет, что в данном алгоритме происходит поиск минимального элемента и его перестановка с первым элементом списка, а затем происходит сортировка оставшейся части списка. \\r\\nЛектор также обсуждает различные алгоритмы сортировки, такие как сортировка вставками, и объясняет, что выбор алгоритма сортировки может существенно влиять на скорость выполнения программы. Он отмечает, что некоторые алгоритмы могут иметь квадратичную сложность, что может привести к значительному увеличению времени выполнения программы при большом количестве элементов в списке. \\r\\nВ конце урока лектор предлагает рассмотреть подробнее код алгоритма сортировки с использованием флага и объясняет его работу. Он также отмечает, что понимание различных алгоритмов сортировки и их оптимизация могут помочь в оптимизации кода и улучшении производительности программы.'),\n",
              " '1ab77231-8cbf-4faf-803c-82ea20313dfb': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Описание алгоритма сортировки вставками\"\\r\\nНа уроке рассказывается о сортировке вставками. Лектор объясняет, что в данном алгоритме используются переменные value, itemToInsert, i и j. Затем он по шагам объясняет, как происходит сортировка. Сначала берется первый элемент и сравнивается с предыдущими элементами. Если предыдущий элемент больше, то он копируется в следующую позицию, и так продолжается до тех пор, пока не будет найдено место для вставки элемента. Затем процесс повторяется для следующего элемента. Лектор также отмечает, что алгоритм имеет квадратичную сложность. Он предлагает перейти к следующему алгоритму - сортировке слиянием.\\r\\n\\r\\n\\r\\nТема: Алгоритм сортировки слиянием с использованием рекурсии\\r\\nНа уроке рассказывается о сортировке слиянием (merge sort). Алгоритм разделяет список на две части, рекурсивно сортирует каждую часть, а затем объединяет их в отсортированный список. Для объединения используется функция merge, которая принимает два отсортированных списка и возвращает новый отсортированный список, объединяя элементы из обоих списков. Алгоритм продолжает разбивать и сортировать списки до тех пор, пока не останется один элемент. Затем он объединяет все списки с помощью функции merge, пока не получит исходный отсортированный список.\\r\\n\\r\\n\\r\\nТема: Запись длины списков и присвоение нулевых значений\\r\\nНа уроке рассказывается о сортировке слиянием и быстрой сортировке. Лектор объясняет, как работает алгоритм сортировки слиянием, где два отсортированных списка объединяются в один отсортированный список. Он также объясняет, как работает алгоритм быстрой сортировки, который использует рекурсию и опорный элемент для разделения списка на две части. Лектор приводит примеры кода и объясняет каждую часть алгоритма. Он также отвечает на вопросы студентов и дает дополнительные объяснения.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Определение опорной точки и проверка значений слева и справа\\r\\nНа уроке рассказывается о быстрой сортировке (Quick Sort). Лектор объясняет, что в начале алгоритма выбирается опорная точка (pivot), которая помогает разделить массив на две части. Затем происходит сравнение значений слева и справа от опорной точки. Если значение слева меньше опорной точки, то индекс i увеличивается на 1. Если значение справа больше опорной точки, то индекс j уменьшается на 1. Если i становится больше или равно j, то алгоритм завершается и возвращается индекс j. Если это не так, то значения i и j меняются местами. Затем алгоритм рекурсивно вызывается для двух подмассивов, до и после опорной точки. Процесс повторяется до тех пор, пока массив не будет полностью отсортирован.\\r\\n\\r\\n\\r\\nТема: Сравнение и сортировка чисел в Python\\r\\nНа уроке лектор рассказывает о быстрой сортировке (Quick Sort) и ее сложности. Он объясняет, что в начале сравниваются два элемента списка, и если первый элемент меньше второго, то они меняются местами. Затем процесс повторяется для каждой пары элементов, пока весь список не будет отсортирован. Лектор также упоминает о других алгоритмах сортировки, таких как сортировка слиянием и комбинированная сортировка (TeamSort), которые работают быстрее и имеют более эффективную сложность. Он также говорит о том, что следующей темой будет алгоритм поиска.\\r\\n\\r\\n\\r\\nТема: \"Бинарный поиск: принцип работы и сложность\"\\r\\nНа уроке рассказывается о бинарном поиске - алгоритме поиска элемента в отсортированном списке. Бинарный поиск основан на разделении списка на две части и последовательном сужении интервала поиска до нахождения искомого элемента. Алгоритм имеет логарифмическую сложность и работает эффективно на больших объемах данных. В примере приводится код на языке Python, который демонстрирует работу бинарного поиска.'),\n",
              " 'e2d43b64-d00b-4fa7-9380-ef2a4d607f36': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Поиск с помощью прыжков и последовательность Фибоначчи\"\\r\\nНа уроке рассказывается о поиске элемента в последовательности с помощью алгоритма Jump Search и о последовательности чисел Фибоначчи. \\r\\nАлгоритм Jump Search основан на прыжках через определенные интервалы в последовательности. Длина интервала определяется как корень из длины последовательности, округленный до целого числа. Если длина последовательности не делится нацело на корень, то остаток отбрасывается. Алгоритм прыгает через интервалы, пропуская элементы, пока не найдет элемент, который больше или равен искомому значению. Затем алгоритм возвращает индекс найденного элемента или -1, если элемент не найден.\\r\\nЧисла Фибоначчи - это последовательность чисел, где каждое число равно сумме двух предыдущих чисел. Начальные значения последовательности - 0 и 1. Числа Фибоначчи имеют много интересных свойств и применений, включая золотое сечение в фотографии. Золотое сечение используется для создания эстетически приятных кадров, где объекты попадают на пересечение определенных рамок в сетке Фибоначчи.\\r\\n\\r\\n\\r\\nТема: \"Использование чисел Фибоначчи для поиска удачных кадров\"\\r\\nНа уроке рассказывается о том, как использовать числа Фибоначчи и золотое сечение для создания удачных кадров в фотографии. Лектор объясняет, что когда объект на снимке совпадает с спиралью Фибоначчи, получается более гармоничное и привлекательное изображение. Затем он предлагает использовать ряд Фибоначчи для определения максимального значения, от которого будем отталкиваться при поиске удачных кадров. Далее он приводит пример кода, который использует значения Фибоначчи для поиска искомого элемента в списке. Цикл выполняется до тех пор, пока значение Фибоначчи не превысит длину списка или не станет равным ей. Затем происходит поиск искомого элемента в списке с использованием значений Фибоначчи. Если значение больше искомого элемента, то значения Фибоначчи обновляются, и цикл продолжается.'),\n",
              " 'cce5c896-23c5-4b88-a3b6-6ec98d9b9b8e': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Алгоритмы поиска: минус 1, плюс, F2, минус 1, плюс 2, получается 1, и i падает по первому индексу.\\r\\nНа уроке лектор рассказывает о различных алгоритмах поиска. Он начинает с алгоритма \"прыжкового поиска\", который основан на итеративном сравнении значения с элементами списка и изменении индекса в зависимости от результата сравнения. Затем он переходит к алгоритму \"экспоненциального поиска\", который является модификацией бинарного поиска и использует увеличение индекса в два раза на каждой итерации. Лектор также упоминает о том, что экспоненциальный поиск может быть ограничен диапазоном, чтобы ускорить процесс. Затем он переходит к алгоритму \"интерполяционного поиска\", который использует интерполяционную функцию для приближенного определения положения искомого значения в списке. Лектор объясняет, что каждый из этих алгоритмов имеет свои преимущества и недостатки, и выбор алгоритма зависит от конкретной задачи.\\r\\n\\r\\n\\r\\nТема: сравнение\\r\\nНа уроке лектор рассказывает о различных алгоритмах поиска в массиве данных. \\r\\nОн объясняет, что линейный поиск осуществляется путем последовательного перебора элементов массива до нахождения нужного значения. \\r\\nБинарный поиск, в свою очередь, применяется только к отсортированным массивам и заключается в делении массива пополам и сравнении искомого значения с серединным элементом. Если искомое значение больше, то поиск продолжается во второй половине массива, иначе - в первой. Джамп-поиск использует шаги фиксированной длины для перехода к ближайшему элементу, превышающему искомое значение, а затем осуществляет линейный поиск в предыдущем блоке. Интерпретационный поиск основан на интерпретации значений массива как индексов искомого значения. \\r\\nЛектор также сравнивает эффективность этих алгоритмов и объясняет, что выбор оптимального алгоритма зависит от характеристик входных данных. \\r\\nОн также упоминает, что следующий урок будет посвящен графам.\\r\\n1062_Алгоритмическое мышление_Алгоритмы поиска и сортировки.txt\\r\\nОткрыть с помощью...\\r\\n  \\r\\n\\r\\n1062_Алгоритмическое мышление_Алгоритмы поиска и сортировки.txt. На экране.'),\n",
              " '98f1eaa1-3276-4da3-bbde-30eb657358bb': Document(metadata={'meta': 'data'}, page_content='\\ufeff<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nВведение.\\r\\nБиологические связи.\\r\\nВначале немного истории. Когда вы впервые услышали термин сверточные\\r\\nнейронные сети, возможно, подумали о чем-то, связанном с нейронауками\\r\\nили биологией, и отчасти были правы.\\r\\n  Сверточные нейронные сети – это своеобразная имитация зрительной коры\\r\\n  мозга. Зрительная кора имеет небольшие участки клеток, которые\\r\\n  чувствительны к конкретным областям поля зрения. Эту идею в 1962 году\\r\\n  детально рассмотрели Хьюбел и Визель с помощью потрясающего\\r\\n  эксперимента (видео).\\r\\n  Они показали, что отдельные мозговые нервные клетки реагировали (или\\r\\n  активировались) только при визуальном восприятии границ определенной\\r\\n  ориентации. Например, некоторые нейроны активировались, когда\\r\\n  воспринимали вертикальные границы, а некоторые — горизонтальные или\\r\\n  диагональные.\\r\\n  Хьюбел и Визель выяснили, что все эти нейроны организованы в блоки\\r\\n  стержневой архитектуры и вместе формируют визуальное восприятие. Эту\\r\\n  идею специализированных компонентов внутри системы, которые решают\\r\\n  конкретные задачи (как клетки зрительной коры, которые ищут\\r\\n  специфические характеристики), и взяли за основу построения сверточных\\r\\n  нейронных сетей (СНС, CNN).\\r\\n\\r\\n\\r\\n\"Входы\" и \"Выходы\"\\r\\nКогда вы смотрите на происходящее вокруг, чаще всего вы можете сразу\\r\\nохарактеризовать место действия и узнать окружающие объекты. Когда вы\\r\\nсмотрите на изображение собаки, вы без труда узнаете ее (при наличии\\r\\nхарактерных особенностей, которые можно идентифицировать, например,\\r\\nлапы).\\r\\nВсе это происходит бессознательно, незаметно для разума. Это и есть\\r\\nнавыки быстрого распознавания шаблонов и обобщения уже полученных\\r\\nзнаний.\\r\\nКогда компьютер \"видит\" изображение (принимает данные на вход), он имеет\\r\\nдело с массивом чисел от 0 до 255, описывающих интенсивность каждого\\r\\nпикселя.\\r\\nРазмер массива чисел зависит от разрешения и размера изображения:\\r\\n-   для цветного (RGB, 3 цвета) с размерами картинки 32 x 32 - это 32х32х3.\\r\\n-   для монохромного (Grey, 1 цвет) с размерами картинки 480 x 480 - это 480х480х1.\\r\\nЗначения пикселей, визуально оставаясь бессмысленными для вас,\\r\\nстановятся единственными вводными данными об изображении, доступными\\r\\nнейросети. Вы подаете ей на вход эту матрицу, а на выходе получаете\\r\\nзначения вероятностей принадлежности картинки (входа) к тому или иному\\r\\nклассу. Например, если классов три, то вы можете получить список [0.80,\\r\\n0.15, 0.05], что означает уверенность нейросети в 80%, что это кошка;\\r\\n15%, что это собака; 5%, что это птица.'),\n",
              " '8657993f-b7dd-4ffe-9ba8-b4659fa78fb5': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСтруктура сверточной нейронной сети\\r\\nСферы применения сверточных нейронных сетей включают:\\r\\n-   работу с изображениями: это великолепный способ структурирования\\r\\n    визуальной информации, построения так называемой инвариантности -\\r\\n    выделения постоянных признаков. Например, возьмём идею распознавания\\r\\n    образов на сцене или фотографии: вы хотите понять, изображён человек\\r\\n    на ней или нет. Неважно, в какой части фотографии он располагается,\\r\\n    где его голова — в центре фотографии или в углу. Здесь инвариантный\\r\\n    (неизменный) признак - голова, которая выделяется и распознается\\r\\n    свёрточной нейронной сетью в разных местах изображения.\\r\\n-   распознавание объектов по фото, обработка медицинских снимков,\\r\\n    космоснимков и пр.\\r\\n-   работу с аудио и видео.\\r\\n-   применение в языковых технологиях, где специалисты используют\\r\\n    глубокое обучение для извлечения смысла и генерации предложений на\\r\\n    естественном языке.\\r\\nЗадача нейронщика в том, чтобы правильно подобрать архитектуру и научить\\r\\nнейросеть распознавать уникальные особенности объектов (аналогично мозгу\\r\\nчеловека), и на этом основании классифицировать все подаваемые на вход\\r\\nнейросети изображения.\\r\\nНейросеть может классифицировать изображения через поиск характеристик\\r\\nбазового уровня, например, границ и кривых. Следующий сверточный слой\\r\\nбудет анализировать уже не исходное изображение, а найденные объекты\\r\\n(границы и кривые), обнаруживая признаки более высокого уровня: контуры,\\r\\nфигуры и т.п. Таким образом, имея группу сверточных слоев, нейросеть\\r\\nсможет выдать требуемый для нас результат, проведя серию операций по\\r\\nобнаружению определенных элементов изображения (при этом каждый новый\\r\\nслой будет искать признаки более высокого уровня, опираясь на результаты\\r\\nодного или нескольких предыдущих слоев).\\r\\n\\r\\n\\r\\nТаково общее описание работы сверточных нейронных сетей. Теперь вы\\r\\nможете углубиться в их структуру.\\r\\nСравнение полносвязных и сверточных сетей\\r\\nСвёрточные нейронные сети очень похожи на обычные полносвязные нейронные\\r\\nсети, которые вы изучали ранее: они состоят из нейронов, которые, в свою\\r\\nочередь, содержат изменяемые в процессе обучения веса и смещения.\\r\\nКаждый нейрон получает какие-то входные данные, вычисляет их скалярное\\r\\nпроизведение с весами и, опционально, использует нелинейную функцию\\r\\nактивации.\\r\\nВся сеть по-прежнему представляет собой единственную дифференцируемую\\r\\nфункцию: от исходного набора пикселей (изображения) на входе до\\r\\nраспределения вероятностей принадлежности к определённому классу на\\r\\nвыходе.\\r\\n\\r\\n\\r\\nУ этих сетей по-прежнему есть функция-классификатор (например, Softmax)\\r\\nв последнем (полносвязном) слое, и все те советы и рекомендации, которые\\r\\nбыли даны к обычным нейронным сетям, применимы и для свёрточных\\r\\nнейронных сетей.\\r\\nТак что же изменилось? Архитектура свёрточных нейронных сетей явно\\r\\nпредполагает получение на входе изображений, что позволяет нам учесть\\r\\nопределённые свойства входных данных в самой архитектуре сети.\\r\\nОбычные нейронные сети плохо масштабируются для больших изображений. В\\r\\nнаборе данных CIFAR-10, например, изображения имеют размер 32х32х3 (32\\r\\nпикселя высота, 32 пикселя ширина, 3 цветовых канала).\\r\\nДля обработки такого изображения полносвязный нейрон в первом скрытом\\r\\nслое обычной нейронной сети будет иметь 32х32х3 = 3072 весов. Такое\\r\\nколичество всё ещё является допустимым, но становится очевидным тот\\r\\nфакт, что подобная структура не будет работать с изображениями большего\\r\\nразмера. Например, для изображения большего размера — 200х200х3 —\\r\\nколичество весов станет равным 200х200х3 = 120000.\\r\\nБолее того, нам понадобится не один подобный нейрон, поэтому общее\\r\\nколичество параметров модели (весов) начнет расти очень быстро.\\r\\nСтановится очевидным тот факт, что полносвязность избыточна, и слишком\\r\\nбольшое количество параметров быстро приведет к переобучению.'),\n",
              " '8dbb1aea-8780-4fb7-8016-7ea6f2510b39': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСверточный слой Conv2D.\\r\\nСуществуют одномерные, двумерные и даже трехмерные сверточные слои. В\\r\\nданном уроке будут рассмотрены двумерные сверточные слои.\\r\\nДля моделей нейронных сетей, построенных на основе класса Sequential,\\r\\nдля создания слоя используйте конструкцию:\\r\\n        .add(Conv2D(32, (3, 3), padding=\\'same\\', activation=\\'relu\\', strides=(1,1))\\r\\n1.  Первый параметр 32 - количество ядер (фильтров) свертки.\\r\\n2.  Второй параметр (3, 3) – размер ядра свертки.\\r\\n3.  Третий параметр padding=\\'same\\' - тип заполнения краев, нужен для\\r\\n    сохранения размеров изображения, по умолчанию значение padding=\\'valid\\'.\\r\\n4.  Четвертый параметр activation=\\'relu\\' – указание функции активации.\\r\\n5.  Пятый параметр strides=(1, 1) – необязательный, задает шаг смещения\\r\\n    фильтра (подробнее в следующем разделе).\\r\\n------------------------------------------------------------------------\\r\\nДополнительная информация (База знаний УИИ - «Функции активации») Ссылка: https://colab.research.google.com/drive/1pGc7CFdrkKBhcXLqZNUzLXH4N83rRAl7?usp=sharing\\r\\n-----------------------------------------------------------------------\\r\\nДополнительная информация (База знаний УИИ - «Сверточный слой Conv2D») Ссылка: https://colab.research.google.com/drive/1bQNGBTEqen_QiYBDGqQ5Uu3iqet_G5ps?usp=sharing\\r\\n------------------------------------------------------------------------\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nРазличия полносвязного и сверточного слоев.\\r\\nПринципиальное различие между полносвязным и сверточным слоем состоит в\\r\\nтом, что первый изучает глобальные характеристики изображения целиком, а\\r\\nвторой - локальные шаблоны в небольших фрагментах изображения.\\r\\nМинус полносвязного слоя в том, что:\\r\\n-   он обладает низкой вариативностью. То, что мы будем подавать на\\r\\n    вход, должно быть практически идентично тому, на чем обучали, даже\\r\\n    смещение объекта на листе уже критично. Локализованные признаки он\\r\\n    не обнаружит.\\r\\n-   при использовании полносвязного слоя каждый нейрон связан с каждым\\r\\n    нейроном предыдущего слоя, т.е. каждый нейрон анализирует все\\r\\n    изображение и может найти ложные зависимости.\\r\\nВариант решения: создаем \"маленький Dense\" и им пробегаем по всему\\r\\nизображению в поисках совпадения. Это позволит обнаружить объект, даже\\r\\nесли он уменьшен или смещен.\\r\\nПринцип работы сверточного слоя\\r\\nОсновная задача сверточного слоя – выделить признаки во входном\\r\\nизображении ядрами свертки (фильтрами) и сформировать карту признаков в\\r\\nвиде тензора.\\r\\nКарта признаков или Карта активации - это обработанное ядром свертки исходное изображение.\\r\\nЯдра свертки (фильтры) - это тензоры одного размера (задается при настройке слоя). Количество ядер в слое определяет глубину выходного массива (т.е. количество ядер вместе с разрешением изображения определяют форму выходного тензора).\\r\\nСвёртка – это операция вычисления нового значения на основе значения\\r\\nвыбранного пикселя и значений окружающих его пикселей.\\r\\n------------------------------------------------------------------------\\r\\nАлгоритм свёртки можно описать так:\\r\\n-   фильтр накладывается на левую верхнюю часть входящего изображения;\\r\\n-   производится поэлементное умножение значений фильтра и значений\\r\\n    пикселей изображения;\\r\\n-   полученные значения складываются, сумма будет результатом свертки\\r\\n    области наложения (одно число);\\r\\n-   фильтр перемещается дальше по изображению (за смещение отвечает\\r\\n    параметр strides, по умолчанию равен (1,1), т.е. смещение на 1\\r\\n    пиксель вправо, при достижении конца строки сдвиг вниз на 1 пиксель,\\r\\n    и вновь с начала строки);\\r\\n-   в новом положении окна фильтра производится поэлементное умножение\\r\\n    значений фильтра ...\\r\\n-   повтор до тех пор, пока аналогичным образом не будут обработаны все\\r\\n    участки.'),\n",
              " 'ca14d200-33fa-490e-ad8e-eb10a00a0c0d': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nЗаполнение (Padding).\\r\\nКогда мы выбираем достаточно малый размер ядра фильтра, потеря в\\r\\nразмерах выходного слоя невелика. Если же это значение увеличивается, то\\r\\nи размеры выходного слоя уменьшается по отношению к оригинальному\\r\\nразмеру входного изображения. Это может стать проблемой, особенно если в\\r\\nмодели нейронной сети подключено последовательно много сверточных слоев.\\r\\nЧтобы избежать потерь в разрешении выходных изображений, в настройках\\r\\nсверточных слоев используют дополнительный параметр – заполнение\\r\\n(padding), позволяющий расширить полученное изображение по краям до того\\r\\nже размера, что и на входе свертки.\\r\\nПример: если мы по картинке размером 5 x 5 пикселей проходим ядром 3 x\\r\\n3, то в конце получим потерю в размерах.\\r\\nРешение: добавляем параметр padding=\\'same\\', тогда Keras дополнит\\r\\nвыходное изображение по внешней границе нулями (другие числа исказили бы\\r\\nинформацию, а нули дают только изменение размера), расположит их так,\\r\\nчтобы выходной массив не потерял в размерах. Заполнение распределяется\\r\\nпо границам исходя из того, как много потерь нужно возместить.\\r\\n\\r\\n\\r\\nРазмеры ядра свертки.\\r\\nСвёрточные нейронные сети используют допущение, что входные данные —\\r\\nизображения, поэтому они образуют более чувствительную архитектуру к\\r\\nподобному типу данных.\\r\\nВ частности, в отличие от обычных нейронных сетей, слои в свёрточной\\r\\nнейронной сети располагают ядра свертки в трех измерениях — ширине,\\r\\nвысоте, глубине (здесь термин \"глубина\" относится к третьему измерению\\r\\nядер, а не глубине самой нейронной сети, измеряемой в количестве слоёв).\\r\\nНапример, входные изображения из набора данных CIFAR-10 являются\\r\\nвходными данными в 3D-представлении, форма которых равна 32х32х3\\r\\n(ширина, высота, глубина). Как мы увидим позднее, нейроны в одном слое\\r\\nбудут связаны с небольшим количеством нейронов предыдущего слоя, вместо\\r\\nтого чтобы быть связанными с ними всеми.\\r\\nТаким образом, глубина фильтров первого слоя совпадает с количеством\\r\\nканалов входного изображения. Если на вход свёрточному слою подаётся RGB\\r\\nизображение (3 канала) и требуется получить 32 карты признаков, то\\r\\nсвёрточный слой должен содержать в себе 32 фильтра глубиной 3.\\r\\n\\r\\n\\r\\nВеса фильтра, который пробегает по изображению, не изменяются во время\\r\\nсамого прохождения, но обучаются от батча к батчу. В результате обучения\\r\\nфильтр начинает отвечать за поиск определенного признака. в конце концов\\r\\nодно ядро будет выделять горизонтальную прямую, другое – вертикальную,\\r\\nтретье – диагональ и т.п.\\r\\nДля обработки изображений (не только нейросетями) нередко требуется\\r\\nрешать конкретные задачи: выделять границы, повышать резкость или\\r\\nприменять размытие.\\r\\nДля подобных целей были получены сверточные фильтры, которые сегодня\\r\\nчасто встраиваются в различные графические редакторы.\\r\\nСамые распространенные размеры ядра двумерных сверточных слоев – 3х3,\\r\\n5х5 и 7х7. Количество ядер, как правило, кратно двум – 8, 16, 32, 64 и\\r\\nт.д. Но никто не запрещает использовать ядро 3х7 или 27х27 и количество\\r\\nядер 315.\\r\\nНапример, такие ядра свертки используются в фильтрах Photoshop.'),\n",
              " 'de84f61b-304c-4532-b0ad-7f7c9fe22920': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСлой подвыборки (Pooling-слой).\\r\\nОсновные задачи слоев типа Pooling:\\r\\n1.  Распознавание объектов вне зависимости от масштаба;\\r\\n2.  Факт наличия признака важнее знания места его точного положения на\\r\\n    изображении.\\r\\nЭтот слой немного похож на сверточный, поскольку у него тоже есть ядро\\r\\n(окно фильтра). Но, в отличие от сверточного слоя, он уменьшает размер\\r\\nизображения, выбирая максимальное (MaxPooling), среднее (AveragePooling)\\r\\nили суммарное (SumPooling) значение из окна фильтра. В некотором смысле\\r\\nслой подвыборки делает информацию более сконцентрированной, обобщенной.\\r\\nСлой подвыборки имеет один обязательный параметр — pool_size (размер\\r\\nокна подвыборки), и один необязательный - strides (шаг смещения окна).\\r\\nПричем если strides не указан, то по умолчанию strides=pool_size, то\\r\\nесть окно смещается на размер фильтра.\\r\\nНаиболее часто используется слой MaxPooling с ядром (2, 2) и смещением\\r\\nпо умолчанию - он уменьшает размеры входного тензора по ширине и высоте\\r\\nв два раза. Поэтому рекомендуется использовать такие разрешения\\r\\nизображений, чтобы размеры по обоим измерениям делились на 2 без\\r\\nостатка, иначе при уменьшении размеров часть информации будет потеряна.\\r\\n  Если \"сжать\" слоем MaxPooling изображение размером 15 x 15, на выходе\\r\\n  получим 7 x 7. А при \"разжатии\" изображения слоем Upsampling или\\r\\n  Conv2DTranspose (выполняют что-то вроде обратной к MaxPooling\\r\\n  операции) получим 14 x 14, что не совпадет с исходными размерами.\\r\\nНекоторые библиотеки позволяют задавать раздельные параметры уменьшения\\r\\nпо высоте и ширине, создавать прямоугольное ядро подвыборки. Однако чаще\\r\\nвсего оно квадратное.\\r\\nЧтобы добавить слой, используйте:\\r\\n    .add(MaxPooling2D(pool_size=(2, 2)), где (2,2) – размер окна, в котором выбирается максимальное значение.\\r\\n\\r\\n\\r\\nФинальная классификация данных.\\r\\nПосле прохождения через сверточные слои и слои подвыборки данные\\r\\nнеобходимо систематизировать и классифицировать. Для этого применяют\\r\\nполносвязные слои Dense, которые вы изучили на предыдущих уроках.\\r\\nЧтобы правильно передать данные от сверточного слоя на полносвязный,\\r\\nнужно сделать данные одномерными. Для данной задачи подходят слои:\\r\\n-   Flatten() - \"сплющивает\" многомерные входные данные в одномерный\\r\\n    вектор, при этом размеры данных по всем осям перемножаются;\\r\\n    дополнительных параметров нет. Например, входящее изображение формы\\r\\n    (28, 28, 3) преобразуется в вектор формы (2352).\\r\\n-   Reshape(...) - смена формы данных. Требуется указать в скобках\\r\\n    желаемую форму данных. Позволяет не только вытягивать данные в\\r\\n    вектор, но и произвольно менять форму, например (28, 28, 3)\\r\\n    преобразовать в (3, 784), или в (14, 14, 12). Объем формы данных\\r\\n    (произведение размеров по всем осям) на входе должен совпадать с\\r\\n    объемом желаемой формы данных.\\r\\nВ зависимости от задачи, выходной Dense-слой может вычислять вероятности\\r\\nдля каждого класса или выдавать номер (метку) класса.'),\n",
              " '283918e4-1482-4613-a595-b54ee31823a2': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСоздание простой модели сверточной нейронной сети.\\r\\nПодключите основу – класс создания последовательной модели Sequential:\\r\\n    from tensorflow.keras.models import Sequential\\r\\nС помощью него создайте экземпляр вашей модели:\\r\\n    model = Sequential()\\r\\nЭто и есть ваша модель! Сейчас она больше похожа на пустую коробку.\\r\\nЧтобы она что-то делала, нужно поместить в нее какой-нибудь механизм.\\r\\nЭто не механизм в обычном смысле слова, потому что вы будете оперировать\\r\\nне предметами, а информацией – главным ресурсом XXI века. Механизм будет\\r\\nпринимать на вход и выдавать на выход какие-то данные.\\r\\nТак из чего же вы можете создать механизм? Для начала определитесь,\\r\\nсколько информации вы будете давать нейросети на вход. Один экземпляр\\r\\nтакой информации называется объектом. Не углубляйтесь пока, какими они\\r\\nбывают и как устроены. Сейчас достаточно знать, что объекты всегда\\r\\nсостоят из чисел.\\r\\nНапример, вы решили, что ваши объекты - изображения. Для подачи в\\r\\nнейросеть их надо оцифровать.\\r\\nУ изображений есть высота img_height, ширина img_width и количество\\r\\nцветовых каналов channels.\\r\\nИх называют входной формой (или формой входных данных) и записывают как:\\r\\n    input_shape=(img_height, img_width, channels)\\r\\n------------------------------------------------------------------------\\r\\nВажно: все изображения, которые вы подаете на вход нейронной сети,\\r\\nдолжны иметь общие высоту, ширину и количество каналов. Ниже вы узнаете,\\r\\nкак это сделать.\\r\\n------------------------------------------------------------------------\\r\\nДобавьте в модель первый слой при помощи .add():\\r\\n    from tensorflow.keras.layers import Conv2D\\r\\n    # Первый сверточный слой\\r\\n    model.add(Conv2D(8, (3, 3), padding=\\'same\\', activation=\\'relu\\', input_shape=(28, 56, 3)))\\r\\nРасшифруем написанное выше: первый сверточный слой принимает на вход\\r\\nцветное изображение (3 канала) размерами 28 на 56 пикселей. То есть\\r\\nформа входящего массива - (28, 56, 3).\\r\\nВнутри слоя к нему применяется свертка 8-ю фильтрами (3, 3) с шагом смещения (1, 1), а затем функция активации relu.\\r\\nКакой формы получится выходной массив?\\r\\nОбратите внимание, что padding =\\'same\\', stride=(1,1) по умолчанию;\\r\\nвычислим pad = (size - 1) / 2 = (3-1) /2 = 1.\\r\\nИспользуем формулу:\\r\\n    output_h = (input_h + 2 * pad - size) // stride + 1 = (28 + 2 * 1 - 3) // 1 + 1 = 27 + 1 = 28\\r\\n    output_w = (input_w + 2 * pad - size) // stride + 1 = (56 + 2 * 1 - 3) // 1 + 1 = 55 + 1 = 56\\r\\nКак видите, при стандартном stride и padding =\\'same\\' длина и ширина\\r\\nвходного и выходного массивов сверточного слоя равны. Но отличие все-таки будет - это глубина!\\r\\nНа вход пришло 3 канала, а сверточный слой имеет 8 ядер свертки, каждое\\r\\nиз которых выдает свою карту признаков, обработав любое количество\\r\\nканалов. Значит, глубина на выходе будет 8 вместо 3. Полная форма данных\\r\\nна выходе получится (28, 56, 8).\\r\\nПроверьте это методом модели .summary():\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 224\\r\\n    Trainable params: 224\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nОтлично!\\r\\nПосмотрим, что будет, если добавить следующий сверточный слой c 5\\r\\nфильтрами с ядром (3, 2), шагом смещения (2, 3) и padding =\\'valid\\':\\r\\n    # Второй сверточный слой\\r\\n    model.add(Conv2D(5, (3, 2), strides = (2,3), padding=\\'valid\\', activation=\\'relu\\'))\\r\\nЕсли padding=\\'valid\\', то по правилам pad = 0. Подставим значения:\\r\\n        output_h = (input_h + 2 * pad - size) // stride + 1 = (28 + 2 * 0 - 3) // 2 + 1 = 25 // 2 + 1 = 12 + 1 = 13\\r\\n        output_w = (input_w + 2 * pad - size) // stride + 1 = (56 + 2 * 0 - 2) // 3 + 1 = 54 // 3 + 1 = 18 + 1 = 19\\r\\nУ вас 5 фильтров, значит форма данных на выходе получится (13, 19, 5).\\r\\nПроверьте:\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nПримените слой MaxPooling2D:\\r\\n    from tensorflow.keras.layers import MaxPooling2D \\r\\n    # Слой подвыборки\\r\\n    model.add(MaxPooling2D(pool_size=(3, 3)))\\r\\nMaxPooling2D изменит форму данных следующим образом (учитывая, что\\r\\nstride=pool_size):\\r\\n     output_h = (input - pool_size) // strides + 1 = (13 - 3) // 3 + 1 = 4 \\r\\n     output_w = (input - pool_size) // strides + 1 = (19 - 3) // 3 + 1 = 6\\r\\nГлубина в MaxPooling2D не меняется, выходная форма данных (4, 6, 5).\\r\\nПроверьте:\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nДалее примените слой Flatten, который вытягивает входящий тензор в\\r\\nодномерный вектор. На входе слоя ожидается тензор (4, 6, 5), а на выходе\\r\\nбудет вектор (4 * 6 * 5) = (120)\\r\\n    from tensorflow.keras.layers import Flatten\\r\\n    # Слой преобразования многомерных данных в одномерные \\r\\n    model.add(Flatten())\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n     flatten (Flatten)           (None, 120)               0         \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nНужно обратить внимание, что размерность первых двух элементов тензора\\r\\nперед Flatten слоем (в этом случае, 4 х 6) не должна быть очень\\r\\nбольшая - можно добавлять слои Conv2D и MaxPooling2D пока эта\\r\\nразмерность не станет равна 1 х 1. Например, если бы мы не добавили слой\\r\\nMaxPooling2D, эта размерность была бы равна 13 х 19, что бы было уже\\r\\nоднозначно много.\\r\\nЕсли размерность, которая мы подаем в Flatten() слой слишком большая,\\r\\nследующий слой не сможет извлечь достаточно значемые признаки для\\r\\nправильной классификации, потому что на вход слоя приходят слишком много\\r\\nданных.\\r\\n------------------------------------------------------------------------\\r\\nДалее мы создадим последний, выходной слой Dense. Он получит на вход\\r\\nодномерный вектор (120), а на выходе выдаст одномерный вектор (3).\\r\\nАктивационная функция softmax выдаст вероятности принадлежности входных\\r\\nданных к каждому из трех классов.\\r\\nПроверьте:\\r\\n    from tensorflow.keras.layers import Dense\\r\\n    model.add(Dense(3, activation=\\'softmax\\'))\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n     flatten (Flatten)           (None, 120)               0         \\r\\n                                                                     \\r\\n     dense (Dense)               (None, 3)                 363                                                                           \\r\\n    =================================================================\\r\\n    Total params: 832\\r\\n    Trainable params: 832\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nВот вы и построили простую сверточную сеть. Теперь у вас есть небольшой\\r\\nопыт, который пригодится для построения более сложной сети, чтобы решить\\r\\nзадачу классификации.\\r\\nПрактический ноутбук 1 - ссылка: https://colab.research.google.com/drive/1wR8vbQ1LSJy1ZER3wIgXENUT0K-Q15Qh?usp=sharing\\r\\nПрактический ноутбук 2 - ссылка: https://colab.research.google.com/drive/1f95JuEQcz2LYrCbWS3GnvnXUSN2GL2HR?usp=sharing\\r\\n\\r\\n\\r\\n761_1_Базовый_блок___Сверточные_нейронные_сети_(Теория)___УИИ.txt')}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Удаление отдельного чанка по его индексу"
      ],
      "metadata": {
        "id": "mjbOOfZDAv8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление элемента по ключу и возврат его значения (опционально)\n",
        "merged_db.delete(ids=['1ab77231-8cbf-4faf-803c-82ea20313dfb'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znVZPalIzwSC",
        "outputId": "b6991a99-84e1-4f1b-e7e8-a717a84e980e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# проверяем - действительно, больше нет такого чанка\n",
        "merged_db.docstore._dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdPVxzO0cgY7",
        "outputId": "0a8bbb4a-c4d3-48f1-99a7-67a6fe7abd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a5196dfa-0c0e-491d-bca0-2dde0b0045a8': Document(metadata={'meta': 'data'}, page_content='\\ufeff<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСсылка https://colab.research.google.com/drive/14n-IdIQlE8mlQhA88vhxXDcC2VcuDIIt?usp=sharing\\r\\nАлгоритмы сортировки\\r\\nСуществуют десятки алгоритмов сортировки. Разные алгоритмы оптимальны\\r\\nдля разных наборов и типов данных. Мы рассмотрим некоторые из них:\\r\\n-   Пузырьковая сортировка\\r\\n-   Сортировка выборкой\\r\\n-   Сортировка вставками\\r\\n-   Сортировка слиянием\\r\\n-   Быстрая сортировка\\r\\n\\r\\n\\r\\nПузырьковая сортировка\\r\\nПузырьковая сортировка или сортировка простыми обменами – один из\\r\\nпростейших алгоритмов сортировки. Он применяется для упорядочивания\\r\\nмассивов небольших размеров.\\r\\n    # Алгоритм пузырьковой сортировки\\r\\nСуть алгоритма в том, что совершается несколько проходов по массиву. При\\r\\nкаждом проходе попарно сравниваются два соседних элемента. Если они\\r\\nнаходятся в верном порядке, то ничего не происходит, в противном случае\\r\\nони меняются местами. В результате первого прохода максимальный элемент\\r\\nокажется в конце, то есть всплывет словно пузырек. Затем все повторяется\\r\\nдо того момента пока весь массив не будет отсортирован.\\r\\n    def bubble_sort(nums):\\r\\n        # Устанавливаем swapped в True, чтобы цикл запустился хотя бы один раз\\r\\n        swapped = True\\r\\n        while swapped:\\r\\n            swapped = False\\r\\n            for i in range(len(nums) - 1):\\r\\n                if nums[i] > nums[i + 1]:\\r\\n                    # Меняем элементы\\r\\n                    nums[i], nums[i + 1] = nums[i + 1], nums[i]\\r\\n                    # Устанавливаем swapped в True для следующей итерации\\r\\n                    swapped = True\\r\\n\\r\\n\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    bubble_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма пузырьковой сортировки\\r\\nВ алгоритме пузырьковой сортировки есть два вложенных цикла while и for.\\r\\nЕсли взять самый худший случай (изначально список отсортирован по\\r\\nубыванию), то сложность алгоритма будет квадратичной:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка выборкой\\r\\nЭтот алгоритм сегментирует список на две части: отсортированные и\\r\\nнесортированные. Он постоянно удаляет наименьший элемент из\\r\\nнесортированного сегмента списка и добавляет его в отсортированный\\r\\nсегмент.\\r\\n    # Алгоритм сортировки выборкой\\r\\nНа практике нам не нужно создавать новый список для отсортированных\\r\\nэлементов, мы будем обрабатывать крайнюю левую часть списка как\\r\\nотсортированный сегмент. Затем мы ищем во всем списке наименьший элемент\\r\\nи меняем его на первый элемент.\\r\\nТеперь мы знаем, что первый элемент списка отсортирован, мы получаем\\r\\nнаименьший элемент из оставшихся элементов и заменяем его вторым\\r\\nэлементом. Это повторяется до тех пор, пока последний элемент списка не\\r\\nстанет оставшимся элементом для изучения.\\r\\n    def selection_sort(nums):\\r\\n        # Значение i соответствует кол-ву отсортированных значений\\r\\n        for i in range(len(nums)):\\r\\n            # Исходно считаем наименьшим первый элемент\\r\\n            lowest_value_index = i\\r\\n            # Этот цикл перебирает несортированные элементы\\r\\n            for j in range(i + 1, len(nums)):\\r\\n                if nums[j] < nums[lowest_value_index]:\\r\\n                    lowest_value_index = j\\r\\n            # Самый маленький элемент меняем с первым в списке\\r\\n            nums[i], nums[lowest_value_index] = nums[lowest_value_index], nums[i]\\r\\n\\r\\n\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    selection_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки выборкой\\r\\nВ этом алгоритме два вложенных цикла for, следовательно сложность\\r\\nквадратичная:\\r\\n     O(n²), где n — количество элементов списка.'),\n",
              " '4562a89f-64ed-457e-8ccb-1d2bb394fd1b': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка вставками\\r\\nКак и сортировка выборкой, этот алгоритм делит список на отсортированные\\r\\nи несортированные части. Он перебирает неотсортированный сегмент и\\r\\nвставляет просматриваемый элемент в правильную позицию отсортированного\\r\\nсписка.\\r\\n    # Алгоритм сортировки вставками\\r\\nПредполагается, что первый элемент списка отсортирован. Переходим к\\r\\nследующему элементу, обозначим его х. Если х больше первого, оставляем\\r\\nего на своём месте. Если он меньше, копируем его на вторую позицию, а х\\r\\nустанавливаем как первый элемент.\\r\\nПереходя к другим элементам несортированного сегмента, перемещаем более\\r\\nкрупные элементы в отсортированном сегменте вверх по списку, пока не\\r\\nвстретим элемент меньше x или не дойдём до конца списка. В первом случае\\r\\nx помещается на правильную позицию.\\r\\n    def insertion_sort(nums):\\r\\n        # Сортировку начинаем со второго элемента, т.к. считается, что первый элемент уже отсортирован\\r\\n        for i in range(1, len(nums)):\\r\\n            item_to_insert = nums[i]\\r\\n            # Сохраняем ссылку на индекс предыдущего элемента\\r\\n            j = i - 1\\r\\n            # Элементы отсортированного сегмента перемещаем вперёд, если они больше\\r\\n            # элемента для вставки\\r\\n            while j >= 0 and nums[j] > item_to_insert:\\r\\n                nums[j + 1] = nums[j]\\r\\n                j -= 1\\r\\n            # Вставляем элемент\\r\\n            nums[j + 1] = item_to_insert\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    insertion_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки вставками\\r\\nТак же, как и предыдущие 2 алгоритма этот имеет 2 вложенных цикла for и\\r\\nwhile, значит сложность тоже квадратичная:\\r\\n     O(n²), где n — количество элементов списка.'),\n",
              " '345a827d-3cb0-4612-b1b1-8bfcf389cbc1': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСортировка слиянием\\r\\nЭтот алгоритм разбивает список на две части, каждую из них он разбивает\\r\\nещё на две и т. д. Список разбивается пополам, пока не останутся\\r\\nединичные элементы.\\r\\nСоседние элементы становятся отсортированными парами. Затем эти пары\\r\\nобъединяются и сортируются с другими парами. Этот процесс продолжается\\r\\nдо тех пор, пока не отсортируются все элементы.\\r\\n    # Алгоритм сортировки слиянием\\r\\nСписок рекурсивно разделяется пополам, пока в итоге не получатся списки\\r\\nразмером в один элемент. Массив из одного элемента считается\\r\\nупорядоченным. Соседние элементы сравниваются и соединяются вместе. Это\\r\\nпроисходит до тех пор, пока не получится полный отсортированный список.\\r\\nСортировка осуществляется путём сравнения наименьших элементов каждого\\r\\nподмассива. Первые элементы каждого подмассива сравниваются первыми.\\r\\nНаименьший элемент перемещается в результирующий массив. Счётчики\\r\\nрезультирующего массива и подмассива, откуда был взят элемент,\\r\\nувеличиваются на 1.\\r\\n    def merge(left_list, right_list):\\r\\n        sorted_list = []\\r\\n        left_list_index = right_list_index = 0\\r\\n        # Длина списков часто используется, поэтому создадим переменные для удобства\\r\\n        left_list_length, right_list_length = len(left_list), len(right_list)\\r\\n        for _ in range(left_list_length + right_list_length):\\r\\n            if left_list_index < left_list_length and right_list_index < right_list_length:\\r\\n                # Сравниваем первые элементы в начале каждого списка\\r\\n                # Если первый элемент левого подсписка меньше, добавляем его\\r\\n                # в отсортированный массив\\r\\n                if left_list[left_list_index] <= right_list[right_list_index]:\\r\\n                    sorted_list.append(left_list[left_list_index])\\r\\n                    left_list_index += 1\\r\\n                # Если первый элемент правого подсписка меньше, добавляем его\\r\\n                # в отсортированный массив\\r\\n                else:\\r\\n                    sorted_list.append(right_list[right_list_index])\\r\\n                    right_list_index += 1\\r\\n            # Если достигнут конец левого списка, элементы правого списка\\r\\n            # добавляем в конец результирующего списка\\r\\n            elif left_list_index == left_list_length:\\r\\n                sorted_list.append(right_list[right_list_index])\\r\\n                right_list_index += 1\\r\\n            # Если достигнут конец правого списка, элементы левого списка\\r\\n            # добавляем в отсортированный массив\\r\\n            elif right_list_index == right_list_length:\\r\\n                sorted_list.append(left_list[left_list_index])\\r\\n                left_list_index += 1\\r\\n        return sorted_list\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nАлгоритм сортировки слиянием\\r\\n    def merge_sort(nums):\\r\\n        # Возвращаем список, если он состоит из одного элемента\\r\\n        if len(nums) <= 1:\\r\\n            return nums\\r\\n        # Для того чтобы найти середину списка, используем деление без остатка\\r\\n        # Индексы должны быть integer\\r\\n        mid = len(nums) // 2\\r\\n        # Сортируем и объединяем подсписки\\r\\n        left_list = merge_sort(nums[:mid])\\r\\n        right_list = merge_sort(nums[mid:])\\r\\n        # Объединяем отсортированные списки в результирующий\\r\\n        return merge(left_list, right_list)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    random_list_of_nums = merge_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма сортировки слиянием\\r\\nБлагодаря тому, что функция merge_sort() возвращает новый список, а не\\r\\nсортирует существующий, такая сортировка имеет только один цикл, но при\\r\\nэтом для такого алгоритма требуется больше памяти.\\r\\nСложность такого алгоритма является линейно-логарифмической:\\r\\n     O(n*log n), где n — количество элементов списка.'),\n",
              " 'a47c6b76-a540-4c73-9fb0-71b7794fbbba': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nБыстрая сортировка\\r\\nПри использовании этого алгоритма, массив разделяется на две части по\\r\\nразные стороны от опорного элемента. В процессе сортировки элементы\\r\\nменьше опорного помещаются перед ним, а равные или большие — позади.\\r\\n    # Алгоритм быстрой сортировки\\r\\nБыстрая сортировка начинается с разбиения списка и выбора одного из\\r\\nэлементов в качестве опорного. А всё остальное передвигаем так, чтобы\\r\\nэтот элемент встал на своё место. Все элементы меньше него перемещаются\\r\\nвлево, а равные и большие элементы перемещаются вправо.\\r\\n    def partition(nums, low, high):\\r\\n        # Выбираем средний элемент в качестве опорного\\r\\n        # Также возможен выбор первого, последнего\\r\\n        # или произвольного элементов в качестве опорного\\r\\n        pivot = nums[(low + high) // 2]\\r\\n        i = low - 1\\r\\n        j = high + 1\\r\\n        while True:\\r\\n            i += 1\\r\\n            while nums[i] < pivot:\\r\\n                i += 1\\r\\n            j -= 1\\r\\n            while nums[j] > pivot:\\r\\n                j -= 1\\r\\n            if i >= j:\\r\\n                return j\\r\\n            # Если элемент с индексом i (слева от опорного) больше, чем\\r\\n            # элемент с индексом j (справа от опорного), меняем их местами\\r\\n            nums[i], nums[j] = nums[j], nums[i]\\r\\n    def quick_sort(nums):\\r\\n        # Создадим вспомогательную функцию, которая вызывается рекурсивно\\r\\n        def _quick_sort(items, low, high):\\r\\n            if low < high:\\r\\n                # This is the index after the pivot, where our lists are split\\r\\n                split_index = partition(items, low, high)\\r\\n                _quick_sort(items, low, split_index)\\r\\n                _quick_sort(items, split_index + 1, high)\\r\\n        _quick_sort(nums, 0, len(nums) - 1)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    quick_sort(random_list_of_nums)\\r\\n    print(random_list_of_nums)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\\r\\nСложность алгоритма быстрой сортировки\\r\\nВ худшем случае, если опорный элемент будет минимальным или максимальным\\r\\nиз списка, то сложность такого алгоритма будет квадратичной:\\r\\n     O(n²), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСравнение рассмотренных алгоритмов сортировки\\r\\nЕсли оценка сложности при помощи Big O это статистический анализ\\r\\nскорости работы алгоритма, то теперь проведем эмпирический анализ.\\r\\nВозьмём один и тот же список из 5000 значений, выполним сортировку\\r\\nразными алгоритмами и сравним время работы каждого из них.\\r\\n    import time\\r\\n    import numpy as np\\r\\n    # Создадим список длиной 5000 значений с рандомными числами от 1 до 1000\\r\\n    exp_list = np.random.randint(0, 1000, 5000)\\r\\nПузырьковая сортировка\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    bubble_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    10.612551212310791\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict = {}\\r\\n    time_dict['bubble'] = round(end, 2)\\r\\nТема: Сортировка выборкой.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    selection_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    4.216295003890991\\r\\n    # Добавим время работы алгоритма в словарь\\r\\n    time_dict['selection'] = round(end, 2)\"),\n",
              " '0f6b4f7d-0444-422d-a574-0ad5fe591495': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nПродолжение Сравнение рассмотренных алгоритмов сортировки\\r\\nТема: Сортировка вставками.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    insertion_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    2.7952945232391357\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict['insertion'] = round(end, 2)\\r\\nТема: Сортировка слиянием.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    merge_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    0.03851127624511719\\r\\n# Добавим время работы алгоритма в словарь\\r\\n    time_dict['merge'] = round(end, 2)\\r\\nТема: Быстрая сортировка.\\r\\n    # Сделаем копию исходного массива\\r\\n    copy_list = exp_list.copy()\\r\\n    final_list = copy_list.copy()\\r\\n    # Точка отсчета времени\\r\\n    start = time.time()\\r\\n    # Выполнение кода\\r\\n    quick_sort(final_list)\\r\\n    # Расчет времени работы кода\\r\\n    end = time.time() - start\\r\\n    # Вывод времени\\r\\n    print(end)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    0.0353546142578125\\r\\n    # Добавим время работы алгоритма в словарь\\r\\n    time_dict['quick'] = round(end, 2)\\r\\nВыведем график скорости работы алгоритмов\\r\\n    import matplotlib.pyplot as plt\\r\\n    plt.figure(figsize=(8,6))\\r\\n    plt.bar(time_dict.keys(), time_dict.values())\\r\\n    plt.show()\\r\\nСтандартная функция сортировки python sort() использует сортировку Тима,\\r\\nкоторая представляет собой комбинацию сортировки слиянием и сортировки\\r\\nвставками.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nАлгоритмы поиска\\r\\nСамый простой пример поиска можно осуществить при помощи операторов in\\r\\nили not in.\\r\\n    # Пример поиска символа в строке\\r\\n    't' in 'qwerty'\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    True\\r\\n# Пример поиска значения в списке\\r\\n    5 in [1,2,3,4]\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    False\\r\\nМинус такого поиска в том, что мы не знаем, где именно находится искомая\\r\\nинформация (например индекс значения в списке), а знаем только то, что\\r\\nона там есть или нет.\\r\\nТема: Линейный поиск.\\r\\nАлгоритм линейного поиска очень простой. Мы просто проходим по всем\\r\\nэлементам массива по порядку и сравниваем с искомым значением.\\r\\n    # Алгоритм линейного поиска\\r\\nПлюс алгоритма линейного поиска в том, что он работает как с\\r\\nотсортированными массивами, так и с несортированными.\\r\\n    def LinearSearch(nums, val):\\r\\n        for i in range(len(nums)):\\r\\n            if nums[i] == val:\\r\\n                return i\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    random_list_of_nums = [6, 4, 2, 3, 8, 5, 1, 7, 9]\\r\\n    result = LinearSearch(random_list_of_nums, 8)\\r\\n    print('Значение 8 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 8 имеет индекс - 4\\r\\nСложность алгоритма линейного поиска\\r\\nСложность данного алгоритма линейная, так как количество итераций\\r\\nнапрямую зависит от размера входных данных.\\r\\n     O(n), где n — количество элементов списка.\"),\n",
              " 'c950f742-459a-4062-8aca-9721a0f92bb6': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Бинарный поиск.\\r\\nБинарный поиск работает по принципу «разделяй и властвуй». Он быстрее,\\r\\nчем линейный поиск, но требует, чтобы массив был отсортирован перед\\r\\nвыполнением алгоритма.\\r\\n    # Алгоритм бинарного поиска\\r\\nРабота данного алгоритма основана на определении среднего значения. Если\\r\\nэто значение не является искомым, то нужно определить в какой половине\\r\\nмассива мы будем продолжать поиски. Если то значение, которое мы ищем\\r\\nбольше среднего, то поиски продолжатся в правой части массива, если\\r\\nменьше, то в левой. Так на каждой итерации мы сужаем область поиска,\\r\\nпостепенно двигаясь к правильному ответу.\\r\\n    def BinarySearch(nums, val):\\r\\n        first = 0\\r\\n        last = len(nums)-1\\r\\n        index = -1\\r\\n        while (first <= last) and (index == -1):\\r\\n            mid = (first+last)//2\\r\\n            if nums[mid] == val:\\r\\n                index = mid\\r\\n            else:\\r\\n                if val<nums[mid]:\\r\\n                    last = mid -1\\r\\n                else:\\r\\n                    first = mid +1\\r\\n        return index\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = BinarySearch(sorted_list_of_nums, 31)\\r\\n    print(\\'Значение 31 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 31 имеет индекс - 6\\r\\nСложность алгоритма бинарного поиска\\r\\nНа каждой итерации бинарного поиска мы делим массив на 2 части (одну из\\r\\nкоторых отбрасываем), следовательно сложность данного алгоритма\\r\\nлогарифмическая.\\r\\n     O(log n), где n — количество элементов списка.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Jump search.\\r\\nJump search - это ещё один алгоритм поиска, который работает по принципу\\r\\n\"разделяй и властвуй\" и требует на входе отсортированный массив.\\r\\n    # Алгоритм jump search\\r\\nСуть алгоритма jump search скрыта в его названии. Вместо того, чтобы\\r\\nпроходить по каждому элементу массива, или делить его пополам, мы будем\\r\\nдвигаться с определённым шагом, как бы перепрыгивая (jump) некоторые\\r\\nэлементы. Как правило, шаг берется не случайно, а считается как\\r\\nквадратный корень из количества элементов массива. Например, для массива\\r\\nдлиной 9, шаг будет составлять 3, а для массива 15 шаг будет тоже 3\\r\\n(округление идёт до целого числа, отбрасывая остаток).\\r\\n    def JumpSearch(nums, val):\\r\\n        length = len(nums)\\r\\n        jump = int(length**0.5)\\r\\n        left, right = 0, 0\\r\\n        while left < length and nums[left] <= val:\\r\\n            right = min(length - 1, left + jump)\\r\\n            if nums[left] <= val and nums[right] >= val:\\r\\n                break\\r\\n            left += jump;\\r\\n        if left >= length or nums[left] > val:\\r\\n            return -1\\r\\n        right = min(length - 1, right)\\r\\n        i = left\\r\\n        while i <= right and nums[i] <= val:\\r\\n            if nums[i] == val:\\r\\n                return i\\r\\n            i += 1\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = JumpSearch(sorted_list_of_nums, 25)\\r\\n    print(\\'Значение 25 имеет индекс -\\', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 25 имеет индекс - 5\\r\\nСложность алгоритма jump search\\r\\nВременная сложность jump search равна:\\r\\n     O(√n), где √n — размер прыжка, n — количество элементов списка.'),\n",
              " 'f2f4cd12-a035-4da0-a93f-d0f8ab82b166': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Поиск Фибоначчи\\r\\nЧисла Фибоначчи — это последовательность чисел, где каждый элемент\\r\\nявляется суммой двух предыдущих чисел.\\r\\n    0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 ...\\r\\nАлгоритм поиска Фибоначчи использует числа Фибоначчи для расчета\\r\\nдиапазона поисков элемента в массиве.\\r\\nРазберем как работает этот подход.\\r\\nПусть дан список из 11 значений. Нужно найти индекс значения 28 в этом\\r\\nсписке. Так как длина списка 11, то мы ищем число в ряде Фибоначчи\\r\\nравное 11 или ближайшее большее. Число 13 является таковым.\\r\\nИтерация 1:\\r\\nДальше сдвигаемся на 2 шага назад (на первой итерации) по ряду\\r\\nФибоначчи, получаем число 5. Для расчета индекса искомого значения в\\r\\nсписке используется формула:\\r\\n     i = min(idx+a, n-1), где\\r\\n        idx - индекс полученный на предыдущем шаге (или -1 на первой итерации)\\r\\n        a - рассчитанное нами число 5\\r\\n        n - длина списка\\r\\nЗначения idx=-1, a=5. Подставляем значения в формулу и считаем:\\r\\n    i = min(-1+5, 11-1) = min(4, 10) = 4\\r\\nИндекс искомого значения на первой итерации получили 4. Переходим к\\r\\nзначению списка с индексом 4 - это значение 13. Сравниваем полученное\\r\\nзначение 13 с искомым 28. Они не равны, следовательно продолжаем поиски дальше.\\r\\nИтерация 2:\\r\\nЕсли искомое значение не найдено, важно то, больше полученное значение\\r\\nили меньше. Если значение, которое мы получили в ходе расчетов меньше,\\r\\nчем искомое, то в ряду Фибоначчи мы смещаемся на 1 шаг назад (от числа 5 переходим к 3).\\r\\nТеперь idx=4, a=3. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на второй итерации получили 7. Переходим к значению\\r\\nсписка с индексом 7 - это значение 25. Сравниваем полученное значение 25\\r\\nс искомым 28. Они не равны, следовательно продолжаем поиски дальше.\\r\\nИтерация 3:\\r\\nПолученное значение 25 снова меньше искомого 28, следовательно в ряду\\r\\nФибоначчи мы смещаемся так же на 1 шаг назад (от числа 3 переходим к 2).\\r\\nТеперь idx=7, a=2. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на третьей итерации получили 9. Переходим к значению\\r\\nсписка с индексом 9 - это значение 34. Сравниваем полученное значение 34\\r\\nс искомым 28. Они не равны, следовательно продолжаем поиски дальше.'),\n",
              " '6ed00722-40f1-40e4-8257-29941af16700': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Продолжаем Поиск Фибоначчи\\r\\nИтерация 4:\\r\\nПосле третьей итерации полученное значение оказалось больше искомого,\\r\\nпоэтому алгоритм дальнейшего поиска немного меняется. Отличие в том, что\\r\\nзначение idx для расчетов мы возьмём не последнее рассчитанное (9), а\\r\\nполученное на предыдущей итерации (7). Что касается смещения в ряду\\r\\nФибоначчи, то теперь оно на 2 шага назад (от числа 2 переходим к 1).\\r\\nТеперь idx=7, a=1. Подставляем новые значения в формулу и считаем.\\r\\nИндекс значения на четвертой итерации получили 8. Переходим к значению\\r\\nсписка с индексом 8 - это значение 28.\\r\\nПолученное значение 28 сравниваем с искомым 28 - они равны,\\r\\nследовательно поиск окончен.\\r\\nИндекс искомого значения 28 равен 8.\\r\\n    def FibonacciSearch(nums, val):\\r\\n        fibM_minus_2 = 0\\r\\n        fibM_minus_1 = 1\\r\\n        fibM = fibM_minus_1 + fibM_minus_2\\r\\n        while (fibM < len(nums)):\\r\\n            fibM_minus_2 = fibM_minus_1\\r\\n            fibM_minus_1 = fibM\\r\\n            fibM = fibM_minus_1 + fibM_minus_2\\r\\n        index = -1;\\r\\n        while (fibM > 1):\\r\\n            i = min(index + fibM_minus_2, (len(nums)-1))\\r\\n            if (nums[i] < val):\\r\\n                fibM = fibM_minus_1\\r\\n                fibM_minus_1 = fibM_minus_2\\r\\n                fibM_minus_2 = fibM - fibM_minus_1\\r\\n                index = i\\r\\n            elif (nums[i] > val):\\r\\n                fibM = fibM_minus_2\\r\\n                fibM_minus_1 = fibM_minus_1 - fibM_minus_2\\r\\n                fibM_minus_2 = fibM - fibM_minus_1\\r\\n            else :\\r\\n                return i\\r\\n        if(fibM_minus_1 and index < (len(nums)-1) and nums[index+1] == val):\\r\\n            return index+1;\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39]\\r\\n    result = FibonacciSearch(sorted_list_of_nums, 13)\\r\\n    print('Значение 13 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 13 имеет индекс - 3\\r\\nСложность алгоритма поиска Фибоначчи\\r\\nВременная сложность данного алгоритма логарифмическая:\\r\\n     O(log n), где n — количество элементов списка.\\r\\nЭтот алгоритм в большинстве случаев работает быстрее, чем линейный поиск\\r\\nи jump search.\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nЭкспоненциальный поиск\\r\\n    # Алгоритм экспоненциального поиска\\r\\nЭкспоненциальный поиск это надстройка над бинарным поиском. Его\\r\\nособенность в том, что индекс при первичном поиске элемента 2^n, а после\\r\\nиспользуется алгоритм бинарного поиска.\\r\\n    def ExponentialSearch(nums, val):\\r\\n        if nums[0] == val:\\r\\n            return 0\\r\\n        index = 1\\r\\n        while index < len(nums) and nums[index] <= val:\\r\\n            prev_index = index\\r\\n            index = index * 2\\r\\n        return BinarySearch(nums[:min(index, len(nums))], val)\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39, 45, 55, 58]\\r\\n    result = ExponentialSearch(sorted_list_of_nums, 55)\\r\\n    print('Значение 55 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 55 имеет индекс - 10\\r\\nСложность алгоритма экспоненциального поиска\\r\\nВременная сложность данного алгоритма логарифмическая:\\r\\n     O(log n), где n — количество элементов списка.\"),\n",
              " '101acec8-30a5-4725-b8a7-21ee5864019b': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nЭкспоненциальный поиск работает лучше, чем бинарный, когда искомый\\r\\nэлемент находится ближе к началу массива.\\r\\nДанный алгоритм поиска считается одним из наиболее эффективных.\\r\\nИнтерполяционный поиск\\r\\nИнтерполяционный поиск является разновидностью бинарного поиска, только\\r\\nвероятную позицию (индекс) искомого элемента он вычисляет по формуле:\\r\\n    index = low + [(val-nums[low])*(high-low) / (nums[high]-nums[low])],\\r\\n    где low  - начальный индекс массива;\\r\\n        high - конечный индекс массива;\\r\\n        val  - искомое значение;\\r\\n        nums - исходный массив данных.\\r\\n    def InterpolationSearch(nums, val):\\r\\n        low = 0\\r\\n        high = (len(nums) - 1)\\r\\n        while low <= high and val >= nums[low] and val <= nums[high]:\\r\\n            index = low + int(((float(high - low) / ( nums[high] - nums[low])) * ( val - nums[low])))\\r\\n            if nums[index] == val:\\r\\n                return index\\r\\n            if nums[index] < val:\\r\\n                low = index + 1;\\r\\n            else:\\r\\n                high = index - 1;\\r\\n        return -1\\r\\n    # Проверяем работоспособность алгоритма\\r\\n    sorted_list_of_nums = [2, 4, 5, 13, 18, 25, 31, 37, 39, 45, 55, 58]\\r\\n    result = InterpolationSearch(sorted_list_of_nums, 45)\\r\\n    print('Значение 45 имеет индекс -', result)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Значение 45 имеет индекс - 9\\r\\nСложность алгоритма интерполяционного поиска\\r\\nДанный алгоритм работает быстрее на отсортированных, равномерно\\r\\nраспределенных массивах. Сложность алгоритма на таких массивах O(log log\\r\\nn), но если значения массива не равномерно распределены, тогда сложность\\r\\nалгоритма линейная:\\r\\n     O(n), где n — количество элементов списка.\"),\n",
              " 'e0df6937-e524-4b65-9acb-b84b551f86bb': Document(metadata={'meta': 'data'}, page_content=\"Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nСравнение рассмотренных алгоритмов поиска\\r\\nВ первой части ноутбука мы проводили сравнение разных алгоритмов\\r\\nсортировки при помощи библиотеки time. Для сравнения скорости работы\\r\\nалгоритмов поиска воспользуемся магической комендой %%timeit\\r\\n\\r\\n\\r\\n    import numpy as np\\r\\n    # Создадим массив из 50 рандомных значений и отсортируем его\\r\\n    random_list = np.random.randint(1,100,50)\\r\\n    random_list.sort()\\r\\n    print(random_list)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    [ 1  7  8  9 10 11 11 15 15 15 16 21 22 24 24 27 30 32 33 38 41 45 49 49 52 55 58 59 77 78 79 79 81 82 82 84 84 85 85 85 87 92 93 93 93 94 95 96 98 98]\\r\\nБудем искать значение 30\\r\\n    %%timeit\\r\\n    # Линейный поиск\\r\\n    result = LinearSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    4.75 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Бинарный поиск\\r\\n    result = BinarySearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    4.32 µs ± 197 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Jump search\\r\\n    result = JumpSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:        \\r\\n    5.81 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n        \\r\\n    %%timeit\\r\\n    # Поиск Фибоначчи\\r\\n    result = FibonacciSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    5.61 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n    %%timeit\\r\\n        \\r\\n    # Экспоненциальный поиск\\r\\n    result = ExponentialSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    6.47 µs ± 152 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n    %%timeit\\r\\n        \\r\\n    # Интерполяционный поиск\\r\\n    result = InterpolationSearch(random_list, 30)\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    8.84 µs ± 224 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\r\\n\\r\\n\\r\\nПостроим график средней скорости\\r\\n    import matplotlib.pyplot as plt\\r\\n\\r\\n\\r\\n    time_dict2 = {'linear':4.75,\\r\\n                  'binary':4.32,\\r\\n                  'jump':5.81,\\r\\n                  'fibonacci':5.61,\\r\\n                  'exponential':6.47,\\r\\n                  'interpolation':8.84}\\r\\n    plt.figure(figsize=(8,6))\\r\\n    plt.bar(time_dict2.keys(), time_dict2.values())\\r\\n    plt.show()\"),\n",
              " 'de5eb69f-5c27-4302-a430-9de81ebb6840': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Алгоритмы поиска и сортировки: важность и принципы работы\\r\\nНа уроке лектор рассказывает о важности алгоритмов поиска и сортировки в цифровой эпохе. Он объясняет, что алгоритмы - это рецепты или планы действий для решения задач. Алгоритмы помогают структурировать действия и делать их эффективнее. Лектор также говорит о том, что алгоритмы помогают оптимизировать ресурсы, такие как время и память, и решать задачи с большими данными быстрее и эффективнее. Он упоминает, что знание алгоритмов помогает понять работу различных библиотек и технологий в Python, таких как SQL и Docker. Лектор также объясняет, как оценивать алгоритмы, используя элементарные шаги и понятие Big O Notation, которое описывает сложность алгоритма. Он приводит примеры различных сложностей, таких как константная, линейная и логарифмическая, и объясняет, как они меняются с увеличением размера входных данных.\\r\\n\\r\\n\\r\\nТема: Квадратичная сложность и алгоритм сортировки пузырьком\\r\\nНа уроке рассказывается о сложности алгоритмов и различных методах сортировки. Лектор объясняет, что квадратичная сложность возникает, когда время выполнения алгоритма увеличивается пропорционально квадрату размера входных данных. Это происходит, например, при использовании двух вложенных циклов. Лектор также упоминает о других видах сложности, таких как кубическая и экспоненциальная.\\r\\nЗатем рассматривается пример алгоритма сортировки пузырьком, который является одним из самых простых. Лектор объясняет, как работает этот алгоритм и демонстрирует его на конкретном списке чисел. Он также обсуждает различные методы сортировки, такие как выборка, вставка, слияние и быстрая сортировка.\\r\\nВ конце урока лектор отвечает на вопросы студентов, в том числе о том, работает ли бинарный поиск только для сортированных списков. Он также объясняет, как поменять местами значения двух переменных без использования третьей переменной.\\r\\nВ целом, на уроке рассматривается тема сложности алгоритмов и методов сортировки, а также демонстрируется пример алгоритма сортировки пузырьком.'),\n",
              " '4a3f475b-029b-4487-b7b9-55d7d73d3104': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Сортировка выборкой и использование флага swap\"\\r\\nНа уроке лектор рассказывает о сортировке пузырьком и сортировке выборкой. Он объясняет, что в сортировке пузырьком используется флаг swap, который указывает, нужно ли повторно запускать цикл FOR для проверки и перестановки элементов. Если после прохождения цикла FOR по списку не было перестановок, то флаг swap становится false и цикл больше не запускается. Лектор также объясняет, что в сортировке выборкой на каждой итерации находится самый маленький элемент и ставится на первое место. Он также демонстрирует код и объясняет его работу. В конце лектор отвечает на вопросы слушателей и переходит к объяснению следующего алгоритма сортировки.\\r\\n\\r\\n\\r\\nТема: Работа флага в цикле while\\r\\nНа уроке рассказывается о сортировке списка значений с использованием флага в цикле while. Лектор объясняет, что сортировка значений может быть полезна в различных задачах и что существуют разные способы сортировки, каждый из которых может быть оптимальным для конкретной задачи. \\r\\nДалее лектор объясняет алгоритм сортировки с использованием флага. Он приводит пример списка значений и объясняет, как происходит сравнение и перестановка элементов в цикле. Лектор также отмечает, что в данном алгоритме используется переменная \"last value index\", которая хранит индекс минимального элемента, и что после каждой итерации цикла значение этой переменной изменяется. \\r\\nЛектор также отмечает, что данный алгоритм сортировки отличается от предыдущего алгоритма, который был рассмотрен ранее. Он объясняет, что в данном алгоритме происходит поиск минимального элемента и его перестановка с первым элементом списка, а затем происходит сортировка оставшейся части списка. \\r\\nЛектор также обсуждает различные алгоритмы сортировки, такие как сортировка вставками, и объясняет, что выбор алгоритма сортировки может существенно влиять на скорость выполнения программы. Он отмечает, что некоторые алгоритмы могут иметь квадратичную сложность, что может привести к значительному увеличению времени выполнения программы при большом количестве элементов в списке. \\r\\nВ конце урока лектор предлагает рассмотреть подробнее код алгоритма сортировки с использованием флага и объясняет его работу. Он также отмечает, что понимание различных алгоритмов сортировки и их оптимизация могут помочь в оптимизации кода и улучшении производительности программы.'),\n",
              " 'e2d43b64-d00b-4fa7-9380-ef2a4d607f36': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: \"Поиск с помощью прыжков и последовательность Фибоначчи\"\\r\\nНа уроке рассказывается о поиске элемента в последовательности с помощью алгоритма Jump Search и о последовательности чисел Фибоначчи. \\r\\nАлгоритм Jump Search основан на прыжках через определенные интервалы в последовательности. Длина интервала определяется как корень из длины последовательности, округленный до целого числа. Если длина последовательности не делится нацело на корень, то остаток отбрасывается. Алгоритм прыгает через интервалы, пропуская элементы, пока не найдет элемент, который больше или равен искомому значению. Затем алгоритм возвращает индекс найденного элемента или -1, если элемент не найден.\\r\\nЧисла Фибоначчи - это последовательность чисел, где каждое число равно сумме двух предыдущих чисел. Начальные значения последовательности - 0 и 1. Числа Фибоначчи имеют много интересных свойств и применений, включая золотое сечение в фотографии. Золотое сечение используется для создания эстетически приятных кадров, где объекты попадают на пересечение определенных рамок в сетке Фибоначчи.\\r\\n\\r\\n\\r\\nТема: \"Использование чисел Фибоначчи для поиска удачных кадров\"\\r\\nНа уроке рассказывается о том, как использовать числа Фибоначчи и золотое сечение для создания удачных кадров в фотографии. Лектор объясняет, что когда объект на снимке совпадает с спиралью Фибоначчи, получается более гармоничное и привлекательное изображение. Затем он предлагает использовать ряд Фибоначчи для определения максимального значения, от которого будем отталкиваться при поиске удачных кадров. Далее он приводит пример кода, который использует значения Фибоначчи для поиска искомого элемента в списке. Цикл выполняется до тех пор, пока значение Фибоначчи не превысит длину списка или не станет равным ей. Затем происходит поиск искомого элемента в списке с использованием значений Фибоначчи. Если значение больше искомого элемента, то значения Фибоначчи обновляются, и цикл продолжается.'),\n",
              " 'cce5c896-23c5-4b88-a3b6-6ec98d9b9b8e': Document(metadata={'meta': 'data'}, page_content='Урок: Алгоритмическое мышление - Алгоритмы поиска и сортировки\\r\\nТема: Алгоритмы поиска: минус 1, плюс, F2, минус 1, плюс 2, получается 1, и i падает по первому индексу.\\r\\nНа уроке лектор рассказывает о различных алгоритмах поиска. Он начинает с алгоритма \"прыжкового поиска\", который основан на итеративном сравнении значения с элементами списка и изменении индекса в зависимости от результата сравнения. Затем он переходит к алгоритму \"экспоненциального поиска\", который является модификацией бинарного поиска и использует увеличение индекса в два раза на каждой итерации. Лектор также упоминает о том, что экспоненциальный поиск может быть ограничен диапазоном, чтобы ускорить процесс. Затем он переходит к алгоритму \"интерполяционного поиска\", который использует интерполяционную функцию для приближенного определения положения искомого значения в списке. Лектор объясняет, что каждый из этих алгоритмов имеет свои преимущества и недостатки, и выбор алгоритма зависит от конкретной задачи.\\r\\n\\r\\n\\r\\nТема: сравнение\\r\\nНа уроке лектор рассказывает о различных алгоритмах поиска в массиве данных. \\r\\nОн объясняет, что линейный поиск осуществляется путем последовательного перебора элементов массива до нахождения нужного значения. \\r\\nБинарный поиск, в свою очередь, применяется только к отсортированным массивам и заключается в делении массива пополам и сравнении искомого значения с серединным элементом. Если искомое значение больше, то поиск продолжается во второй половине массива, иначе - в первой. Джамп-поиск использует шаги фиксированной длины для перехода к ближайшему элементу, превышающему искомое значение, а затем осуществляет линейный поиск в предыдущем блоке. Интерпретационный поиск основан на интерпретации значений массива как индексов искомого значения. \\r\\nЛектор также сравнивает эффективность этих алгоритмов и объясняет, что выбор оптимального алгоритма зависит от характеристик входных данных. \\r\\nОн также упоминает, что следующий урок будет посвящен графам.\\r\\n1062_Алгоритмическое мышление_Алгоритмы поиска и сортировки.txt\\r\\nОткрыть с помощью...\\r\\n  \\r\\n\\r\\n1062_Алгоритмическое мышление_Алгоритмы поиска и сортировки.txt. На экране.'),\n",
              " '98f1eaa1-3276-4da3-bbde-30eb657358bb': Document(metadata={'meta': 'data'}, page_content='\\ufeff<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nВведение.\\r\\nБиологические связи.\\r\\nВначале немного истории. Когда вы впервые услышали термин сверточные\\r\\nнейронные сети, возможно, подумали о чем-то, связанном с нейронауками\\r\\nили биологией, и отчасти были правы.\\r\\n  Сверточные нейронные сети – это своеобразная имитация зрительной коры\\r\\n  мозга. Зрительная кора имеет небольшие участки клеток, которые\\r\\n  чувствительны к конкретным областям поля зрения. Эту идею в 1962 году\\r\\n  детально рассмотрели Хьюбел и Визель с помощью потрясающего\\r\\n  эксперимента (видео).\\r\\n  Они показали, что отдельные мозговые нервные клетки реагировали (или\\r\\n  активировались) только при визуальном восприятии границ определенной\\r\\n  ориентации. Например, некоторые нейроны активировались, когда\\r\\n  воспринимали вертикальные границы, а некоторые — горизонтальные или\\r\\n  диагональные.\\r\\n  Хьюбел и Визель выяснили, что все эти нейроны организованы в блоки\\r\\n  стержневой архитектуры и вместе формируют визуальное восприятие. Эту\\r\\n  идею специализированных компонентов внутри системы, которые решают\\r\\n  конкретные задачи (как клетки зрительной коры, которые ищут\\r\\n  специфические характеристики), и взяли за основу построения сверточных\\r\\n  нейронных сетей (СНС, CNN).\\r\\n\\r\\n\\r\\n\"Входы\" и \"Выходы\"\\r\\nКогда вы смотрите на происходящее вокруг, чаще всего вы можете сразу\\r\\nохарактеризовать место действия и узнать окружающие объекты. Когда вы\\r\\nсмотрите на изображение собаки, вы без труда узнаете ее (при наличии\\r\\nхарактерных особенностей, которые можно идентифицировать, например,\\r\\nлапы).\\r\\nВсе это происходит бессознательно, незаметно для разума. Это и есть\\r\\nнавыки быстрого распознавания шаблонов и обобщения уже полученных\\r\\nзнаний.\\r\\nКогда компьютер \"видит\" изображение (принимает данные на вход), он имеет\\r\\nдело с массивом чисел от 0 до 255, описывающих интенсивность каждого\\r\\nпикселя.\\r\\nРазмер массива чисел зависит от разрешения и размера изображения:\\r\\n-   для цветного (RGB, 3 цвета) с размерами картинки 32 x 32 - это 32х32х3.\\r\\n-   для монохромного (Grey, 1 цвет) с размерами картинки 480 x 480 - это 480х480х1.\\r\\nЗначения пикселей, визуально оставаясь бессмысленными для вас,\\r\\nстановятся единственными вводными данными об изображении, доступными\\r\\nнейросети. Вы подаете ей на вход эту матрицу, а на выходе получаете\\r\\nзначения вероятностей принадлежности картинки (входа) к тому или иному\\r\\nклассу. Например, если классов три, то вы можете получить список [0.80,\\r\\n0.15, 0.05], что означает уверенность нейросети в 80%, что это кошка;\\r\\n15%, что это собака; 5%, что это птица.'),\n",
              " '8657993f-b7dd-4ffe-9ba8-b4659fa78fb5': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСтруктура сверточной нейронной сети\\r\\nСферы применения сверточных нейронных сетей включают:\\r\\n-   работу с изображениями: это великолепный способ структурирования\\r\\n    визуальной информации, построения так называемой инвариантности -\\r\\n    выделения постоянных признаков. Например, возьмём идею распознавания\\r\\n    образов на сцене или фотографии: вы хотите понять, изображён человек\\r\\n    на ней или нет. Неважно, в какой части фотографии он располагается,\\r\\n    где его голова — в центре фотографии или в углу. Здесь инвариантный\\r\\n    (неизменный) признак - голова, которая выделяется и распознается\\r\\n    свёрточной нейронной сетью в разных местах изображения.\\r\\n-   распознавание объектов по фото, обработка медицинских снимков,\\r\\n    космоснимков и пр.\\r\\n-   работу с аудио и видео.\\r\\n-   применение в языковых технологиях, где специалисты используют\\r\\n    глубокое обучение для извлечения смысла и генерации предложений на\\r\\n    естественном языке.\\r\\nЗадача нейронщика в том, чтобы правильно подобрать архитектуру и научить\\r\\nнейросеть распознавать уникальные особенности объектов (аналогично мозгу\\r\\nчеловека), и на этом основании классифицировать все подаваемые на вход\\r\\nнейросети изображения.\\r\\nНейросеть может классифицировать изображения через поиск характеристик\\r\\nбазового уровня, например, границ и кривых. Следующий сверточный слой\\r\\nбудет анализировать уже не исходное изображение, а найденные объекты\\r\\n(границы и кривые), обнаруживая признаки более высокого уровня: контуры,\\r\\nфигуры и т.п. Таким образом, имея группу сверточных слоев, нейросеть\\r\\nсможет выдать требуемый для нас результат, проведя серию операций по\\r\\nобнаружению определенных элементов изображения (при этом каждый новый\\r\\nслой будет искать признаки более высокого уровня, опираясь на результаты\\r\\nодного или нескольких предыдущих слоев).\\r\\n\\r\\n\\r\\nТаково общее описание работы сверточных нейронных сетей. Теперь вы\\r\\nможете углубиться в их структуру.\\r\\nСравнение полносвязных и сверточных сетей\\r\\nСвёрточные нейронные сети очень похожи на обычные полносвязные нейронные\\r\\nсети, которые вы изучали ранее: они состоят из нейронов, которые, в свою\\r\\nочередь, содержат изменяемые в процессе обучения веса и смещения.\\r\\nКаждый нейрон получает какие-то входные данные, вычисляет их скалярное\\r\\nпроизведение с весами и, опционально, использует нелинейную функцию\\r\\nактивации.\\r\\nВся сеть по-прежнему представляет собой единственную дифференцируемую\\r\\nфункцию: от исходного набора пикселей (изображения) на входе до\\r\\nраспределения вероятностей принадлежности к определённому классу на\\r\\nвыходе.\\r\\n\\r\\n\\r\\nУ этих сетей по-прежнему есть функция-классификатор (например, Softmax)\\r\\nв последнем (полносвязном) слое, и все те советы и рекомендации, которые\\r\\nбыли даны к обычным нейронным сетям, применимы и для свёрточных\\r\\nнейронных сетей.\\r\\nТак что же изменилось? Архитектура свёрточных нейронных сетей явно\\r\\nпредполагает получение на входе изображений, что позволяет нам учесть\\r\\nопределённые свойства входных данных в самой архитектуре сети.\\r\\nОбычные нейронные сети плохо масштабируются для больших изображений. В\\r\\nнаборе данных CIFAR-10, например, изображения имеют размер 32х32х3 (32\\r\\nпикселя высота, 32 пикселя ширина, 3 цветовых канала).\\r\\nДля обработки такого изображения полносвязный нейрон в первом скрытом\\r\\nслое обычной нейронной сети будет иметь 32х32х3 = 3072 весов. Такое\\r\\nколичество всё ещё является допустимым, но становится очевидным тот\\r\\nфакт, что подобная структура не будет работать с изображениями большего\\r\\nразмера. Например, для изображения большего размера — 200х200х3 —\\r\\nколичество весов станет равным 200х200х3 = 120000.\\r\\nБолее того, нам понадобится не один подобный нейрон, поэтому общее\\r\\nколичество параметров модели (весов) начнет расти очень быстро.\\r\\nСтановится очевидным тот факт, что полносвязность избыточна, и слишком\\r\\nбольшое количество параметров быстро приведет к переобучению.'),\n",
              " '8dbb1aea-8780-4fb7-8016-7ea6f2510b39': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСверточный слой Conv2D.\\r\\nСуществуют одномерные, двумерные и даже трехмерные сверточные слои. В\\r\\nданном уроке будут рассмотрены двумерные сверточные слои.\\r\\nДля моделей нейронных сетей, построенных на основе класса Sequential,\\r\\nдля создания слоя используйте конструкцию:\\r\\n        .add(Conv2D(32, (3, 3), padding=\\'same\\', activation=\\'relu\\', strides=(1,1))\\r\\n1.  Первый параметр 32 - количество ядер (фильтров) свертки.\\r\\n2.  Второй параметр (3, 3) – размер ядра свертки.\\r\\n3.  Третий параметр padding=\\'same\\' - тип заполнения краев, нужен для\\r\\n    сохранения размеров изображения, по умолчанию значение padding=\\'valid\\'.\\r\\n4.  Четвертый параметр activation=\\'relu\\' – указание функции активации.\\r\\n5.  Пятый параметр strides=(1, 1) – необязательный, задает шаг смещения\\r\\n    фильтра (подробнее в следующем разделе).\\r\\n------------------------------------------------------------------------\\r\\nДополнительная информация (База знаний УИИ - «Функции активации») Ссылка: https://colab.research.google.com/drive/1pGc7CFdrkKBhcXLqZNUzLXH4N83rRAl7?usp=sharing\\r\\n-----------------------------------------------------------------------\\r\\nДополнительная информация (База знаний УИИ - «Сверточный слой Conv2D») Ссылка: https://colab.research.google.com/drive/1bQNGBTEqen_QiYBDGqQ5Uu3iqet_G5ps?usp=sharing\\r\\n------------------------------------------------------------------------\\r\\n\\r\\n\\r\\n<Chunk>\\r\\nУрок: Сверточные нейронные сети. Теоретическая часть.\\r\\nРазличия полносвязного и сверточного слоев.\\r\\nПринципиальное различие между полносвязным и сверточным слоем состоит в\\r\\nтом, что первый изучает глобальные характеристики изображения целиком, а\\r\\nвторой - локальные шаблоны в небольших фрагментах изображения.\\r\\nМинус полносвязного слоя в том, что:\\r\\n-   он обладает низкой вариативностью. То, что мы будем подавать на\\r\\n    вход, должно быть практически идентично тому, на чем обучали, даже\\r\\n    смещение объекта на листе уже критично. Локализованные признаки он\\r\\n    не обнаружит.\\r\\n-   при использовании полносвязного слоя каждый нейрон связан с каждым\\r\\n    нейроном предыдущего слоя, т.е. каждый нейрон анализирует все\\r\\n    изображение и может найти ложные зависимости.\\r\\nВариант решения: создаем \"маленький Dense\" и им пробегаем по всему\\r\\nизображению в поисках совпадения. Это позволит обнаружить объект, даже\\r\\nесли он уменьшен или смещен.\\r\\nПринцип работы сверточного слоя\\r\\nОсновная задача сверточного слоя – выделить признаки во входном\\r\\nизображении ядрами свертки (фильтрами) и сформировать карту признаков в\\r\\nвиде тензора.\\r\\nКарта признаков или Карта активации - это обработанное ядром свертки исходное изображение.\\r\\nЯдра свертки (фильтры) - это тензоры одного размера (задается при настройке слоя). Количество ядер в слое определяет глубину выходного массива (т.е. количество ядер вместе с разрешением изображения определяют форму выходного тензора).\\r\\nСвёртка – это операция вычисления нового значения на основе значения\\r\\nвыбранного пикселя и значений окружающих его пикселей.\\r\\n------------------------------------------------------------------------\\r\\nАлгоритм свёртки можно описать так:\\r\\n-   фильтр накладывается на левую верхнюю часть входящего изображения;\\r\\n-   производится поэлементное умножение значений фильтра и значений\\r\\n    пикселей изображения;\\r\\n-   полученные значения складываются, сумма будет результатом свертки\\r\\n    области наложения (одно число);\\r\\n-   фильтр перемещается дальше по изображению (за смещение отвечает\\r\\n    параметр strides, по умолчанию равен (1,1), т.е. смещение на 1\\r\\n    пиксель вправо, при достижении конца строки сдвиг вниз на 1 пиксель,\\r\\n    и вновь с начала строки);\\r\\n-   в новом положении окна фильтра производится поэлементное умножение\\r\\n    значений фильтра ...\\r\\n-   повтор до тех пор, пока аналогичным образом не будут обработаны все\\r\\n    участки.'),\n",
              " 'ca14d200-33fa-490e-ad8e-eb10a00a0c0d': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nЗаполнение (Padding).\\r\\nКогда мы выбираем достаточно малый размер ядра фильтра, потеря в\\r\\nразмерах выходного слоя невелика. Если же это значение увеличивается, то\\r\\nи размеры выходного слоя уменьшается по отношению к оригинальному\\r\\nразмеру входного изображения. Это может стать проблемой, особенно если в\\r\\nмодели нейронной сети подключено последовательно много сверточных слоев.\\r\\nЧтобы избежать потерь в разрешении выходных изображений, в настройках\\r\\nсверточных слоев используют дополнительный параметр – заполнение\\r\\n(padding), позволяющий расширить полученное изображение по краям до того\\r\\nже размера, что и на входе свертки.\\r\\nПример: если мы по картинке размером 5 x 5 пикселей проходим ядром 3 x\\r\\n3, то в конце получим потерю в размерах.\\r\\nРешение: добавляем параметр padding=\\'same\\', тогда Keras дополнит\\r\\nвыходное изображение по внешней границе нулями (другие числа исказили бы\\r\\nинформацию, а нули дают только изменение размера), расположит их так,\\r\\nчтобы выходной массив не потерял в размерах. Заполнение распределяется\\r\\nпо границам исходя из того, как много потерь нужно возместить.\\r\\n\\r\\n\\r\\nРазмеры ядра свертки.\\r\\nСвёрточные нейронные сети используют допущение, что входные данные —\\r\\nизображения, поэтому они образуют более чувствительную архитектуру к\\r\\nподобному типу данных.\\r\\nВ частности, в отличие от обычных нейронных сетей, слои в свёрточной\\r\\nнейронной сети располагают ядра свертки в трех измерениях — ширине,\\r\\nвысоте, глубине (здесь термин \"глубина\" относится к третьему измерению\\r\\nядер, а не глубине самой нейронной сети, измеряемой в количестве слоёв).\\r\\nНапример, входные изображения из набора данных CIFAR-10 являются\\r\\nвходными данными в 3D-представлении, форма которых равна 32х32х3\\r\\n(ширина, высота, глубина). Как мы увидим позднее, нейроны в одном слое\\r\\nбудут связаны с небольшим количеством нейронов предыдущего слоя, вместо\\r\\nтого чтобы быть связанными с ними всеми.\\r\\nТаким образом, глубина фильтров первого слоя совпадает с количеством\\r\\nканалов входного изображения. Если на вход свёрточному слою подаётся RGB\\r\\nизображение (3 канала) и требуется получить 32 карты признаков, то\\r\\nсвёрточный слой должен содержать в себе 32 фильтра глубиной 3.\\r\\n\\r\\n\\r\\nВеса фильтра, который пробегает по изображению, не изменяются во время\\r\\nсамого прохождения, но обучаются от батча к батчу. В результате обучения\\r\\nфильтр начинает отвечать за поиск определенного признака. в конце концов\\r\\nодно ядро будет выделять горизонтальную прямую, другое – вертикальную,\\r\\nтретье – диагональ и т.п.\\r\\nДля обработки изображений (не только нейросетями) нередко требуется\\r\\nрешать конкретные задачи: выделять границы, повышать резкость или\\r\\nприменять размытие.\\r\\nДля подобных целей были получены сверточные фильтры, которые сегодня\\r\\nчасто встраиваются в различные графические редакторы.\\r\\nСамые распространенные размеры ядра двумерных сверточных слоев – 3х3,\\r\\n5х5 и 7х7. Количество ядер, как правило, кратно двум – 8, 16, 32, 64 и\\r\\nт.д. Но никто не запрещает использовать ядро 3х7 или 27х27 и количество\\r\\nядер 315.\\r\\nНапример, такие ядра свертки используются в фильтрах Photoshop.'),\n",
              " 'de84f61b-304c-4532-b0ad-7f7c9fe22920': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСлой подвыборки (Pooling-слой).\\r\\nОсновные задачи слоев типа Pooling:\\r\\n1.  Распознавание объектов вне зависимости от масштаба;\\r\\n2.  Факт наличия признака важнее знания места его точного положения на\\r\\n    изображении.\\r\\nЭтот слой немного похож на сверточный, поскольку у него тоже есть ядро\\r\\n(окно фильтра). Но, в отличие от сверточного слоя, он уменьшает размер\\r\\nизображения, выбирая максимальное (MaxPooling), среднее (AveragePooling)\\r\\nили суммарное (SumPooling) значение из окна фильтра. В некотором смысле\\r\\nслой подвыборки делает информацию более сконцентрированной, обобщенной.\\r\\nСлой подвыборки имеет один обязательный параметр — pool_size (размер\\r\\nокна подвыборки), и один необязательный - strides (шаг смещения окна).\\r\\nПричем если strides не указан, то по умолчанию strides=pool_size, то\\r\\nесть окно смещается на размер фильтра.\\r\\nНаиболее часто используется слой MaxPooling с ядром (2, 2) и смещением\\r\\nпо умолчанию - он уменьшает размеры входного тензора по ширине и высоте\\r\\nв два раза. Поэтому рекомендуется использовать такие разрешения\\r\\nизображений, чтобы размеры по обоим измерениям делились на 2 без\\r\\nостатка, иначе при уменьшении размеров часть информации будет потеряна.\\r\\n  Если \"сжать\" слоем MaxPooling изображение размером 15 x 15, на выходе\\r\\n  получим 7 x 7. А при \"разжатии\" изображения слоем Upsampling или\\r\\n  Conv2DTranspose (выполняют что-то вроде обратной к MaxPooling\\r\\n  операции) получим 14 x 14, что не совпадет с исходными размерами.\\r\\nНекоторые библиотеки позволяют задавать раздельные параметры уменьшения\\r\\nпо высоте и ширине, создавать прямоугольное ядро подвыборки. Однако чаще\\r\\nвсего оно квадратное.\\r\\nЧтобы добавить слой, используйте:\\r\\n    .add(MaxPooling2D(pool_size=(2, 2)), где (2,2) – размер окна, в котором выбирается максимальное значение.\\r\\n\\r\\n\\r\\nФинальная классификация данных.\\r\\nПосле прохождения через сверточные слои и слои подвыборки данные\\r\\nнеобходимо систематизировать и классифицировать. Для этого применяют\\r\\nполносвязные слои Dense, которые вы изучили на предыдущих уроках.\\r\\nЧтобы правильно передать данные от сверточного слоя на полносвязный,\\r\\nнужно сделать данные одномерными. Для данной задачи подходят слои:\\r\\n-   Flatten() - \"сплющивает\" многомерные входные данные в одномерный\\r\\n    вектор, при этом размеры данных по всем осям перемножаются;\\r\\n    дополнительных параметров нет. Например, входящее изображение формы\\r\\n    (28, 28, 3) преобразуется в вектор формы (2352).\\r\\n-   Reshape(...) - смена формы данных. Требуется указать в скобках\\r\\n    желаемую форму данных. Позволяет не только вытягивать данные в\\r\\n    вектор, но и произвольно менять форму, например (28, 28, 3)\\r\\n    преобразовать в (3, 784), или в (14, 14, 12). Объем формы данных\\r\\n    (произведение размеров по всем осям) на входе должен совпадать с\\r\\n    объемом желаемой формы данных.\\r\\nВ зависимости от задачи, выходной Dense-слой может вычислять вероятности\\r\\nдля каждого класса или выдавать номер (метку) класса.'),\n",
              " '283918e4-1482-4613-a595-b54ee31823a2': Document(metadata={'meta': 'data'}, page_content='Урок: Сверточные нейронные сети. Теоретическая часть.\\r\\nСоздание простой модели сверточной нейронной сети.\\r\\nПодключите основу – класс создания последовательной модели Sequential:\\r\\n    from tensorflow.keras.models import Sequential\\r\\nС помощью него создайте экземпляр вашей модели:\\r\\n    model = Sequential()\\r\\nЭто и есть ваша модель! Сейчас она больше похожа на пустую коробку.\\r\\nЧтобы она что-то делала, нужно поместить в нее какой-нибудь механизм.\\r\\nЭто не механизм в обычном смысле слова, потому что вы будете оперировать\\r\\nне предметами, а информацией – главным ресурсом XXI века. Механизм будет\\r\\nпринимать на вход и выдавать на выход какие-то данные.\\r\\nТак из чего же вы можете создать механизм? Для начала определитесь,\\r\\nсколько информации вы будете давать нейросети на вход. Один экземпляр\\r\\nтакой информации называется объектом. Не углубляйтесь пока, какими они\\r\\nбывают и как устроены. Сейчас достаточно знать, что объекты всегда\\r\\nсостоят из чисел.\\r\\nНапример, вы решили, что ваши объекты - изображения. Для подачи в\\r\\nнейросеть их надо оцифровать.\\r\\nУ изображений есть высота img_height, ширина img_width и количество\\r\\nцветовых каналов channels.\\r\\nИх называют входной формой (или формой входных данных) и записывают как:\\r\\n    input_shape=(img_height, img_width, channels)\\r\\n------------------------------------------------------------------------\\r\\nВажно: все изображения, которые вы подаете на вход нейронной сети,\\r\\nдолжны иметь общие высоту, ширину и количество каналов. Ниже вы узнаете,\\r\\nкак это сделать.\\r\\n------------------------------------------------------------------------\\r\\nДобавьте в модель первый слой при помощи .add():\\r\\n    from tensorflow.keras.layers import Conv2D\\r\\n    # Первый сверточный слой\\r\\n    model.add(Conv2D(8, (3, 3), padding=\\'same\\', activation=\\'relu\\', input_shape=(28, 56, 3)))\\r\\nРасшифруем написанное выше: первый сверточный слой принимает на вход\\r\\nцветное изображение (3 канала) размерами 28 на 56 пикселей. То есть\\r\\nформа входящего массива - (28, 56, 3).\\r\\nВнутри слоя к нему применяется свертка 8-ю фильтрами (3, 3) с шагом смещения (1, 1), а затем функция активации relu.\\r\\nКакой формы получится выходной массив?\\r\\nОбратите внимание, что padding =\\'same\\', stride=(1,1) по умолчанию;\\r\\nвычислим pad = (size - 1) / 2 = (3-1) /2 = 1.\\r\\nИспользуем формулу:\\r\\n    output_h = (input_h + 2 * pad - size) // stride + 1 = (28 + 2 * 1 - 3) // 1 + 1 = 27 + 1 = 28\\r\\n    output_w = (input_w + 2 * pad - size) // stride + 1 = (56 + 2 * 1 - 3) // 1 + 1 = 55 + 1 = 56\\r\\nКак видите, при стандартном stride и padding =\\'same\\' длина и ширина\\r\\nвходного и выходного массивов сверточного слоя равны. Но отличие все-таки будет - это глубина!\\r\\nНа вход пришло 3 канала, а сверточный слой имеет 8 ядер свертки, каждое\\r\\nиз которых выдает свою карту признаков, обработав любое количество\\r\\nканалов. Значит, глубина на выходе будет 8 вместо 3. Полная форма данных\\r\\nна выходе получится (28, 56, 8).\\r\\nПроверьте это методом модели .summary():\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 224\\r\\n    Trainable params: 224\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nОтлично!\\r\\nПосмотрим, что будет, если добавить следующий сверточный слой c 5\\r\\nфильтрами с ядром (3, 2), шагом смещения (2, 3) и padding =\\'valid\\':\\r\\n    # Второй сверточный слой\\r\\n    model.add(Conv2D(5, (3, 2), strides = (2,3), padding=\\'valid\\', activation=\\'relu\\'))\\r\\nЕсли padding=\\'valid\\', то по правилам pad = 0. Подставим значения:\\r\\n        output_h = (input_h + 2 * pad - size) // stride + 1 = (28 + 2 * 0 - 3) // 2 + 1 = 25 // 2 + 1 = 12 + 1 = 13\\r\\n        output_w = (input_w + 2 * pad - size) // stride + 1 = (56 + 2 * 0 - 2) // 3 + 1 = 54 // 3 + 1 = 18 + 1 = 19\\r\\nУ вас 5 фильтров, значит форма данных на выходе получится (13, 19, 5).\\r\\nПроверьте:\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nПримените слой MaxPooling2D:\\r\\n    from tensorflow.keras.layers import MaxPooling2D \\r\\n    # Слой подвыборки\\r\\n    model.add(MaxPooling2D(pool_size=(3, 3)))\\r\\nMaxPooling2D изменит форму данных следующим образом (учитывая, что\\r\\nstride=pool_size):\\r\\n     output_h = (input - pool_size) // strides + 1 = (13 - 3) // 3 + 1 = 4 \\r\\n     output_w = (input - pool_size) // strides + 1 = (19 - 3) // 3 + 1 = 6\\r\\nГлубина в MaxPooling2D не меняется, выходная форма данных (4, 6, 5).\\r\\nПроверьте:\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nДалее примените слой Flatten, который вытягивает входящий тензор в\\r\\nодномерный вектор. На входе слоя ожидается тензор (4, 6, 5), а на выходе\\r\\nбудет вектор (4 * 6 * 5) = (120)\\r\\n    from tensorflow.keras.layers import Flatten\\r\\n    # Слой преобразования многомерных данных в одномерные \\r\\n    model.add(Flatten())\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n     flatten (Flatten)           (None, 120)               0         \\r\\n                                                                     \\r\\n    =================================================================\\r\\n    Total params: 469\\r\\n    Trainable params: 469\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nНужно обратить внимание, что размерность первых двух элементов тензора\\r\\nперед Flatten слоем (в этом случае, 4 х 6) не должна быть очень\\r\\nбольшая - можно добавлять слои Conv2D и MaxPooling2D пока эта\\r\\nразмерность не станет равна 1 х 1. Например, если бы мы не добавили слой\\r\\nMaxPooling2D, эта размерность была бы равна 13 х 19, что бы было уже\\r\\nоднозначно много.\\r\\nЕсли размерность, которая мы подаем в Flatten() слой слишком большая,\\r\\nследующий слой не сможет извлечь достаточно значемые признаки для\\r\\nправильной классификации, потому что на вход слоя приходят слишком много\\r\\nданных.\\r\\n------------------------------------------------------------------------\\r\\nДалее мы создадим последний, выходной слой Dense. Он получит на вход\\r\\nодномерный вектор (120), а на выходе выдаст одномерный вектор (3).\\r\\nАктивационная функция softmax выдаст вероятности принадлежности входных\\r\\nданных к каждому из трех классов.\\r\\nПроверьте:\\r\\n    from tensorflow.keras.layers import Dense\\r\\n    model.add(Dense(3, activation=\\'softmax\\'))\\r\\n    model.summary()\\r\\nПосле запуска ячейки на экране увидим:\\r\\n    Model: \"sequential\"\\r\\n    _________________________________________________________________\\r\\n     Layer (type)                Output Shape              Param #   \\r\\n    =================================================================\\r\\n     conv2d (Conv2D)             (None, 28, 56, 8)         224       \\r\\n                                                                     \\r\\n     conv2d_1 (Conv2D)           (None, 13, 19, 5)         245       \\r\\n                                                                     \\r\\n     max_pooling2d (MaxPooling2D  (None, 4, 6, 5)          0         \\r\\n     )                                                               \\r\\n                                                                     \\r\\n     flatten (Flatten)           (None, 120)               0         \\r\\n                                                                     \\r\\n     dense (Dense)               (None, 3)                 363                                                                           \\r\\n    =================================================================\\r\\n    Total params: 832\\r\\n    Trainable params: 832\\r\\n    Non-trainable params: 0\\r\\n    _________________________________________________________________\\r\\n\\r\\n\\r\\nВот вы и построили простую сверточную сеть. Теперь у вас есть небольшой\\r\\nопыт, который пригодится для построения более сложной сети, чтобы решить\\r\\nзадачу классификации.\\r\\nПрактический ноутбук 1 - ссылка: https://colab.research.google.com/drive/1wR8vbQ1LSJy1ZER3wIgXENUT0K-Q15Qh?usp=sharing\\r\\nПрактический ноутбук 2 - ссылка: https://colab.research.google.com/drive/1f95JuEQcz2LYrCbWS3GnvnXUSN2GL2HR?usp=sharing\\r\\n\\r\\n\\r\\n761_1_Базовый_блок___Сверточные_нейронные_сети_(Теория)___УИИ.txt')}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ly3zeou0cgg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bepau_1GcgjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}