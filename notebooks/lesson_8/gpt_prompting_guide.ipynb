{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNhVAX0oQntj"
      },
      "source": [
        "Напишите функцию, которая принимает ввод от пользователя и возвращает ответ от модели GPT. Ваша функция должна использовать код, представленный в материалах, и должна быть способна обрабатывать любой ввод пользователя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Bnkz_MXSf_u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mistralai in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.5)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mistralai) (0.2.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mistralai) (0.27.2)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mistralai) (1.0.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mistralai) (2.10.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mistralai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mistralai) (0.9.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (4.7.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.6)\n",
            "Requirement already satisfied: sniffio in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->mistralai) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sasakinme\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZBK1D7yE_x_"
      },
      "outputs": [],
      "source": [
        "# если вы не используете colab secrets\n",
        "import getpass\n",
        "import os\n",
        "from mistralai import Mistral\n",
        "# Получение ключа API от пользователя и установка его как переменной окружения\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"] \n",
        "#api_key = getpass.getpass(\"MISTRAL_API_KEY:\")\n",
        "#os.environ[\"MISTRAL_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0OroQ5UgFAK1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openai'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# запустите эту ячейку, если используете секретный ключ в колабе\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistralai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mistral\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
          ]
        }
      ],
      "source": [
        "# запустите эту ячейку, если используете секретный ключ в колабе\n",
        "import os\n",
        "from mistralai import Mistral\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"MISTRAL_API_KEY\")\n",
        "os.environ[\"MISTRAL_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jObwGHZWG_8P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id='ed9b5f6e6a1148bb898dfe257ef84e01' object='chat.completion' model='mistral-small-latest' usage=UsageInfo(prompt_tokens=16, completion_tokens=20, total_tokens=36) created=1734121712 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Claude Monet, the founder of French Impressionism, is often considered the best French painter.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n"
          ]
        }
      ],
      "source": [
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "def get_gpt_response(user_input):\n",
        "  res = client.chat.complete(model=\"mistral-small-latest\", messages=[\n",
        "        {\n",
        "            \"content\": user_input,\n",
        "            \"role\": \"user\",\n",
        "        },\n",
        "    ])\n",
        "\n",
        "  if res is not None:\n",
        "    # Возвращаем текст ответа\n",
        "    return res\n",
        "  return None\n",
        "\n",
        "\n",
        "response = get_gpt_response(\"Who is the best French painter? Answer in one short sentence.\")\n",
        "print(response)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Claude Monet, the founder of French Impressionism, is often considered the best French painter.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer = response.choices[0].message.content\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Напишите функцию, которая принимает ввод от пользователя и возвращает ответ от модели GPT, а также подсчитывает количество токенов. Ваша функция должна быть способна обрабатывать любой ввод пользователя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ GPT: Many consider Claude Monet the best French painter.\n",
            "Количество токенов: 23\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def get_gpt_response_and_token_count(user_input):\n",
        "    \"\"\"\n",
        "    Получает ответ от Mistral API и подсчитывает количество токенов\n",
        "    \n",
        "    Args:\n",
        "        user_input (str): Входной текст пользователя\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (ответ модели, количество токенов)\n",
        "    \"\"\"\n",
        "    # Получаем API ключ и инициализируем клиент\n",
        "    client = Mistral(api_key=api_key)\n",
        "    \n",
        "    # Получаем ответ от API\n",
        "    response = client.chat.complete(model=\"mistral-small-latest\", messages=[\n",
        "        {\n",
        "            \"content\": user_input,\n",
        "            \"role\": \"user\",\n",
        "        },\n",
        "    ])\n",
        "    \n",
        "    # Получаем текст ответа\n",
        "    response_text = response.choices[0].message.content\n",
        "    \n",
        "    # Подсчет токенов (используем cl100k_base как приближение)\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    \n",
        "    # Подсчитываем токены для входного текста и ответа\n",
        "    input_tokens = len(encoding.encode(user_input))\n",
        "    output_tokens = len(encoding.encode(response_text))\n",
        "    total_tokens = input_tokens + output_tokens\n",
        "    \n",
        "    return response_text, total_tokens\n",
        "\n",
        "response, token_count = get_gpt_response_and_token_count(\"Who is the best French painter? Answer in one short sentence.\")\n",
        "\n",
        "print(\"Ответ GPT:\", response)\n",
        "print(\"Количество токенов:\", token_count)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
